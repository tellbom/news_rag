from flask import Flask, request, jsonify
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from openai import OpenAI
import pandas as pd
import uuid
from typing import List, Dict, Optional, Union, Any, Tuple
import json
from datetime import datetime
import os
import numpy as np
from bs4 import BeautifulSoup
from PIL import Image
from io import BytesIO
import requests
from paddleocr import PaddleOCR
import mammoth
import PyPDF2
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document as LCDocument
import tempfile
import threading
import queue
import time
import uuid
from typing import Dict, Callable, Any, List, Optional
import logging

from flask import Flask

app = Flask(__name__)


class VectorServiceClient:
    """å‘é‡æœåŠ¡å®¢æˆ·ç«¯ï¼Œç”¨äºä¸Qdrant+LlamaIndexæœåŠ¡é€šä¿¡"""

    def __init__(self, base_url: str = "http://localhost:5001"):
        """
        åˆå§‹åŒ–å‘é‡æœåŠ¡å®¢æˆ·ç«¯

        Args:
            base_url: å‘é‡æœåŠ¡çš„åŸºç¡€URL
        """
        self.base_url = base_url.rstrip('/')
        self.is_available = self._check_health()
        self.session = requests.Session()
        self.logger = logging.getLogger("VectorServiceClient")

    def _check_health(self) -> bool:
        """
        æ£€æŸ¥å‘é‡æœåŠ¡æ˜¯å¦å¯ç”¨

        Returns:
            bool: æœåŠ¡æ˜¯å¦å¯ç”¨
        """
        try:
            response = requests.get(f"{self.base_url}/health", timeout=5)
            return response.status_code == 200 and response.json().get("status") == "healthy"
        except Exception as e:
            self.logger.error(f"å‘é‡æœåŠ¡å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")
            return False

    def search_vector(self, query: str, n_results: int = 5, search_type: str = "all") -> Dict:
        """
        æ‰§è¡Œå‘é‡æœç´¢

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            n_results: è¿”å›ç»“æœæ•°é‡
            search_type: æœç´¢ç±»å‹ (all, news, announcements)

        Returns:
            Dict: æœç´¢ç»“æœ
        """
        if not self.is_available:
            return {"news": [], "announcements": []}

        try:
            response = self.session.post(
                f"{self.base_url}/vector/search",
                json={
                    "query": query,
                    "n_results": n_results,
                    "search_type": search_type,
                    "mmr_enabled": True
                },
                timeout=10
            )

            if response.status_code == 200:
                return response.json()
            else:
                self.logger.error(f"å‘é‡æœç´¢å¤±è´¥: {response.text}")
                return {"news": [], "announcements": []}
        except Exception as e:
            self.logger.error(f"å‘é‡æœç´¢è¯·æ±‚å‡ºé”™: {str(e)}")
            return {"news": [], "announcements": []}

    def add_news(self, title: str, content: str, source: str = None,
                 publish_date: str = None, tags: List = None, id: str = None) -> Dict:
        """
        æ·»åŠ æ–°é—»åˆ°å‘é‡æœåŠ¡

        Args:
            title: æ–°é—»æ ‡é¢˜
            content: æ–°é—»å†…å®¹ï¼ˆå·²å¤„ç†çš„çº¯æ–‡æœ¬ï¼‰
            source: æ¥æº
            publish_date: å‘å¸ƒæ—¥æœŸ
            tags: æ ‡ç­¾åˆ—è¡¨
            id: æ–‡æ¡£ID

        Returns:
            Dict: åŒ…å«ä»»åŠ¡IDçš„å“åº”
        """
        if not self.is_available:
            return {"success": False, "error": "å‘é‡æœåŠ¡ä¸å¯ç”¨"}

        data = {
            "title": title,
            "content": content
        }

        if source:
            data["source"] = source
        if publish_date:
            data["publish_date"] = publish_date
        if tags:
            data["tags"] = tags
        if id:
            data["id"] = id

        try:
            response = self.session.post(
                f"{self.base_url}/news",
                json=data,
                timeout=10
            )

            if response.status_code == 200:
                return response.json()
            else:
                self.logger.error(f"æ·»åŠ æ–°é—»å¤±è´¥: {response.text}")
                return {"success": False, "error": response.text}
        except Exception as e:
            self.logger.error(f"æ·»åŠ æ–°é—»è¯·æ±‚å‡ºé”™: {str(e)}")
            return {"success": False, "error": str(e)}

    def add_announcement(self, title: str, content: str, department: str = None,
                         publish_date: str = None, importance: str = "normal", id: str = None) -> Dict:
        """
        æ·»åŠ å…¬å‘Šåˆ°å‘é‡æœåŠ¡

        Args:
            title: å…¬å‘Šæ ‡é¢˜
            content: å…¬å‘Šå†…å®¹ï¼ˆå·²å¤„ç†çš„çº¯æ–‡æœ¬ï¼‰
            department: éƒ¨é—¨
            publish_date: å‘å¸ƒæ—¥æœŸ
            importance: é‡è¦æ€§
            id: æ–‡æ¡£ID

        Returns:
            Dict: åŒ…å«ä»»åŠ¡IDçš„å“åº”
        """
        if not self.is_available:
            return {"success": False, "error": "å‘é‡æœåŠ¡ä¸å¯ç”¨"}

        data = {
            "title": title,
            "content": content
        }

        if department:
            data["department"] = department
        if publish_date:
            data["publish_date"] = publish_date
        if importance:
            data["importance"] = importance
        if id:
            data["id"] = id

        try:
            response = self.session.post(
                f"{self.base_url}/announcement",
                json=data,
                timeout=10
            )

            if response.status_code == 200:
                return response.json()
            else:
                self.logger.error(f"æ·»åŠ å…¬å‘Šå¤±è´¥: {response.text}")
                return {"success": False, "error": response.text}
        except Exception as e:
            self.logger.error(f"æ·»åŠ å…¬å‘Šè¯·æ±‚å‡ºé”™: {str(e)}")
            return {"success": False, "error": str(e)}

    def delete_news(self, doc_id: str) -> bool:
        """
        åˆ é™¤æ–°é—»

        Args:
            doc_id: æ–‡æ¡£ID

        Returns:
            bool: æ˜¯å¦æˆåŠŸåˆ é™¤
        """
        if not self.is_available:
            return False

        try:
            response = self.session.delete(
                f"{self.base_url}/news/{doc_id}",
                timeout=10
            )
            return response.status_code == 200
        except Exception as e:
            self.logger.error(f"åˆ é™¤æ–°é—»è¯·æ±‚å‡ºé”™: {str(e)}")
            return False

    def delete_announcement(self, doc_id: str) -> bool:
        """
        åˆ é™¤å…¬å‘Š

        Args:
            doc_id: æ–‡æ¡£ID

        Returns:
            bool: æ˜¯å¦æˆåŠŸåˆ é™¤
        """
        if not self.is_available:
            return False

        try:
            response = self.session.delete(
                f"{self.base_url}/announcement/{doc_id}",
                timeout=10
            )
            return response.status_code == 200
        except Exception as e:
            self.logger.error(f"åˆ é™¤å…¬å‘Šè¯·æ±‚å‡ºé”™: {str(e)}")
            return False

    def get_task_status(self, task_id: str) -> Dict:
        """
        è·å–ä»»åŠ¡çŠ¶æ€

        Args:
            task_id: ä»»åŠ¡ID

        Returns:
            Dict: ä»»åŠ¡çŠ¶æ€
        """
        if not self.is_available:
            return {"status": "unavailable"}

        try:
            response = self.session.get(
                f"{self.base_url}/task/{task_id}",
                timeout=10
            )

            if response.status_code == 200:
                return response.json()
            else:
                self.logger.error(f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {response.text}")
                return {"status": "error", "error": response.text}
        except Exception as e:
            self.logger.error(f"è·å–ä»»åŠ¡çŠ¶æ€è¯·æ±‚å‡ºé”™: {str(e)}")
            return {"status": "error", "error": str(e)}

class ESServiceClient:
    """
    ESæœåŠ¡å®¢æˆ·ç«¯ï¼Œç”¨äºä¸ESæœåŠ¡APIé€šä¿¡
    """

    def __init__(self, base_url: str = "http://localhost:8085"):
        """
        åˆå§‹åŒ–ESæœåŠ¡å®¢æˆ·ç«¯

        Args:
            base_url: ESæœåŠ¡çš„åŸºç¡€URL
        """
        self.base_url = base_url.rstrip('/')
        self.is_available = self._check_health()
        self.session = requests.Session()

    def _check_health(self) -> bool:
        """
        æ£€æŸ¥ESæœåŠ¡æ˜¯å¦å¯ç”¨

        Returns:
            bool: æœåŠ¡æ˜¯å¦å¯ç”¨
        """
        try:
            response = requests.get(f"{self.base_url}/health", timeout=5)
            return response.status_code == 200 and response.json().get("status") == "healthy"
        except Exception as e:
            app.logger.error(f"ESæœåŠ¡å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")
            return False

    def index_news(self, document: Dict, async_mode: bool = True) -> bool:
        """
        ç´¢å¼•æ–°é—»åˆ°ES

        Args:
            document: æ–°é—»æ–‡æ¡£
            async_mode: æ˜¯å¦å¼‚æ­¥æ‰§è¡Œ

        Returns:
            bool: åŒæ­¥æ¨¡å¼ä¸‹æ˜¯å¦æˆåŠŸï¼Œå¼‚æ­¥æ¨¡å¼ä¸‹å§‹ç»ˆè¿”å›True
        """
        if not self.is_available:
            return False

        url = f"{self.base_url}/index/news"

        if async_mode:
            # åˆ›å»ºçº¿ç¨‹æ¥æ‰§è¡Œè¯·æ±‚
            thread = threading.Thread(
                target=self._make_request,
                args=(url, document)
            )
            thread.daemon = True
            thread.start()
            return True
        else:
            # åŒæ­¥æ‰§è¡Œ
            return self._make_request(url, document)

    def index_notice(self, document: Dict, async_mode: bool = True) -> bool:
        """
        ç´¢å¼•å…¬å‘Šåˆ°ES

        Args:
            document: å…¬å‘Šæ–‡æ¡£
            async_mode: æ˜¯å¦å¼‚æ­¥æ‰§è¡Œ

        Returns:
            bool: åŒæ­¥æ¨¡å¼ä¸‹æ˜¯å¦æˆåŠŸï¼Œå¼‚æ­¥æ¨¡å¼ä¸‹å§‹ç»ˆè¿”å›True
        """
        if not self.is_available:
            return False

        url = f"{self.base_url}/index/notice"

        if async_mode:
            # åˆ›å»ºçº¿ç¨‹æ¥æ‰§è¡Œè¯·æ±‚
            thread = threading.Thread(
                target=self._make_request,
                args=(url, document)
            )
            thread.daemon = True
            thread.start()
            return True
        else:
            # åŒæ­¥æ‰§è¡Œ
            return self._make_request(url, document)

    def search_news(self, query: str, n_results: int = 5) -> List[Dict]:
        """
        ä»ESæœç´¢æ–°é—»

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            n_results: è¿”å›ç»“æœæ•°é‡

        Returns:
            List[Dict]: æ–°é—»åˆ—è¡¨
        """
        if not self.is_available:
            return []

        url = f"{self.base_url}/search/news"
        data = {"query": query, "n_results": n_results}

        try:
            response = self.session.post(url, json=data, timeout=10)
            if response.status_code == 200:
                return response.json().get("results", [])
            else:
                app.logger.error(f"æœç´¢æ–°é—»å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return []
        except Exception as e:
            app.logger.error(f"æœç´¢æ–°é—»æ—¶å‡ºé”™: {str(e)}")
            return []

    def search_notice(self, query: str, n_results: int = 5) -> List[Dict]:
        """
        ä»ESæœç´¢å…¬å‘Š

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            n_results: è¿”å›ç»“æœæ•°é‡

        Returns:
            List[Dict]: å…¬å‘Šåˆ—è¡¨
        """
        if not self.is_available:
            return []

        url = f"{self.base_url}/search/notice"
        data = {"query": query, "n_results": n_results}

        try:
            response = self.session.post(url, json=data, timeout=10)
            if response.status_code == 200:
                return response.json().get("results", [])
            else:
                app.logger.error(f"æœç´¢å…¬å‘Šå¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return []
        except Exception as e:
            app.logger.error(f"æœç´¢å…¬å‘Šæ—¶å‡ºé”™: {str(e)}")
            return []

    def search_all(self, query: str, n_results: int = 5) -> Dict[str, List[Dict]]:
        """
        ä»ESåŒæ—¶æœç´¢æ–°é—»å’Œå…¬å‘Š

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            n_results: æ¯ç§ç±»å‹è¿”å›çš„ç»“æœæ•°é‡

        Returns:
            Dict[str, List[Dict]]: åŒ…å«æ–°é—»å’Œå…¬å‘Šçš„å­—å…¸
        """
        if not self.is_available:
            return {"news": [], "announcements": []}

        url = f"{self.base_url}/search/all"
        data = {"query": query, "n_results": n_results}

        try:
            response = self.session.post(url, json=data, timeout=10)
            if response.status_code == 200:
                return response.json()
            else:
                app.logger.error(f"æœç´¢å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return {"news": [], "announcements": []}
        except Exception as e:
            app.logger.error(f"æœç´¢æ—¶å‡ºé”™: {str(e)}")
            return {"news": [], "announcements": []}

    def delete_news(self, doc_id: str) -> bool:
        """
        åˆ é™¤æ–°é—»

        Args:
            doc_id: æ–‡æ¡£IDæˆ–åŸºç¡€ID

        Returns:
            bool: æ˜¯å¦æˆåŠŸåˆ é™¤
        """
        success = True

        # å¦‚æœå¯ç”¨äº†æ··åˆæœç´¢ï¼Œä»ESåˆ é™¤
        if self.es_client and self.es_client.is_available:
            es_success = self.es_client.delete_news(doc_id)
            if not es_success:
                app.logger.error(f"ä»ESæœåŠ¡åˆ é™¤æ–°é—» {doc_id} å¤±è´¥")
                success = False

        return success

    def delete_notice(self, doc_id: str) -> bool:
        """
        åˆ é™¤å…¬å‘Š

        Args:
            doc_id: æ–‡æ¡£IDæˆ–åŸºç¡€ID

        Returns:
            bool: æ˜¯å¦æˆåŠŸåˆ é™¤
        """
        success = True
        # å¦‚æœå¯ç”¨äº†æ··åˆæœç´¢ï¼Œä»ESåˆ é™¤
        if self.es_client and self.es_client.is_available:
            es_success = self.es_client.delete_notice(doc_id)
            if not es_success:
                app.logger.error(f"ä»ESæœåŠ¡åˆ é™¤å…¬å‘Š {doc_id} å¤±è´¥")
                success = False

        return success

    def _make_request(self, url: str, data: Dict) -> bool:
        """
        æ‰§è¡Œè¯·æ±‚

        Args:
            url: è¯·æ±‚URL
            data: è¯·æ±‚æ•°æ®

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            response = self.session.post(url, json=data, timeout=10)
            return response.status_code == 200
        except Exception as e:
            app.logger.error(f"è¯·æ±‚ {url} æ—¶å‡ºé”™: {str(e)}")
            return False

# RankFusionç±»ç®—æ³•ç±»
class RankFusion:
    """ç»“æœèåˆç®—æ³•å·¥å…·ç±»ï¼Œç”¨äºèåˆå‘é‡å’ŒBM25æœç´¢ç»“æœ"""

    @staticmethod
    def contextual_fusion(query: str, dense_results: dict, lexical_results: dict, k: int = 60,
                          rerank_url: str = None) -> dict:
        """
        ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„èåˆç®—æ³•ï¼Œé’ˆå¯¹ä¸åŒç±»å‹çš„æŸ¥è¯¢åŠ¨æ€è°ƒæ•´æƒé‡

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            dense_results: å‘é‡æ£€ç´¢ç»“æœ (æ ¼å¼: {"news": [...], "announcements": [...]})
            lexical_results: æ–‡æœ¬æ£€ç´¢ç»“æœ (æ ¼å¼: {"news": [...], "announcements": [...]})
            k: RRFå¸¸æ•°
            rerank_url: é‡æ’åºæœåŠ¡URLï¼Œå¦‚æœæä¾›åˆ™ä½¿ç”¨é‡æ’åºæœåŠ¡

        Returns:
            èåˆåçš„ç»“æœå­—å…¸ (æ ¼å¼: {"news": [...], "announcements": [...]})
        """
        # ç”¨äºä¿å­˜èåˆç»“æœ
        fused_results = {"news": [], "announcements": []}

        # å¤„ç†æ–°é—»å’Œå…¬å‘Š
        for content_type in ["news", "announcements"]:
            # è·å–å„è‡ªçš„ç»“æœ
            vector_content = dense_results.get(content_type, [])
            lexical_content = lexical_results.get(content_type, [])

            # åˆå¹¶ç»“æœ
            combined_results = []
            seen_ids = set()

            # é¦–å…ˆæ·»åŠ å‘é‡ç»“æœ
            for item in vector_content:
                item_id = item.get("id", "")
                if item_id and item_id not in seen_ids:
                    seen_ids.add(item_id)
                    # ç¡®ä¿æºç±»å‹æ ‡è®°æ­£ç¡®
                    item["source_type"] = item.get("source_type", "vector")
                    combined_results.append(item)

            # ç„¶åæ·»åŠ BM25ç»“æœ
            for item in lexical_content:
                item_id = item.get("id", "")
                if item_id and item_id not in seen_ids:
                    seen_ids.add(item_id)
                    # ç¡®ä¿æºç±»å‹æ ‡è®°æ­£ç¡®
                    item["source_type"] = item.get("source_type", "lexical")
                    combined_results.append(item)

            # å¦‚æœæä¾›äº†é‡æ’åºæœåŠ¡URLï¼Œä½¿ç”¨é‡æ’åº
            if rerank_url and combined_results:
                try:
                    # å¤åˆ¶ä¸€ä»½ç»“æœï¼Œä»¥ä¾¿åœ¨é‡æ’åºå¤±è´¥æ—¶å›é€€
                    backup_results = combined_results.copy()

                    # æ ‡å‡†åŒ–ç»“æœæ ¼å¼ï¼Œç¡®ä¿å†…å®¹å­—æ®µä¸€è‡´
                    normalized_results = []
                    for item in combined_results:
                        normalized_item = item.copy()
                        # ç¡®ä¿æ¯ä¸ªé¡¹ç›®éƒ½æœ‰contentå­—æ®µ
                        if "content" not in normalized_item:
                            normalized_item["content"] = normalized_item.get("text", "")
                        normalized_results.append(normalized_item)

                    # è°ƒç”¨é‡æ’åºæœåŠ¡
                    response = requests.post(
                        rerank_url + "/rerank",
                        json={
                            "query": query,
                            "documents": normalized_results,
                            "top_k": len(normalized_results)  # ä¿ç•™æ‰€æœ‰ç»“æœ
                        },
                        timeout=10
                    )

                    if response.status_code == 200:
                        # ä½¿ç”¨é‡æ’åºç»“æœ
                        reranked = response.json().get("results", [])
                        if reranked:
                            fused_results[content_type] = reranked
                            continue  # è·³è¿‡åç»­å¤„ç†
                    else:
                        app.logger.warning(f"é‡æ’åºæœåŠ¡è¿”å›é200çŠ¶æ€ç : {response.status_code}")
                except Exception as e:
                    app.logger.error(f"è°ƒç”¨é‡æ’åºæœåŠ¡å¤±è´¥: {str(e)}")
                    # å›é€€åˆ°åŸå§‹ç»“æœ
                    combined_results = backup_results

            # å¦‚æœæ²¡æœ‰ä½¿ç”¨é‡æ’åºæˆ–é‡æ’åºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹èåˆæ–¹æ³•
            # æå–æŸ¥è¯¢ç‰¹å¾
            query_terms = set(query.lower().split())
            is_status_query = any(term in query_terms for term in ['çŠ¶æ€', 'å–æ¶ˆ', 'å®Œæˆ', 'æ”¯ä»˜'])
            is_time_query = any(term in query_terms for term in ['æ—¶é—´', 'æ—¥æœŸ', 'å¹´', 'æœˆ', 'æ—¥'])
            is_type_query = any(term in query_terms for term in ['ç±»å‹', 'ç§ç±»', 'åˆ†ç±»'])

            # åŠ¨æ€è°ƒæ•´æƒé‡
            if is_status_query or is_type_query:
                vector_weight = 0.4
                lexical_weight = 0.6
            elif is_time_query:
                vector_weight = 0.5
                lexical_weight = 0.5
            else:
                vector_weight = 0.7
                lexical_weight = 0.3

            # è®¡ç®—èåˆåˆ†æ•°
            scores = {}

            # å¤„ç†å‘é‡ç»“æœ
            for rank, item in enumerate(vector_content, start=1):
                item_id = item.get("id", "")
                if not item_id:
                    continue

                if item_id not in scores:
                    scores[item_id] = {"item": item, "score": 0, "matches": set()}

                # è·å–ç›¸å…³æ€§åˆ†æ•° - å…¼å®¹ä¸åŒæ ¼å¼
                relevance_score = item.get("relevance_score", 0.0)

                # åŠ æƒRRFåˆ†æ•°ï¼Œè€ƒè™‘åŸå§‹ç›¸å…³æ€§åˆ†æ•°
                scores[item_id]["score"] += vector_weight * (1.0 / (k + rank)) * (1 + 0.2 * relevance_score)
                scores[item_id]["matches"].add("vector")

            # å¤„ç†BM25ç»“æœ
            for rank, item in enumerate(lexical_content, start=1):
                item_id = item.get("id", "")
                if not item_id:
                    continue

                if item_id not in scores:
                    scores[item_id] = {"item": item, "score": 0, "matches": set()}

                scores[item_id]["score"] += lexical_weight * (1.0 / (k + rank))
                scores[item_id]["matches"].add("lexical")

                # é¢å¤–çš„ä¸Šä¸‹æ–‡å¥–åŠ±
                content = item.get("content", "")
                title = item.get("title", "")

                # æ£€æŸ¥ç»“æœæ˜¯å¦åŒ…å«æŸ¥è¯¢è¯
                term_matches = sum(1 for term in query_terms if term in (content + title).lower())
                term_match_ratio = term_matches / len(query_terms) if query_terms else 0

                # è¯åŒ¹é…å¥–åŠ±
                scores[item_id]["score"] *= (1 + 0.2 * term_match_ratio)

            # å¤šæ£€ç´¢æºå¥–åŠ±
            for item_id, data in scores.items():
                if len(data["matches"]) > 1:  # åŒæ—¶å‡ºç°åœ¨ä¸¤ç§æ£€ç´¢ä¸­
                    data["score"] *= 1.25

            # æŒ‰åˆ†æ•°æ’åº
            sorted_items = sorted(scores.values(), key=lambda x: x["score"], reverse=True)
            fused_results[content_type] = [item_data["item"] for item_data in sorted_items]

        return fused_results

class AsyncTaskManager:
    """å¼‚æ­¥ä»»åŠ¡ç®¡ç†å™¨ï¼Œä½¿ç”¨çº¿ç¨‹æ± å¤„ç†è€—æ—¶ä»»åŠ¡"""

    def __init__(self, max_workers=5):
        """
        åˆå§‹åŒ–ä»»åŠ¡ç®¡ç†å™¨

        Args:
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
        """
        self.max_workers = max_workers
        self.task_queue = queue.Queue()
        self.workers = []
        self.results = {}  # å­˜å‚¨ä»»åŠ¡ç»“æœ
        self.status = {}  # å­˜å‚¨ä»»åŠ¡çŠ¶æ€ (pending, running, completed, failed)
        self.callbacks = {}  # ä»»åŠ¡å®Œæˆåçš„å›è°ƒå‡½æ•°
        self.logger = logging.getLogger("AsyncTaskManager")
        self._start_workers()

    def _worker_loop(self):
        """å·¥ä½œçº¿ç¨‹å¾ªç¯ï¼Œä¸æ–­ä»é˜Ÿåˆ—è·å–ä»»åŠ¡å¹¶æ‰§è¡Œ"""
        while True:
            try:
                # ä»é˜Ÿåˆ—è·å–ä»»åŠ¡
                task_id, task_func, args, kwargs = self.task_queue.get()

                # æ›´æ–°ä»»åŠ¡çŠ¶æ€
                self.status[task_id] = "running"
                self.logger.info(f"å¼€å§‹æ‰§è¡Œä»»åŠ¡ {task_id}")

                try:
                    # æ‰§è¡Œä»»åŠ¡
                    result = task_func(*args, **kwargs)
                    # å­˜å‚¨ç»“æœ
                    self.results[task_id] = result
                    self.status[task_id] = "completed"
                    self.logger.info(f"ä»»åŠ¡ {task_id} å®Œæˆ")

                    # æ‰§è¡Œå›è°ƒï¼ˆå¦‚æœæœ‰ï¼‰
                    if task_id in self.callbacks and self.callbacks[task_id]:
                        try:
                            self.callbacks[task_id](result)
                            self.logger.info(f"ä»»åŠ¡ {task_id} å›è°ƒæ‰§è¡ŒæˆåŠŸ")
                        except Exception as e:
                            self.logger.error(f"ä»»åŠ¡ {task_id} å›è°ƒæ‰§è¡Œå¤±è´¥: {str(e)}")

                except Exception as e:
                    # ä»»åŠ¡æ‰§è¡Œå¤±è´¥
                    self.status[task_id] = "failed"
                    self.results[task_id] = str(e)
                    self.logger.error(f"ä»»åŠ¡ {task_id} æ‰§è¡Œå¤±è´¥: {str(e)}")

                # æ ‡è®°ä»»åŠ¡å®Œæˆ
                self.task_queue.task_done()

            except Exception as e:
                self.logger.error(f"å·¥ä½œçº¿ç¨‹æ‰§è¡Œå‡ºé”™: {str(e)}")
                # çŸ­æš‚ä¼‘æ¯ä»¥é¿å…CPUå ç”¨è¿‡é«˜
                time.sleep(0.1)

    def _start_workers(self):
        """å¯åŠ¨å·¥ä½œçº¿ç¨‹"""
        for i in range(self.max_workers):
            thread = threading.Thread(target=self._worker_loop, daemon=True)
            thread.start()
            self.workers.append(thread)
            self.logger.info(f"å¯åŠ¨å·¥ä½œçº¿ç¨‹ {i + 1}")

    def submit_task(self, task_func: Callable, callback: Optional[Callable] = None, *args, **kwargs) -> str:
        """
        æäº¤ä»»åŠ¡åˆ°é˜Ÿåˆ—

        Args:
            task_func: è¦æ‰§è¡Œçš„å‡½æ•°
            callback: ä»»åŠ¡å®Œæˆåçš„å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶ä»»åŠ¡ç»“æœä½œä¸ºå‚æ•°
            *args, **kwargs: ä¼ é€’ç»™ä»»åŠ¡å‡½æ•°çš„å‚æ•°

        Returns:
            str: ä»»åŠ¡ID
        """
        task_id = str(uuid.uuid4())
        self.status[task_id] = "pending"

        if callback:
            self.callbacks[task_id] = callback

        # å°†ä»»åŠ¡æ”¾å…¥é˜Ÿåˆ—
        self.task_queue.put((task_id, task_func, args, kwargs))
        self.logger.info(f"æäº¤ä»»åŠ¡ {task_id} åˆ°é˜Ÿåˆ—")

        return task_id

    def get_task_status(self, task_id: str) -> Dict:
        """
        è·å–ä»»åŠ¡çŠ¶æ€

        Args:
            task_id: ä»»åŠ¡ID

        Returns:
            Dict: åŒ…å«ä»»åŠ¡çŠ¶æ€å’Œç»“æœï¼ˆå¦‚æœå·²å®Œæˆï¼‰
        """
        if task_id not in self.status:
            return {"status": "not_found"}

        result = {
            "status": self.status[task_id]
        }

        # å¦‚æœä»»åŠ¡å·²å®Œæˆæˆ–å¤±è´¥ï¼ŒåŒ…å«ç»“æœ
        if self.status[task_id] in ["completed", "failed"] and task_id in self.results:
            result["result"] = self.results[task_id]

        return result

    def wait_for_task(self, task_id: str, timeout: Optional[float] = None) -> Any:
        """
        ç­‰å¾…ä»»åŠ¡å®Œæˆå¹¶è¿”å›ç»“æœ

        Args:
            task_id: ä»»åŠ¡ID
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            Any: ä»»åŠ¡ç»“æœ

        Raises:
            TimeoutError: å¦‚æœç­‰å¾…è¶…æ—¶
            ValueError: å¦‚æœä»»åŠ¡ä¸å­˜åœ¨
            RuntimeError: å¦‚æœä»»åŠ¡æ‰§è¡Œå¤±è´¥
        """
        if task_id not in self.status:
            raise ValueError(f"ä»»åŠ¡ {task_id} ä¸å­˜åœ¨")

        start_time = time.time()
        while self.status[task_id] in ["pending", "running"]:
            time.sleep(0.1)

            if timeout and (time.time() - start_time) > timeout:
                raise TimeoutError(f"ç­‰å¾…ä»»åŠ¡ {task_id} è¶…æ—¶")

        if self.status[task_id] == "failed":
            raise RuntimeError(f"ä»»åŠ¡ {task_id} æ‰§è¡Œå¤±è´¥: {self.results[task_id]}")

        return self.results[task_id]

    def clean_old_tasks(self, max_age: float = 3600):
        """
        æ¸…ç†æ—§ä»»åŠ¡æ•°æ®

        Args:
            max_age: æœ€å¤§ä¿ç•™æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤1å°æ—¶
        """
        # å®ç°æ¸…ç†é€»è¾‘...
        pass

class DocumentProcessor:
    """Document processing utilities for different file types"""

    def __init__(self):
        """Initialize document processor with OCR model"""
        # Handle numpy compatibility issue
        if not hasattr(np, 'int'):
            np.int = np.int32

        # Initialize OCR model
        try:
            self.ocr = PaddleOCR(
                det_model_dir="/root/.paddleocr/whl/det/ch/ch_PP-OCRv3_det_infer",
                rec_model_dir="/root/.paddleocr/whl/rec/ch/ch_PP-OCRv3_rec_infer",
                cls_model_dir="/root/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer",
                use_angle_cls=True,
                lang="ch"
            )
            app.logger.info("OCR model initialized successfully")
        except Exception as e:
            self.ocr = None
            app.logger.error(f"Failed to initialize OCR model: {str(e)}")

    def sanitize_html(self, html_content: str) -> str:
        """
        Sanitize HTML content to extract clean text

        Args:
            html_content: HTML string to sanitize

        Returns:
            str: Clean text extracted from HTML
        """
        try:
            soup = BeautifulSoup(html_content, 'html.parser')

            # Remove script and style elements
            for script in soup(["script", "style"]):
                script.extract()

            # Get text
            text = soup.get_text(separator=' ', strip=True)

            # Clean up whitespace
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = '\n'.join(chunk for chunk in chunks if chunk)

            return text
        except Exception as e:
            app.logger.error(f"HTML sanitization error: {str(e)}")
            # Return the original content if sanitization fails
            return html_content

    def process_image(self, image_data: bytes) -> Tuple[str, float]:
        """
        Extract text from image using OCR

        Args:
            image_data: Binary image data

        Returns:
            Tuple[str, float]: Extracted text and average confidence score
        """
        if self.ocr is None:
            app.logger.error("OCR model not initialized")
            return "OCR model not available", 0.0

        try:
            # Convert bytes to PIL Image
            img = Image.open(BytesIO(image_data))

            # Convert to numpy array
            img_np = np.array(img)

            # Run OCR
            result = self.ocr.ocr(img_np, cls=True)

            # Extract text and calculate average confidence
            text_parts = []
            confidence_sum = 0.0
            count = 0

            if result:
                for line in result:
                    if isinstance(line, list) and line and len(line) > 0:
                        if len(line[-1]) >= 2:
                            text = line[-1][0]  # Text content
                            confidence = float(line[-1][1])  # Confidence score

                            if text and confidence > 0.5:  # Only include reasonably confident results
                                text_parts.append(text)
                                confidence_sum += confidence
                                count += 1

            # Calculate average confidence if there are valid detections
            avg_confidence = confidence_sum / count if count > 0 else 0.0
            full_text = "\n".join(text_parts)

            if not full_text.strip():
                return "No text detected in image", 0.0

            return full_text, avg_confidence

        except Exception as e:
            app.logger.error(f"Image processing error: {str(e)}")
            return f"Failed to process image: {str(e)}", 0.0

    def process_word_document(self, docx_data: bytes) -> str:
        """
        Extract text from Word document

        Args:
            docx_data: Binary Word document data

        Returns:
            str: Extracted text
        """
        try:
            result = mammoth.extract_raw_text(BytesIO(docx_data))
            return result.value
        except Exception as e:
            app.logger.error(f"Word document processing error: {str(e)}")
            return f"Failed to process Word document: {str(e)}"

    def process_pdf(self, pdf_data: bytes) -> str:
        """
        Extract text from PDF document

        Args:
            pdf_data: Binary PDF data

        Returns:
            str: Extracted text
        """
        try:
            with BytesIO(pdf_data) as pdf_file:
                pdf_reader = PyPDF2.PdfReader(pdf_file)
                text = []

                for page_num in range(len(pdf_reader.pages)):
                    page = pdf_reader.pages[page_num]
                    text.append(page.extract_text())

            return "\n\n".join(text)
        except Exception as e:
            app.logger.error(f"PDF processing error: {str(e)}")
            return f"Failed to process PDF: {str(e)}"

    def get_file_content(self, file_data: bytes, file_type: str) -> str:
        """
        Process file based on its type and return text content

        Args:
            file_data: Binary file data
            file_type: MIME type or file extension

        Returns:
            str: Extracted text content
        """
        file_type = file_type.lower()

        if file_type in ['image/jpeg', 'image/png', 'image/jpg', 'image/gif', 'image/bmp']:
            text, confidence = self.process_image(file_data)
            return text
        elif file_type in ['application/vnd.openxmlformats-officedocument.wordprocessingml.document',
                           'application/msword', '.docx', '.doc']:
            return self.process_word_document(file_data)
        elif file_type in ['application/pdf', '.pdf']:
            return self.process_pdf(file_data)
        elif file_type in ['text/html', '.html', '.htm']:
            return self.sanitize_html(file_data.decode('utf-8', errors='replace'))
        elif file_type in ['text/plain', '.txt']:
            return file_data.decode('utf-8', errors='replace')
        else:
            return f"Unsupported file type: {file_type}"

class ChineseRAGSystem:
    def __init__(
            self,
            llm_api_key: str = None,
            llm_base_url: str = "https://dashscope.aliyuncs.com/compatible-mode/v1",
            llm_model: str = "qwen-plus",
            use_langchain: bool = True,
            chunk_size: int = 1000,
            chunk_overlap: int = 200,
            use_hybrid_search: bool = False,
            es_service_url: str = "http://localhost:8085",
            vector_service_url: str = "http://localhost:5001"
    ):
        """
        åˆå§‹åŒ–ä¸­æ–‡RAGç³»ç»Ÿ

        Args:
            llm_api_key: å¤§æ¨¡å‹APIå¯†é’¥
            llm_base_url: å¤§æ¨¡å‹APIåŸºç¡€URL
            llm_model: ä½¿ç”¨çš„å¤§æ¨¡å‹åç§°
            use_langchain: æ˜¯å¦ä½¿ç”¨LangChainè¿›è¡Œæ–‡æ¡£åˆ†å—
            chunk_size: æ–‡æ¡£åˆ†å—å¤§å°
            chunk_overlap: æ–‡æ¡£åˆ†å—é‡å éƒ¨åˆ†å¤§å°
            use_hybrid_search: æ˜¯å¦å¼€å¯æ··åˆæ£€ç´¢(vector+bm25)
            es_service_url: esæœåŠ¡å™¨
            vector_service_url: å‘é‡æœåŠ¡URL
        """

        # ESæœåŠ¡é…ç½®
        self.use_hybrid_search = use_hybrid_search
        self.es_service_url = es_service_url

        # åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨
        self.doc_processor = DocumentProcessor()

        # æ˜¯å¦ä½¿ç”¨LangChain
        self.use_langchain = use_langchain
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

        # åˆå§‹åŒ–å‘é‡æœåŠ¡å®¢æˆ·ç«¯
        self.vector_service_url = vector_service_url
        self.vector_client = VectorServiceClient(base_url=vector_service_url)
        app.logger.info(f"å‘é‡æœåŠ¡å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆï¼ŒæœåŠ¡å¯ç”¨: {self.vector_client.is_available}")

        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        self.llm_api_key = llm_api_key or os.environ.get("LLM_API_KEY")
        self.llm_client = OpenAI(
            api_key=self.llm_api_key,
            base_url=llm_base_url
        )
        self.llm_model = llm_model
        app.logger.info(f"LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {llm_model}")

        # åˆå§‹åŒ–ESæœåŠ¡å®¢æˆ·ç«¯ï¼ˆå¦‚æœé…ç½®äº†ESæœåŠ¡URLï¼‰
        self.es_client = None
        if self.es_service_url and self.use_hybrid_search:
            self.es_client = ESServiceClient(base_url=self.es_service_url)
            app.logger.info(f"ESæœåŠ¡å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆï¼ŒæœåŠ¡å¯ç”¨: {self.es_client.is_available}")
        else:
            app.logger.info("æœªé…ç½®ESæœåŠ¡URLæˆ–æœªå¯ç”¨æ··åˆæœç´¢ï¼Œä¸åˆå§‹åŒ–ESå®¢æˆ·ç«¯")


        # åˆå§‹åŒ–LangChainæ–‡æœ¬åˆ†å‰²å™¨
        if self.use_langchain:
            self.text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap,
                length_function=len,
                separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", " ", ""]
            )

    def split_text(self, text: str) -> List[str]:
        """
        å°†é•¿æ–‡æœ¬åˆ†å‰²æˆè¾ƒå°çš„å—

        Args:
            text: è¦åˆ†å‰²çš„æ–‡æœ¬

        Returns:
            List[str]: åˆ†å‰²åçš„æ–‡æœ¬å—åˆ—è¡¨
        """
        if self.use_langchain:
            try:
                # ä½¿ç”¨LangChainçš„æ–‡æœ¬åˆ†å‰²å™¨
                chunks = self.text_splitter.split_text(text)
                # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªå—
                if not chunks:
                    chunks = [text]
                return chunks
            except Exception as e:
                app.logger.error(f"ä½¿ç”¨LangChainåˆ†å‰²æ–‡æœ¬æ—¶å‡ºé”™: {str(e)}")
                # å›é€€åˆ°ç®€å•åˆ†å‰²
                return [text]
        else:
            # ç®€å•çš„åŸºäºæ®µè½çš„åˆ†å‰²
            paragraphs = [p.strip() for p in text.split("\n\n") if p.strip()]

            # å¦‚æœæ®µè½å¾ˆå°‘ï¼Œç›´æ¥è¿”å›
            if len(paragraphs) <= 1:
                return [text]

            # åˆå¹¶çŸ­æ®µè½
            chunks = []
            current_chunk = ""

            for para in paragraphs:
                if len(current_chunk) + len(para) <= self.chunk_size:
                    current_chunk += (para + "\n\n")
                else:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = para + "\n\n"

            # æ·»åŠ æœ€åä¸€ä¸ªå—
            if current_chunk:
                chunks.append(current_chunk.strip())

            # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªå—
            if not chunks:
                chunks = [text]

            return chunks

    def generate_response(
            self,
            query: str,
            context: str,
            temperature: float = 0.7,
            max_tokens: int = 1000
    ) -> str:
        """
        æ ¹æ®æŸ¥è¯¢å’Œä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context: æ£€ç´¢çš„ä¸Šä¸‹æ–‡å†…å®¹
            temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶å›ç­”çš„åˆ›é€ æ€§ï¼Œå€¼è¶Šé«˜è¶Šåˆ›é€ æ€§
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°ï¼Œé™åˆ¶å›ç­”é•¿åº¦

        Returns:
            str: ç”Ÿæˆçš„å›ç­”
        """
        try:
            # è®¾ç½®ç³»ç»Ÿæç¤ºï¼ŒæŒ‡å¯¼å¤§æ¨¡å‹çš„è¡Œä¸º
            system_prompt = """
            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸­æ–‡æ–°é—»ä¸å…¬å‘Šæ™ºèƒ½åŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ï¼Œä¸è¦æ·»åŠ ä»»ä½•æœªåœ¨ä¸Šä¸‹æ–‡ä¸­æ˜ç¡®æåˆ°çš„ä¿¡æ¯ã€‚
            å›ç­”è¦æ±‚ï¼š
            1. ç®€æ´æ˜äº†ï¼šä¿æŒå›ç­”ç®€æ´ã€ç»“æ„æ¸…æ™°ï¼Œé‡ç‚¹çªå‡º
            2. ä¿¡æ¯å½’å› ï¼šå¼•ç”¨ä¿¡æ¯æ—¶æŒ‡æ˜æ¥æºï¼ˆä¾‹å¦‚"æ ¹æ®XXæ–°é—»æŠ¥é“/XXå…¬å‘Šé€šçŸ¥..."ï¼‰
            3. å¤„ç†ä¸ç¡®å®šæ€§ï¼šå¦‚æœä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è¶³æˆ–å­˜åœ¨çŸ›ç›¾ï¼Œæ˜ç¡®æŒ‡å‡ºå¹¶è¯´æ˜é™åˆ¶
            4. æ—¶æ•ˆæ€§æ ‡æ³¨ï¼šæåŠæ—¥æœŸå’Œæ—¶é—´ä¿¡æ¯æ—¶ï¼Œæ³¨æ˜ä¿¡æ¯çš„æ—¶é—´èƒŒæ™¯
            5. åŒºåˆ†å¤„ç†ï¼šæ–°é—»å†…å®¹ä»¥å®¢è§‚é™ˆè¿°ä¸ºä¸»ï¼Œå…¬å‘Šå†…å®¹éœ€å¼ºè°ƒå…¶å®˜æ–¹æ€§å’ŒæŒ‡å¯¼æ„ä¹‰

            å½“æ— æ³•ä»ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ—¶ï¼Œè¯·ç›´æ¥å›ç­”ï¼š"æ ¹æ®ç°æœ‰ä¿¡æ¯ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚è¯·é—®æ‚¨æ˜¯å¦æƒ³äº†è§£æˆ‘ä»¬ç³»ç»Ÿä¸­çš„å…¶ä»–æ–°é—»æˆ–å…¬å‘Šï¼Ÿ"

            å¯¹äºå¤æ‚è¯¢é—®ï¼Œå…ˆåˆ†æé—®é¢˜çš„æ ¸å¿ƒéœ€æ±‚ï¼Œå†ä»ä¸Šä¸‹æ–‡æå–ç›¸å…³ä¿¡æ¯ï¼Œç¡®ä¿å›ç­”å…¨é¢ä¸”å‡†ç¡®ã€‚
            """

            # è®¾ç½®ç”¨æˆ·æç¤ºï¼ŒåŒ…å«æŸ¥è¯¢å’Œä¸Šä¸‹æ–‡
            user_prompt = f"""ç”¨æˆ·é—®é¢˜: {query}

                ----ä¸Šä¸‹æ–‡ä¿¡æ¯----
                {context}
                ----ä¸Šä¸‹æ–‡ä¿¡æ¯ç»“æŸ----

                åŸºäºä¸Šè¿°ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¯·å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è¶³ä»¥å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºã€‚"""

            # è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹APIç”Ÿæˆå›ç­”
            response = self.llm_client.chat.completions.create(
                model=self.llm_model,  # ä½¿ç”¨é…ç½®çš„æ¨¡å‹ï¼Œå¦‚"qwen-plus"
                messages=[
                    {"role": "system", "content": system_prompt},  # ç³»ç»Ÿè§’è‰²æç¤º
                    {"role": "user", "content": user_prompt}  # ç”¨æˆ·è§’è‰²æç¤º
                ],
                temperature=temperature,  # æ§åˆ¶å¤šæ ·æ€§
                max_tokens=max_tokens  # æ§åˆ¶å›ç­”é•¿åº¦
            )

            # æå–å¹¶è¿”å›ç”Ÿæˆçš„å›ç­”å†…å®¹
            return response.choices[0].message.content

        except Exception as e:
            # å¼‚å¸¸å¤„ç†ï¼šè®°å½•é”™è¯¯å¹¶è¿”å›é”™è¯¯ä¿¡æ¯
            app.logger.error(f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}")
            return f"ç”Ÿæˆå›ç­”æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"

    def format_context(self, search_results: Dict[str, List[Dict]]) -> str:
        """
        å°†æœç´¢ç»“æœæ ¼å¼åŒ–ä¸ºä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç”¨äºLLMè¾“å…¥

        Args:
            search_results: æœç´¢ç»“æœ

        Returns:
            str: æ ¼å¼åŒ–åçš„ä¸Šä¸‹æ–‡
        """
        context = []

        # æ·»åŠ æ–°é—»
        if search_results["news"]:
            context.append("## ç›¸å…³æ–°é—»")
            for i, news in enumerate(search_results["news"]):
                context.append(f"{i + 1}. æ ‡é¢˜: {news['title']}")
                context.append(f"   æ¥æº: {news['source']} ({news['publish_date']})")
                context.append(f"   å†…å®¹: {news['content']}")
                context.append("")

        # æ·»åŠ å…¬å‘Š
        if search_results["announcements"]:
            context.append("## ç›¸å…³å…¬å‘Š")
            for i, announcement in enumerate(search_results["announcements"]):
                importance_marker = "ğŸ”´" if announcement['importance'] == "high" else "ğŸŸ¢"
                context.append(f"{i + 1}. {importance_marker} {announcement['title']}")
                context.append(f"   å‘å¸ƒ: {announcement['department']} ({announcement['publish_date']})")
                context.append(f"   å†…å®¹: {announcement['content']}")
                context.append("")

        if not context:
            return "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"

        return "\n".join(context)

    def query(
            self,
            query: str,
            n_results: int = 3,
            temperature: float = 0.7,
            max_tokens: int = 1000,
            use_hybrid_search: bool = None  # å¯é€‰å‚æ•°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼
    ) -> Dict:
        """
        ç«¯åˆ°ç«¯RAGæŸ¥è¯¢æµç¨‹

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            n_results: æ¯ç±»æ£€ç´¢çš„ç»“æœæ•°é‡
            temperature: LLMæ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            use_hybrid_search: æ˜¯å¦ä½¿ç”¨æ··åˆæœç´¢ï¼ˆå‘é‡+BM25+rerankå€’æ’ï¼‰ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨é…ç½®å€¼

        Returns:
            Dict: åŒ…å«æ£€ç´¢ç»“æœå’Œç”Ÿæˆçš„å›ç­”
        """
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        try:
            # ç¡®å®šæ˜¯å¦ä½¿ç”¨æ··åˆæœç´¢
            if use_hybrid_search is None:
                use_hybrid_search = self.use_hybrid_search and self.es_client and self.es_client.is_available

            # æ‰§è¡Œæ£€ç´¢
            if use_hybrid_search and self.es_client and self.es_client.is_available:
                search_results = self.hybrid_search_all(query, n_results)
                app.logger.info(f"ä½¿ç”¨æ··åˆæœç´¢ï¼ˆå‘é‡+BM25+rerankå€’æ’ï¼‰æ£€ç´¢ç»“æœ")
            else:
                search_results = self.search_vector(query, n_results)
                app.logger.info(f"ä½¿ç”¨çº¯å‘é‡æ£€ç´¢ç»“æœ")

            # 2. æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
            context = self.format_context(search_results)

            # 3. ç”Ÿæˆå›ç­”
            answer = self.generate_response(
                query=query,
                context=context,
                temperature=temperature,
                max_tokens=max_tokens
            )

            # 4. è¿”å›ç»“æœ
            return {
                "query": query,
                "search_results": search_results,
                "context": context,
                "answer": answer,
                "search_type": "hybrid" if use_hybrid_search else "vector"
            }
        except Exception as e:
            app.logger.error(f"æŸ¥è¯¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            # è¿”å›åŸºæœ¬å“åº”
            return {
                "query": query,
                "search_results": {"news": [], "announcements": []},
                "context": "æŸ¥è¯¢å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯",
                "answer": f"å¾ˆæŠ±æ­‰ï¼Œåœ¨å¤„ç†æ‚¨çš„æŸ¥è¯¢æ—¶å‘ç”Ÿäº†é”™è¯¯: {str(e)}ã€‚è¯·ç¨åå†è¯•æˆ–è”ç³»ç®¡ç†å‘˜ã€‚",
                "search_type": "error"
            }

    def add_news_async(self,
                       title: str,
                       content: str,
                       source: str = None,
                       publish_date: str = None,
                       tags: List[str] = None,
                       id: str = None) -> str:
        """
        å¼‚æ­¥æ·»åŠ æ–°é—»æ–‡ç« ï¼Œå¤„ç†HTMLå†…å®¹

        Args:
            title: æ–°é—»æ ‡é¢˜
            content: æ–°é—»æ­£æ–‡ï¼ˆå¯èƒ½åŒ…å«HTMLï¼‰
            source: æ–°é—»æ¥æº
            publish_date: å‘å¸ƒæ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼‰
            tags: æ ‡ç­¾åˆ—è¡¨
            id: å”¯ä¸€IDï¼Œå¦‚æœæœªæä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆ

        Returns:
            str: ä»»åŠ¡ID
        """
        # å¦‚æœæ²¡æœ‰æä¾›IDï¼Œç”Ÿæˆä¸€ä¸ª
        base_id = id or str(uuid.uuid4())

        # å‡†å¤‡è¦åœ¨çº¿ç¨‹ä¸­æ‰§è¡Œçš„ä»»åŠ¡å‡½æ•°
        def process_and_add_task():
            try:
                original_content = content
                processed_content = original_content

                # æ£€æŸ¥contentæ˜¯å¦åŒ…å«HTMLå†…å®¹
                if '<' in original_content and '>' in original_content:
                    try:
                        # ä½¿ç”¨BeautifulSoupè§£æHTMLå†…å®¹
                        soup = BeautifulSoup(original_content, 'html.parser')

                        # è·å–çº¯æ–‡æœ¬å†…å®¹
                        text_content = self.doc_processor.sanitize_html(original_content)

                        # å¤„ç†åµŒå…¥çš„å›¾ç‰‡
                        embedded_contents = []
                        for img in soup.find_all('img'):
                            src = img.get('src', '')
                            if src and (src.startswith('http://') or src.startswith('https://')):
                                try:
                                    # ä¸‹è½½å›¾ç‰‡
                                    img_response = requests.get(src, timeout=10)
                                    if img_response.status_code == 200:
                                        # å¤„ç†å›¾ç‰‡ä¸­çš„æ–‡æœ¬
                                        img_text, confidence = self.doc_processor.process_image(img_response.content)
                                        if img_text and img_text != "No text detected in image":
                                            embedded_contents.append(f"ã€å›¾ç‰‡å†…å®¹ã€‘: {img_text}")
                                except Exception as e:
                                    app.logger.warning(f"å¤„ç†åµŒå…¥å›¾ç‰‡æ—¶å‡ºé”™: {str(e)}")

                        # å¤„ç†åµŒå…¥çš„æ–‡æ¡£é“¾æ¥
                        for a in soup.find_all('a'):
                            href = a.get('href', '')
                            if href and (href.endswith('.pdf') or href.endswith('.docx') or href.endswith('.doc')):
                                try:
                                    # ä¸‹è½½æ–‡æ¡£
                                    doc_response = requests.get(href, timeout=20)
                                    if doc_response.status_code == 200:
                                        # æ ¹æ®æ–‡ä»¶ç±»å‹å¤„ç†æ–‡æ¡£
                                        if href.endswith('.pdf'):
                                            doc_text = self.doc_processor.process_pdf(doc_response.content)
                                            embedded_contents.append(f"ã€PDFæ–‡æ¡£å†…å®¹ã€‘: {doc_text}")
                                        elif href.endswith('.docx') or href.endswith('.doc'):
                                            doc_text = self.doc_processor.process_word_document(doc_response.content)
                                            embedded_contents.append(f"ã€Wordæ–‡æ¡£å†…å®¹ã€‘: {doc_text}")
                                except Exception as e:
                                    app.logger.warning(f"å¤„ç†åµŒå…¥æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")

                        # ç»„åˆæ‰€æœ‰å†…å®¹
                        if embedded_contents:
                            extracted_content = "\n\n".join(embedded_contents)
                            processed_content = f"{text_content}\n\n{extracted_content}"
                        else:
                            processed_content = text_content

                    except Exception as e:
                        app.logger.error(f"å¤„ç†HTMLå†…å®¹æ—¶å‡ºé”™: {str(e)}")
                        # å¤±è´¥æ—¶è‡³å°‘æ¸…ç†HTMLæ ‡ç­¾
                        processed_content = self.doc_processor.sanitize_html(original_content)

                # 1. å‘é‡æœåŠ¡ï¼šå‘é€åˆ°å‘é‡æœåŠ¡
                if self.vector_client and self.vector_client.is_available:
                    vector_response = self.vector_client.add_news(
                        title=title,
                        content=processed_content,
                        source=source,
                        publish_date=publish_date,
                        tags=tags,
                        id=base_id
                    )

                    if vector_response.get("success"):
                        app.logger.info(f"æˆåŠŸå°†æ–°é—»æäº¤åˆ°å‘é‡æœåŠ¡ï¼Œä»»åŠ¡ID: {vector_response.get('task_id')}")
                    else:
                        app.logger.error(f"æäº¤æ–°é—»åˆ°å‘é‡æœåŠ¡å¤±è´¥: {vector_response.get('error')}")

                # 2. ESæœåŠ¡ï¼šå¦‚æœå¯ç”¨äº†æ··åˆæœç´¢ï¼ŒåŒæ­¥åˆ°ES
                if self.es_client and self.es_client.is_available:
                    # åˆ†å‰²é•¿æ–‡æœ¬
                    document_chunks = self.split_text(processed_content)
                    app.logger.info(f"å°†æ–°é—»æ–‡ç«  '{title}' åˆ†å‰²ä¸º {len(document_chunks)} ä¸ªå—")

                    for i, chunk in enumerate(document_chunks):
                        doc_id = f"{base_id}_{i}" if len(document_chunks) > 1 else base_id

                        # å‡†å¤‡ESæ–‡æ¡£
                        es_document = {
                            "id": doc_id,
                            "title": title,
                            "content": chunk,
                            "source": source or "æœªçŸ¥æ¥æº",
                            "publish_date": publish_date or datetime.now().strftime("%Y-%m-%d"),
                            "base_id": base_id,
                            "chunk_index": i,
                            "total_chunks": len(document_chunks)
                        }

                        if tags:
                            if isinstance(tags, list):
                                es_document["tags"] = ",".join(tags)
                            else:
                                es_document["tags"] = tags

                        # å¼‚æ­¥ç´¢å¼•åˆ°ES
                        self.es_client.index_news(es_document, async_mode=True)

                    app.logger.info(f"å·²å°†æ–°é—»åŒæ­¥ç´¢å¼•åˆ°ESæœåŠ¡")

                return base_id

            except Exception as e:
                app.logger.error(f"å¼‚æ­¥æ·»åŠ æ–°é—»æ—¶å‡ºé”™: {str(e)}")
                raise

        # æäº¤åˆ°ä»»åŠ¡ç®¡ç†å™¨
        task_id = task_manager.submit_task(process_and_add_task)
        app.logger.info(f"æäº¤æ·»åŠ æ–°é—»ä»»åŠ¡: {task_id}, æ–‡æ¡£åŸºç¡€ID: {base_id}")

        return task_id

    def add_announcement_async(self,
                               title: str,
                               content: str,
                               department: str = None,
                               publish_date: str = None,
                               importance: str = "normal",
                               id: str = None) -> str:
        """
        å¼‚æ­¥æ·»åŠ å…¬å‘Šï¼Œå¤„ç†HTMLå†…å®¹

        Args:
            title: å…¬å‘Šæ ‡é¢˜
            content: å…¬å‘Šå†…å®¹ï¼ˆå¯èƒ½åŒ…å«HTMLï¼‰
            department: å‘å¸ƒéƒ¨é—¨
            publish_date: å‘å¸ƒæ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼‰
            importance: é‡è¦æ€§ï¼ˆhigh, normal, lowï¼‰
            id: å”¯ä¸€IDï¼Œå¦‚æœæœªæä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆ

        Returns:
            str: ä»»åŠ¡ID
        """
        # å¦‚æœæ²¡æœ‰æä¾›IDï¼Œç”Ÿæˆä¸€ä¸ª
        base_id = id or str(uuid.uuid4())

        # å‡†å¤‡è¦åœ¨çº¿ç¨‹ä¸­æ‰§è¡Œçš„ä»»åŠ¡å‡½æ•°
        def process_and_add_task():
            try:
                original_content = content
                processed_content = original_content

                # æ£€æŸ¥contentæ˜¯å¦åŒ…å«HTMLå†…å®¹
                if '<' in original_content and '>' in original_content:
                    try:
                        # ä½¿ç”¨BeautifulSoupè§£æHTMLå†…å®¹
                        soup = BeautifulSoup(original_content, 'html.parser')

                        # è·å–çº¯æ–‡æœ¬å†…å®¹
                        text_content = self.doc_processor.sanitize_html(original_content)

                        # å¤„ç†åµŒå…¥çš„å›¾ç‰‡
                        embedded_contents = []
                        for img in soup.find_all('img'):
                            src = img.get('src', '')
                            if src and (src.startswith('http://') or src.startswith('https://')):
                                try:
                                    # ä¸‹è½½å›¾ç‰‡
                                    img_response = requests.get(src, timeout=10)
                                    if img_response.status_code == 200:
                                        # å¤„ç†å›¾ç‰‡ä¸­çš„æ–‡æœ¬
                                        img_text, confidence = self.doc_processor.process_image(img_response.content)
                                        if img_text and img_text != "No text detected in image":
                                            embedded_contents.append(f"ã€å›¾ç‰‡å†…å®¹ã€‘: {img_text}")
                                except Exception as e:
                                    app.logger.warning(f"å¤„ç†åµŒå…¥å›¾ç‰‡æ—¶å‡ºé”™: {str(e)}")

                        # å¤„ç†åµŒå…¥çš„æ–‡æ¡£é“¾æ¥
                        for a in soup.find_all('a'):
                            href = a.get('href', '')
                            if href and (href.endswith('.pdf') or href.endswith('.docx') or href.endswith('.doc')):
                                try:
                                    # ä¸‹è½½æ–‡æ¡£
                                    doc_response = requests.get(href, timeout=20)
                                    if doc_response.status_code == 200:
                                        # æ ¹æ®æ–‡ä»¶ç±»å‹å¤„ç†æ–‡æ¡£
                                        if href.endswith('.pdf'):
                                            doc_text = self.doc_processor.process_pdf(doc_response.content)
                                            embedded_contents.append(f"ã€PDFæ–‡æ¡£å†…å®¹ã€‘: {doc_text}")
                                        elif href.endswith('.docx') or href.endswith('.doc'):
                                            doc_text = self.doc_processor.process_word_document(doc_response.content)
                                            embedded_contents.append(f"ã€Wordæ–‡æ¡£å†…å®¹ã€‘: {doc_text}")
                                except Exception as e:
                                    app.logger.warning(f"å¤„ç†åµŒå…¥æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")

                        # ç»„åˆæ‰€æœ‰å†…å®¹
                        if embedded_contents:
                            extracted_content = "\n\n".join(embedded_contents)
                            processed_content = f"{text_content}\n\n{extracted_content}"
                        else:
                            processed_content = text_content

                    except Exception as e:
                        app.logger.error(f"å¤„ç†HTMLå†…å®¹æ—¶å‡ºé”™: {str(e)}")
                        # å¤±è´¥æ—¶è‡³å°‘æ¸…ç†HTMLæ ‡ç­¾
                        processed_content = self.doc_processor.sanitize_html(original_content)

                # 1. å‘é‡æœåŠ¡ï¼šå‘é€åˆ°å‘é‡æœåŠ¡
                if self.vector_client and self.vector_client.is_available:
                    vector_response = self.vector_client.add_announcement(
                        title=title,
                        content=processed_content,
                        department=department,
                        publish_date=publish_date,
                        importance=importance,
                        id=base_id
                    )

                    if vector_response.get("success"):
                        app.logger.info(f"æˆåŠŸå°†å…¬å‘Šæäº¤åˆ°å‘é‡æœåŠ¡ï¼Œä»»åŠ¡ID: {vector_response.get('task_id')}")
                    else:
                        app.logger.error(f"æäº¤å…¬å‘Šåˆ°å‘é‡æœåŠ¡å¤±è´¥: {vector_response.get('error')}")

                # 2. ESæœåŠ¡ï¼šå¦‚æœå¯ç”¨äº†æ··åˆæœç´¢ï¼ŒåŒæ­¥åˆ°ES
                if self.es_client and self.es_client.is_available:
                    # åˆ†å‰²é•¿æ–‡æœ¬
                    document_chunks = self.split_text(processed_content)
                    app.logger.info(f"å°†å…¬å‘Š '{title}' åˆ†å‰²ä¸º {len(document_chunks)} ä¸ªå—")

                    for i, chunk in enumerate(document_chunks):
                        doc_id = f"{base_id}_{i}" if len(document_chunks) > 1 else base_id

                        # å‡†å¤‡ESæ–‡æ¡£
                        es_document = {
                            "id": doc_id,
                            "title": title,
                            "content": chunk,
                            "department": department or "æœªçŸ¥éƒ¨é—¨",
                            "publish_date": publish_date or datetime.now().strftime("%Y-%m-%d"),
                            "importance": importance,
                            "base_id": base_id,
                            "chunk_index": i,
                            "total_chunks": len(document_chunks)
                        }

                        # å¼‚æ­¥ç´¢å¼•åˆ°ES
                        self.es_client.index_notice(es_document, async_mode=True)

                    app.logger.info(f"å·²å°†å…¬å‘ŠåŒæ­¥ç´¢å¼•åˆ°ESæœåŠ¡")

                return base_id

            except Exception as e:
                app.logger.error(f"å¼‚æ­¥æ·»åŠ å…¬å‘Šæ—¶å‡ºé”™: {str(e)}")
                raise

        # æäº¤åˆ°ä»»åŠ¡ç®¡ç†å™¨
        task_id = task_manager.submit_task(process_and_add_task)
        app.logger.info(f"æäº¤æ·»åŠ å…¬å‘Šä»»åŠ¡: {task_id}, æ–‡æ¡£åŸºç¡€ID: {base_id}")

        return task_id

    def search_vector(self, query: str, n_results: int = 5, search_type: str = "all") -> Dict[str, List[Dict]]:
        """
        ä½¿ç”¨å‘é‡æœåŠ¡è¿›è¡Œæœç´¢

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            n_results: è¿”å›ç»“æœæ•°é‡
            search_type: æœç´¢ç±»å‹ (all, news, announcements)

        Returns:
            Dict[str, List[Dict]]: æœç´¢ç»“æœ
        """
        if not self.vector_client or not self.vector_client.is_available:
            app.logger.warning("å‘é‡æœåŠ¡ä¸å¯ç”¨ï¼Œæ— æ³•æ‰§è¡Œå‘é‡æœç´¢")
            return {"news": [], "announcements": []}

        # è°ƒç”¨å‘é‡æœåŠ¡å®¢æˆ·ç«¯
        return self.vector_client.search_vector(query, n_results, search_type)

    def hybrid_search_all(self, query: str, n_results: int = 5) -> Dict[str, List[Dict]]:
        """
        æ··åˆæœç´¢ï¼šä½¿ç”¨å‘é‡æ£€ç´¢å’ŒBM25èåˆæœç´¢ç»“æœ

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            n_results: æ¯ç§ç±»å‹è¿”å›çš„ç»“æœæ•°é‡

        Returns:
            Dict[str, List[Dict]]: èåˆåçš„æœç´¢ç»“æœ
        """
        # 1. å‘é‡æ£€ç´¢ - ä½¿ç”¨å‘é‡æœåŠ¡
        vector_results = self.search_vector(query, n_results)

        # 2. å¦‚æœESæœåŠ¡å¯ç”¨ï¼Œæ‰§è¡ŒBM25æ£€ç´¢
        if self.es_client and self.es_client.is_available:
            bm25_results = self.es_client.search_all(query, n_results)

            # 3. èåˆç»“æœ
            try:
                # è·å–é‡æ’åºæœåŠ¡URL
                rerank_url = RERANK_SERVICE_URL if 'RERANK_SERVICE_URL' in globals() else None

                # ä½¿ç”¨èåˆæ–¹æ³•
                fused_results = RankFusion.contextual_fusion(
                    query=query,
                    dense_results=vector_results,
                    lexical_results=bm25_results,
                    rerank_url=rerank_url
                )

                app.logger.info(f"ä½¿ç”¨{'é‡æ’åºå¢å¼ºçš„' if rerank_url else ''}æ··åˆæœç´¢ç»“æœ")
                return fused_results
            except Exception as e:
                app.logger.error(f"èåˆæœç´¢ç»“æœæ—¶å‡ºé”™: {str(e)}")
                # å‡ºé”™æ—¶å›é€€åˆ°å‘é‡æ£€ç´¢ç»“æœ
                return vector_results

        # å¦‚æœESæœåŠ¡ä¸å¯ç”¨ï¼Œç›´æ¥è¿”å›å‘é‡æ£€ç´¢ç»“æœ
        return vector_results

    def delete_news(self, doc_id: str) -> bool:
        success = True
        # 1. ä»å‘é‡æœåŠ¡åˆ é™¤
        if self.vector_client and self.vector_client.is_available:
            vector_success = self.vector_client.delete_news(doc_id)
            if not vector_success:
                success = False
        # 2. ä»ESåˆ é™¤
        if self.es_client and self.es_client.is_available:
            es_success = self.es_client.delete_news(doc_id)
            if not es_success:
                success = False
        return success

    def delete_announcement(self, doc_id: str) -> bool:
        success = True
        # 1. ä»å‘é‡æœåŠ¡åˆ é™¤
        if self.vector_client and self.vector_client.is_available:
            vector_success = self.vector_client.delete_announcement(doc_id)
            if not vector_success:
                success = False
        # 2. ä»ESåˆ é™¤
        if self.es_client and self.es_client.is_available:
            es_success = self.es_client.delete_notice(doc_id)
            if not es_success:
                success = False
        return success

# åˆå§‹åŒ–RAGç³»ç»Ÿ
# ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
VECTOR_SERVICE_URL = os.environ.get('VECTOR_SERVICE_URL', 'http://localhost:5001')
LLM_API_KEY = os.environ.get('LLM_API_KEY', 'sk-9f8124e18aa242af830c8a502c015c40')
LLM_BASE_URL = os.environ.get('LLM_BASE_URL', 'https://dashscope.aliyuncs.com/compatible-mode/v1')
LLM_MODEL = os.environ.get('LLM_MODEL', 'qwen-plus')
CHROMA_PORT = os.environ.get('CHROMA_PORT', '8000')
USE_LANGCHAIN = os.environ.get('USE_LANGCHAIN', 'true').lower() == 'true'
CHUNK_SIZE = int(os.environ.get('CHUNK_SIZE', '1000'))
CHUNK_OVERLAP = int(os.environ.get('CHUNK_OVERLAP', '200'))
USE_HYBRID_SEARCH = os.environ.get('use_hybrid_search', 'true').lower() == 'true'
ES_SERVICE_URL = os.environ.get('es_service_url', 'http://192.168.222.128:8085')
RERANK_SERVICE_URL = os.environ.get('rerank_service_url', 'http://192.168.222.128:8091')

if CHROMA_PORT and CHROMA_PORT.isdigit():
    CHROMA_PORT = int(CHROMA_PORT)
else:
    CHROMA_PORT = None

# åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–RAGç³»ç»Ÿ
rag_system = None

# æ›¿æ¢ä¹‹å‰çš„ @app.before_first_request
with app.app_context():
    try:
        app.logger.info("åˆå§‹åŒ–RAGç³»ç»Ÿ...")
        rag_system = ChineseRAGSystem(
            llm_api_key=LLM_API_KEY,
            llm_base_url=LLM_BASE_URL,
            llm_model=LLM_MODEL,
            use_langchain=USE_LANGCHAIN,
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            use_hybrid_search=USE_HYBRID_SEARCH,
            es_service_url=ES_SERVICE_URL if USE_HYBRID_SEARCH else None,
            vector_service_url=VECTOR_SERVICE_URL
        )

        app.logger.info("RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        app.logger.error(f"åˆå§‹åŒ–RAGç³»ç»Ÿæ—¶å‡ºé”™: {str(e)}")

# åˆ›å»ºä»»åŠ¡ç®¡ç†å™¨
task_manager = AsyncTaskManager(max_workers=3)  # è®¾ç½®3ä¸ªå·¥ä½œçº¿ç¨‹


# æ³¨å†Œè·¯ç”±
@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return jsonify({
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "system_info": {
            "use_langchain": USE_LANGCHAIN,
            "chunk_size": CHUNK_SIZE,
            "chunk_overlap": CHUNK_OVERLAP,
            "vector_service_available": rag_system.vector_client.is_available if rag_system else False,
            "es_service_available": rag_system.es_client.is_available if rag_system and rag_system.es_client else False
        }
    })


@app.route('/vector_status', methods=['GET'])
def vector_status_endpoint():
    """å‘é‡æœåŠ¡çŠ¶æ€æ¥å£"""
    if not rag_system:
        return jsonify({"error": "RAGç³»ç»Ÿå°šæœªåˆå§‹åŒ–"}), 500

    if rag_system.vector_client:
        is_available = rag_system.vector_client.is_available
        return jsonify({
            "vector_service_enabled": True,
            "vector_service_available": is_available,
            "vector_service_url": rag_system.vector_service_url
        })
    else:
        return jsonify({
            "vector_service_enabled": False,
            "vector_service_available": False
        })


@app.route('/query', methods=['POST'])
def query_endpoint():
    """æŸ¥è¯¢æ¥å£"""
    if not rag_system:
        return jsonify({"error": "RAGç³»ç»Ÿå°šæœªåˆå§‹åŒ–"}), 500

    data = request.json
    if not data or 'query' not in data:
        return jsonify({"error": "è¯·æä¾›æŸ¥è¯¢å†…å®¹"}), 400

    query = data['query']
    n_results = data.get('n_results', 3)
    temperature = data.get('temperature', 0.7)
    max_tokens = data.get('max_tokens', 1000)

    try:
        result = rag_system.query(
            query=query,
            n_results=n_results,
            temperature=temperature,
            max_tokens=max_tokens
        )
        # ä½¿ç”¨ensure_ascii=Falseç¡®ä¿ä¸­æ–‡å­—ç¬¦ä¸ä¼šè¢«ç¼–ç æˆUnicodeè½¬ä¹‰åºåˆ—
        return app.response_class(
            response=json.dumps(result, ensure_ascii=False),
            status=200,
            mimetype='application/json'
        )
    except Exception as e:
        app.logger.error(f"å¤„ç†æŸ¥è¯¢è¯·æ±‚æ—¶å‡ºé”™: {str(e)}")
        return jsonify({"error": str(e)}), 500


@app.route('/add/news', methods=['POST'])
def add_news_endpoint():
    """æ·»åŠ æ–°é—»æ¥å£"""
    if not rag_system:
        return jsonify({"error": "RAGç³»ç»Ÿå°šæœªåˆå§‹åŒ–"}), 500

    # æ£€æŸ¥è¡¨å•æˆ–JSONæ•°æ®
    if request.is_json:
        data = request.json
    else:
        data = request.form.to_dict()

    # éªŒè¯å¿…è¦å­—æ®µ
    if not data or 'title' not in data:
        return jsonify({"error": "è¯·æä¾›æ ‡é¢˜"}), 400

    if 'content' not in data:
        return jsonify({"error": "è¯·æä¾›å†…å®¹"}), 400

    try:
        # å‡†å¤‡æ ‡ç­¾
        tags = None
        if 'tags' in data and data['tags']:
            if isinstance(data['tags'], str):
                tags = [tag.strip() for tag in data['tags'].split(',') if tag.strip()]
            elif isinstance(data['tags'], list):
                tags = data['tags']

        # å¼‚æ­¥æ·»åŠ æ–°é—»
        task_id = rag_system.add_news_async(
            title=data['title'],
            content=data['content'],
            source=data.get('source'),
            publish_date=data.get('publish_date'),
            tags=tags
        )

        # è¿”å›ä»»åŠ¡IDå’ŒæˆåŠŸæ¶ˆæ¯
        return jsonify({
            "success": True,
            "message": "æ–°é—»æ­£åœ¨å¼‚æ­¥å¤„ç†ä¸­ï¼ŒåŒ…æ‹¬HTMLå†…å®¹æå–å’ŒåµŒå…¥å›¾ç‰‡/æ–‡æ¡£çš„å¤„ç†",
            "task_id": task_id
        })
    except Exception as e:
        app.logger.error(f"æ·»åŠ æ–°é—»æ—¶å‡ºé”™: {str(e)}")
        return jsonify({"error": str(e)}), 500


@app.route('/add/announcement', methods=['POST'])
def add_announcement_endpoint():
    """æ·»åŠ å…¬å‘Šæ¥å£"""
    if not rag_system:
        return jsonify({"error": "RAGç³»ç»Ÿå°šæœªåˆå§‹åŒ–"}), 500

    # æ£€æŸ¥è¡¨å•æˆ–JSONæ•°æ®
    if request.is_json:
        data = request.json
    else:
        data = request.form.to_dict()

    # éªŒè¯å¿…è¦å­—æ®µ
    if not data or 'title' not in data:
        return jsonify({"error": "è¯·æä¾›æ ‡é¢˜"}), 400

    if 'content' not in data:
        return jsonify({"error": "è¯·æä¾›å†…å®¹"}), 400

    try:
        # å¼‚æ­¥æ·»åŠ å…¬å‘Š
        task_id = rag_system.add_announcement_async(
            title=data['title'],
            content=data['content'],
            department=data.get('department'),
            publish_date=data.get('publish_date'),
            importance=data.get('importance', 'normal')
        )

        # è¿”å›ä»»åŠ¡IDå’ŒæˆåŠŸæ¶ˆæ¯
        return jsonify({
            "success": True,
            "message": "å…¬å‘Šæ­£åœ¨å¼‚æ­¥å¤„ç†ä¸­ï¼ŒåŒ…æ‹¬HTMLå†…å®¹æå–å’ŒåµŒå…¥å›¾ç‰‡/æ–‡æ¡£çš„å¤„ç†",
            "task_id": task_id
        })
    except Exception as e:
        app.logger.error(f"æ·»åŠ å…¬å‘Šæ—¶å‡ºé”™: {str(e)}")
        return jsonify({"error": str(e)}), 500


@app.route('/task/status/<task_id>', methods=['GET'])
def task_status_endpoint(task_id):
    """æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€æ¥å£"""
    status = task_manager.get_task_status(task_id)
    return jsonify(status)


@app.route('/delete/news/<doc_id>', methods=['DELETE'])
def delete_news_endpoint(doc_id):
    """åˆ é™¤æŒ‡å®šIDçš„æ–°é—»"""
    if not rag_system:
        return jsonify({"error": "RAGç³»ç»Ÿå°šæœªåˆå§‹åŒ–"}), 500

    try:
        success = rag_system.delete_news(doc_id)
        if success:
            return jsonify({"success": True, "message": f"æˆåŠŸåˆ é™¤æ–°é—»: {doc_id}"})
        else:
            return jsonify({"error": f"æ‰¾ä¸åˆ°IDä¸º{doc_id}çš„æ–°é—»"}), 404
    except Exception as e:
        app.logger.error(f"åˆ é™¤æ–°é—»æ—¶å‡ºé”™: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/delete/announcement/<doc_id>', methods=['DELETE'])
def delete_announcement_endpoint(doc_id):
    """åˆ é™¤æŒ‡å®šIDçš„å…¬å‘Š"""
    if not rag_system:
        return jsonify({"error": "RAGç³»ç»Ÿå°šæœªåˆå§‹åŒ–"}), 500

    try:
        success = rag_system.delete_announcement(doc_id)
        if success:
            return jsonify({"success": True, "message": f"æˆåŠŸåˆ é™¤å…¬å‘Š: {doc_id}"})
        else:
            return jsonify({"error": f"æ‰¾ä¸åˆ°IDä¸º{doc_id}çš„æ–°é—»"}), 404
    except Exception as e:
        app.logger.error(f"åˆ é™¤å…¬å‘Šæ—¶å‡ºé”™: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/process/file', methods=['POST'])
def process_file_endpoint():
    """å¤„ç†æ–‡ä»¶æå–å†…å®¹æ¥å£"""
    if not rag_system:
        return jsonify({"error": "RAGç³»ç»Ÿå°šæœªåˆå§‹åŒ–"}), 500

    if 'file' not in request.files:
        return jsonify({"error": "è¯·æä¾›æ–‡ä»¶"}), 400

    file = request.files['file']
    if file.filename == '':
        return jsonify({"error": "æœªé€‰æ‹©æ–‡ä»¶"}), 400

    try:
        file_data = file.read()
        file_type = file.content_type or os.path.splitext(file.filename)[1]

        extracted_content = rag_system.doc_processor.get_file_content(file_data, file_type)

        return jsonify({
            "success": True,
            "filename": file.filename,
            "file_type": file_type,
            "content": extracted_content,
            "content_length": len(extracted_content)
        })
    except Exception as e:
        app.logger.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        return jsonify({"error": str(e)}), 500


@app.route('/process/html', methods=['POST'])
def process_html_endpoint():
    """å¤„ç†HTMLå†…å®¹æ¥å£"""
    if not rag_system:
        return jsonify({"error": "RAGç³»ç»Ÿå°šæœªåˆå§‹åŒ–"}), 500

    data = request.json
    if not data or 'html' not in data:
        return jsonify({"error": "è¯·æä¾›HTMLå†…å®¹"}), 400

    try:
        html_content = data['html']
        sanitized_content = rag_system.doc_processor.sanitize_html(html_content)

        return jsonify({
            "success": True,
            "content": sanitized_content,
            "content_length": len(sanitized_content)
        })
    except Exception as e:
        app.logger.error(f"å¤„ç†HTMLå†…å®¹æ—¶å‡ºé”™: {str(e)}")
        return jsonify({"error": str(e)}), 500


@app.route('/hybrid_query', methods=['POST'])
def hybrid_query_endpoint():
    """æ··åˆæŸ¥è¯¢æ¥å£ï¼ˆå‘é‡+BM25+rerankå€’æ’æ’åºï¼‰"""
    if not rag_system:
        return jsonify({"error": "RAGç³»ç»Ÿå°šæœªåˆå§‹åŒ–"}), 500

    data = request.json
    if not data or 'query' not in data:
        return jsonify({"error": "è¯·æä¾›æŸ¥è¯¢å†…å®¹"}), 400

    query = data['query']
    n_results = data.get('n_results', 3)
    temperature = data.get('temperature', 0.7)
    max_tokens = data.get('max_tokens', 1000)

    try:
        result = rag_system.query(
            query=query,
            n_results=n_results,
            temperature=temperature,
            max_tokens=max_tokens,
            use_hybrid_search=True  # å¼ºåˆ¶ä½¿ç”¨æ··åˆæœç´¢
        )
        # ä½¿ç”¨ensure_ascii=Falseç¡®ä¿ä¸­æ–‡å­—ç¬¦ä¸ä¼šè¢«ç¼–ç æˆUnicodeè½¬ä¹‰åºåˆ—
        return app.response_class(
            response=json.dumps(result, ensure_ascii=False),
            status=200,
            mimetype='application/json'
        )
    except Exception as e:
        app.logger.error(f"å¤„ç†æ··åˆæŸ¥è¯¢è¯·æ±‚æ—¶å‡ºé”™: {str(e)}")
        return jsonify({"error": str(e)}), 500


@app.route('/es_status', methods=['GET'])
def es_status_endpoint():
    """ESæœåŠ¡çŠ¶æ€æ¥å£"""
    if not rag_system:
        return jsonify({"error": "RAGç³»ç»Ÿå°šæœªåˆå§‹åŒ–"}), 500

    if rag_system.es_client:
        is_available = rag_system.es_client.is_available
        return jsonify({
            "es_service_enabled": True,
            "es_service_available": is_available,
            "es_service_url": rag_system.es_service_url,
            "hybrid_search_enabled": rag_system.use_hybrid_search
        })
    else:
        return jsonify({
            "es_service_enabled": False,
            "es_service_available": False,
            "hybrid_search_enabled": rag_system.use_hybrid_search
        })


# å¯åŠ¨å‡½æ•°
if __name__ == "__main__":
    # å¦‚æœå­˜åœ¨PORTç¯å¢ƒå˜é‡ï¼Œä½¿ç”¨å®ƒï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤çš„5000
    port = int(os.environ.get("PORT", 5000))
    # åœ¨å¼€å‘æ¨¡å¼ä¸‹å¯ç”¨è°ƒè¯•ï¼Œç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨ gunicorn
    debug = os.environ.get("FLASK_ENV") == "development"

    app.run(host="0.0.0.0", port=port, debug=debug)
