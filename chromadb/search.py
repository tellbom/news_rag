from flask import Flask, request, jsonify
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from openai import OpenAI
import pandas as pd
import uuid
from typing import List, Dict, Optional, Union, Any, Tuple
import json
from datetime import datetime
import os
import numpy as np
from bs4 import BeautifulSoup
from PIL import Image
from io import BytesIO
import requests
from paddleocr import PaddleOCR
import mammoth
import PyPDF2
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document as LCDocument
import tempfile

from flask import Flask
app = Flask(__name__)

class DocumentProcessor:
    """Document processing utilities for different file types"""
    
    def __init__(self):
        """Initialize document processor with OCR model"""
        # Handle numpy compatibility issue
        if not hasattr(np, 'int'):
            np.int = np.int32
        
        # Initialize OCR model
        try:
            self.ocr = PaddleOCR(
                det_model_dir="/root/.paddleocr/whl/det/ch/ch_PP-OCRv3_det_infer",
                rec_model_dir="/root/.paddleocr/whl/rec/ch/ch_PP-OCRv3_rec_infer",
                cls_model_dir="/root/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer",
                use_angle_cls=True,
                lang="ch"
            )
            app.logger.info("OCR model initialized successfully")
        except Exception as e:
            self.ocr = None
            app.logger.error(f"Failed to initialize OCR model: {str(e)}")
    
    def sanitize_html(self, html_content: str) -> str:
        """
        Sanitize HTML content to extract clean text
        
        Args:
            html_content: HTML string to sanitize
            
        Returns:
            str: Clean text extracted from HTML
        """
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Remove script and style elements
            for script in soup(["script", "style"]):
                script.extract()
                
            # Get text
            text = soup.get_text(separator=' ', strip=True)
            
            # Clean up whitespace
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = '\n'.join(chunk for chunk in chunks if chunk)
            
            return text
        except Exception as e:
            app.logger.error(f"HTML sanitization error: {str(e)}")
            # Return the original content if sanitization fails
            return html_content

    def process_image(self, image_data: bytes) -> Tuple[str, float]:
        """
        Extract text from image using OCR
        
        Args:
            image_data: Binary image data
            
        Returns:
            Tuple[str, float]: Extracted text and average confidence score
        """
        if self.ocr is None:
            app.logger.error("OCR model not initialized")
            return "OCR model not available", 0.0
            
        try:
            # Convert bytes to PIL Image
            img = Image.open(BytesIO(image_data))
            
            # Convert to numpy array
            img_np = np.array(img)
            
            # Run OCR
            result = self.ocr.ocr(img_np, cls=True)
            
            # Extract text and calculate average confidence
            text_parts = []
            confidence_sum = 0.0
            count = 0
            
            if result:
                for line in result:
                    if isinstance(line, list) and line and len(line) > 0:
                        if len(line[-1]) >= 2:
                            text = line[-1][0]  # Text content
                            confidence = float(line[-1][1])  # Confidence score
                            
                            if text and confidence > 0.5:  # Only include reasonably confident results
                                text_parts.append(text)
                                confidence_sum += confidence
                                count += 1
            
            # Calculate average confidence if there are valid detections
            avg_confidence = confidence_sum / count if count > 0 else 0.0
            full_text = "\n".join(text_parts)
            
            if not full_text.strip():
                return "No text detected in image", 0.0
                
            return full_text, avg_confidence
        
        except Exception as e:
            app.logger.error(f"Image processing error: {str(e)}")
            return f"Failed to process image: {str(e)}", 0.0

    def process_word_document(self, docx_data: bytes) -> str:
        """
        Extract text from Word document
        
        Args:
            docx_data: Binary Word document data
            
        Returns:
            str: Extracted text
        """
        try:
            result = mammoth.extract_raw_text(BytesIO(docx_data))
            return result.value
        except Exception as e:
            app.logger.error(f"Word document processing error: {str(e)}")
            return f"Failed to process Word document: {str(e)}"

    def process_pdf(self, pdf_data: bytes) -> str:
        """
        Extract text from PDF document
        
        Args:
            pdf_data: Binary PDF data
            
        Returns:
            str: Extracted text
        """
        try:
            with BytesIO(pdf_data) as pdf_file:
                pdf_reader = PyPDF2.PdfReader(pdf_file)
                text = []
                
                for page_num in range(len(pdf_reader.pages)):
                    page = pdf_reader.pages[page_num]
                    text.append(page.extract_text())
                
            return "\n\n".join(text)
        except Exception as e:
            app.logger.error(f"PDF processing error: {str(e)}")
            return f"Failed to process PDF: {str(e)}"

    def get_file_content(self, file_data: bytes, file_type: str) -> str:
        """
        Process file based on its type and return text content
        
        Args:
            file_data: Binary file data
            file_type: MIME type or file extension
            
        Returns:
            str: Extracted text content
        """
        file_type = file_type.lower()
        
        if file_type in ['image/jpeg', 'image/png', 'image/jpg', 'image/gif', 'image/bmp']:
            text, confidence = self.process_image(file_data)
            return text
        elif file_type in ['application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'application/msword', '.docx', '.doc']:
            return self.process_word_document(file_data)
        elif file_type in ['application/pdf', '.pdf']:
            return self.process_pdf(file_data)
        elif file_type in ['text/html', '.html', '.htm']:
            return self.sanitize_html(file_data.decode('utf-8', errors='replace'))
        elif file_type in ['text/plain', '.txt']:
            return file_data.decode('utf-8', errors='replace')
        else:
            return f"Unsupported file type: {file_type}"


class ChineseRAGSystem:
    def __init__(
        self, 
        embedding_model_path: str = "/models/sentence-transformers_text2vec-large-chinese",
        llm_api_key: str = None,
        llm_base_url: str = "https://dashscope.aliyuncs.com/compatible-mode/v1",
        llm_model: str = "qwen-plus",
        chroma_host: str = None, 
        chroma_port: int = None,
        use_langchain: bool = True,
        chunk_size: int = 1000,
        chunk_overlap: int = 200
    ):
        """
        åˆå§‹åŒ–ä¸­æ–‡RAGç³»ç»Ÿ
        
        Args:
            embedding_model_path: åµŒå…¥æ¨¡å‹çš„æœ¬åœ°è·¯å¾„
            llm_api_key: å¤§æ¨¡å‹APIå¯†é’¥
            llm_base_url: å¤§æ¨¡å‹APIåŸºç¡€URL
            llm_model: ä½¿ç”¨çš„å¤§æ¨¡å‹åç§°
            chroma_host: ChromaDBæœåŠ¡å™¨åœ°å€ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æœ¬åœ°å†…å­˜æ¨¡å¼
            chroma_port: ChromaDBæœåŠ¡å™¨ç«¯å£
            use_langchain: æ˜¯å¦ä½¿ç”¨LangChainè¿›è¡Œæ–‡æ¡£åˆ†å—
            chunk_size: æ–‡æ¡£åˆ†å—å¤§å°
            chunk_overlap: æ–‡æ¡£åˆ†å—é‡å éƒ¨åˆ†å¤§å°
        """
        # åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨
        self.doc_processor = DocumentProcessor()
        
        # æ˜¯å¦ä½¿ç”¨LangChain
        self.use_langchain = use_langchain
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # åŠ è½½åµŒå…¥æ¨¡å‹
        app.logger.info(f"æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹: {embedding_model_path}...")
        self.embedding_model = SentenceTransformer(
            model_name_or_path=embedding_model_path,
            local_files_only=True
        )
        self.vector_dim = self.embedding_model.get_sentence_embedding_dimension()
        app.logger.info(f"åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆï¼Œå‘é‡ç»´åº¦ï¼š{self.vector_dim}")
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        self.llm_api_key = llm_api_key or os.environ.get("LLM_API_KEY")
        self.llm_client = OpenAI(
            api_key=self.llm_api_key,
            base_url=llm_base_url
        )
        self.llm_model = llm_model
        app.logger.info(f"LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {llm_model}")
        
        # è®¾ç½®ChromaDBå®¢æˆ·ç«¯
        try:
            if chroma_host and chroma_port:
                app.logger.info(f"å°è¯•è¿æ¥åˆ°è¿œç¨‹ChromaDB: {chroma_host}:{chroma_port}")
                self.db_client = chromadb.HttpClient(
                    host=chroma_host,
                    port=chroma_port,
                    settings=Settings(anonymized_telemetry=False)
                )
                # å°è¯•ä¸€ä¸ªç®€å•æ“ä½œæ¥æµ‹è¯•è¿æ¥
                self.db_client.heartbeat()
                app.logger.info("æˆåŠŸè¿æ¥åˆ°ChromaDBæœåŠ¡å™¨")
            else:
                app.logger.info("ä½¿ç”¨å†…å­˜æ¨¡å¼ChromaDB")
                self.db_client = chromadb.EphemeralClient(
                    settings=Settings(anonymized_telemetry=False)
                )
        except Exception as e:
            app.logger.error(f"è¿æ¥ChromaDBæ—¶å‡ºé”™: {str(e)}")
            app.logger.info("å›é€€åˆ°å†…å­˜æ¨¡å¼ChromaDB")
            self.db_client = chromadb.EphemeralClient(
                settings=Settings(anonymized_telemetry=False)
            )
        
        # ç›´æ¥åˆ›å»ºé›†åˆï¼Œä¸è®¾ç½®embedding_functionï¼Œæˆ‘ä»¬å°†æ‰‹åŠ¨è®¡ç®—åµŒå…¥å‘é‡
        try:
            self.news_collection = self.db_client.get_or_create_collection(
                name="cooper_news"
            )
            
            self.announcement_collection = self.db_client.get_or_create_collection(
                name="cooper_notice"
            )
            app.logger.info("æˆåŠŸåˆ›å»º/è·å–é›†åˆ")
        except Exception as e:
            app.logger.error(f"åˆ›å»ºé›†åˆæ—¶å‡ºé”™: {str(e)}")
            raise
            
        # åˆå§‹åŒ–LangChainæ–‡æœ¬åˆ†å‰²å™¨
        if self.use_langchain:
            self.text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap,
                length_function=len,
                separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", " ", ""]
            )
    
    def compute_embeddings(self, texts):
        """è®¡ç®—æ–‡æœ¬çš„åµŒå…¥å‘é‡"""
        try:
            # ç¡®ä¿è¾“å…¥æ˜¯åˆ—è¡¨
            if not isinstance(texts, list):
                texts = [texts]
                
            # è®¡ç®—åµŒå…¥å‘é‡
            embeddings = self.embedding_model.encode(texts)
            
            # è½¬æ¢ä¸ºåˆ—è¡¨
            if isinstance(embeddings, np.ndarray):
                return embeddings.tolist()
            elif isinstance(embeddings, list):
                # æ£€æŸ¥æ˜¯å¦æ˜¯numpyæ•°ç»„çš„åˆ—è¡¨
                if embeddings and isinstance(embeddings[0], np.ndarray):
                    return [emb.tolist() for emb in embeddings]
                return embeddings
            else:
                app.logger.warning(f"Warning: Unexpected embedding type: {type(embeddings)}")
                return [[0.0] * self.vector_dim]  # è¿”å›é›¶å‘é‡ä½œä¸ºåå¤‡
        except Exception as e:
            app.logger.error(f"è®¡ç®—åµŒå…¥å‘é‡æ—¶å‡ºé”™: {str(e)}")
            return [[0.0] * self.vector_dim]  # è¿”å›é›¶å‘é‡ä½œä¸ºåå¤‡
    
    def split_text(self, text: str) -> List[str]:
        """
        å°†é•¿æ–‡æœ¬åˆ†å‰²æˆè¾ƒå°çš„å—
        
        Args:
            text: è¦åˆ†å‰²çš„æ–‡æœ¬
            
        Returns:
            List[str]: åˆ†å‰²åçš„æ–‡æœ¬å—åˆ—è¡¨
        """
        if self.use_langchain:
            try:
                # ä½¿ç”¨LangChainçš„æ–‡æœ¬åˆ†å‰²å™¨
                chunks = self.text_splitter.split_text(text)
                # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªå—
                if not chunks:
                    chunks = [text]
                return chunks
            except Exception as e:
                app.logger.error(f"ä½¿ç”¨LangChainåˆ†å‰²æ–‡æœ¬æ—¶å‡ºé”™: {str(e)}")
                # å›é€€åˆ°ç®€å•åˆ†å‰²
                return [text]
        else:
            # ç®€å•çš„åŸºäºæ®µè½çš„åˆ†å‰²
            paragraphs = [p.strip() for p in text.split("\n\n") if p.strip()]
            
            # å¦‚æœæ®µè½å¾ˆå°‘ï¼Œç›´æ¥è¿”å›
            if len(paragraphs) <= 1:
                return [text]
                
            # åˆå¹¶çŸ­æ®µè½
            chunks = []
            current_chunk = ""
            
            for para in paragraphs:
                if len(current_chunk) + len(para) <= self.chunk_size:
                    current_chunk += (para + "\n\n")
                else:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = para + "\n\n"
            
            # æ·»åŠ æœ€åä¸€ä¸ªå—
            if current_chunk:
                chunks.append(current_chunk.strip())
            
            # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªå—
            if not chunks:
                chunks = [text]
                
            return chunks
    
    def add_news(self, 
                title: str, 
                content: str, 
                source: str = None, 
                publish_date: str = None, 
                tags: List[str] = None, 
                id: str = None,
                file_data: bytes = None,
                file_type: str = None) -> str:
        """
        æ·»åŠ æ–°é—»æ–‡ç« 
        
        Args:
            title: æ–°é—»æ ‡é¢˜
            content: æ–°é—»æ­£æ–‡
            source: æ–°é—»æ¥æº
            publish_date: å‘å¸ƒæ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼‰
            tags: æ ‡ç­¾åˆ—è¡¨
            id: å”¯ä¸€IDï¼Œå¦‚æœæœªæä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆ
            file_data: æ–‡ä»¶æ•°æ®
            file_type: æ–‡ä»¶ç±»å‹
            
        Returns:
            str: æ–‡æ¡£ID
        """
        # å¦‚æœæä¾›äº†æ–‡ä»¶ï¼Œå¤„ç†æ–‡ä»¶å†…å®¹
        if file_data and file_type:
            file_content = self.doc_processor.get_file_content(file_data, file_type)
            # å¦‚æœcontentä¸ºç©ºï¼Œç›´æ¥ä½¿ç”¨æ–‡ä»¶å†…å®¹
            if not content or content.strip() == "":
                content = file_content
            # å¦åˆ™å°†æ–‡ä»¶å†…å®¹æ·»åŠ åˆ°ç°æœ‰å†…å®¹
            else:
                content = f"{content}\n\n{file_content}"
        
        # å‡†å¤‡å…ƒæ•°æ®
        metadata = {
            "title": title,
            "source": source or "æœªçŸ¥æ¥æº",
            "publish_date": publish_date or datetime.now().strftime("%Y-%m-%d"),
            "type": "news",
        }
        
        if tags:
            metadata["tags"] = ",".join(tags)
        
        # å¦‚æœæ²¡æœ‰æä¾›IDï¼Œç”Ÿæˆä¸€ä¸ª
        base_id = id or str(uuid.uuid4())
        
        # åˆ†å‰²é•¿æ–‡æœ¬
        document_chunks = self.split_text(content)
        app.logger.info(f"å°†æ–°é—»æ–‡ç«  '{title}' åˆ†å‰²ä¸º {len(document_chunks)} ä¸ªå—")
        
        all_doc_ids = []
        
        for i, chunk in enumerate(document_chunks):
            # ä¸ºæ¯ä¸ªå—ç”Ÿæˆå”¯ä¸€ID
            doc_id = f"{base_id}_{i}" if len(document_chunks) > 1 else base_id
            
            # å‡†å¤‡æ–‡æ¡£å†…å®¹ (ä¸ºå—æ·»åŠ æ ‡é¢˜ä»¥ä¿æŒä¸Šä¸‹æ–‡)
            document = f"{title}\n{chunk}"
            
            # ä¸ºè¿™ä¸ªå—æ›´æ–°å…ƒæ•°æ®
            chunk_metadata = metadata.copy()
            chunk_metadata["chunk_index"] = i
            chunk_metadata["total_chunks"] = len(document_chunks)
            chunk_metadata["base_id"] = base_id
            
            # è®¡ç®—åµŒå…¥å‘é‡
            embeddings = self.compute_embeddings([document])
            
            # æ·»åŠ åˆ°é›†åˆ
            try:
                self.news_collection.add(
                    documents=[document],
                    metadatas=[chunk_metadata],
                    ids=[doc_id],
                    embeddings=embeddings
                )
                all_doc_ids.append(doc_id)
            except Exception as e:
                app.logger.error(f"æ·»åŠ æ–°é—»å— {i+1}/{len(document_chunks)} æ—¶å‡ºé”™: {str(e)}")
                # ç»§ç»­æ·»åŠ å…¶ä»–å—
                continue
        
        if all_doc_ids:
            app.logger.info(f"æˆåŠŸæ·»åŠ æ–°é—»: {title}ï¼Œåˆ†ä¸º {len(all_doc_ids)}/{len(document_chunks)} ä¸ªå—")
            return base_id
        else:
            raise Exception("æ·»åŠ æ–°é—»æ—¶æ‰€æœ‰å—éƒ½å¤±è´¥")
    
    def add_announcement(self, 
                       title: str, 
                       content: str, 
                       department: str = None, 
                       publish_date: str = None,
                       importance: str = "normal",
                       id: str = None,
                       file_data: bytes = None,
                       file_type: str = None) -> str:
        """
        æ·»åŠ å…¬å‘Š
        
        Args:
            title: å…¬å‘Šæ ‡é¢˜
            content: å…¬å‘Šå†…å®¹
            department: å‘å¸ƒéƒ¨é—¨
            publish_date: å‘å¸ƒæ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼‰
            importance: é‡è¦æ€§ï¼ˆhigh, normal, lowï¼‰
            id: å”¯ä¸€IDï¼Œå¦‚æœæœªæä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆ
            file_data: æ–‡ä»¶æ•°æ®
            file_type: æ–‡ä»¶ç±»å‹
            
        Returns:
            str: æ–‡æ¡£ID
        """
        # å¦‚æœæä¾›äº†æ–‡ä»¶ï¼Œå¤„ç†æ–‡ä»¶å†…å®¹
        if file_data and file_type:
            file_content = self.doc_processor.get_file_content(file_data, file_type)
            # å¦‚æœcontentä¸ºç©ºï¼Œç›´æ¥ä½¿ç”¨æ–‡ä»¶å†…å®¹
            if not content or content.strip() == "":
                content = file_content
            # å¦åˆ™å°†æ–‡ä»¶å†…å®¹æ·»åŠ åˆ°ç°æœ‰å†…å®¹
            else:
                content = f"{content}\n\n{file_content}"
        
        # å‡†å¤‡å…ƒæ•°æ®
        metadata = {
            "title": title,
            "department": department or "æœªçŸ¥éƒ¨é—¨",
            "publish_date": publish_date or datetime.now().strftime("%Y-%m-%d"),
            "importance": importance,
            "type": "announcement",
        }
        
        # å¦‚æœæ²¡æœ‰æä¾›IDï¼Œç”Ÿæˆä¸€ä¸ª
        base_id = id or str(uuid.uuid4())
        
        # åˆ†å‰²é•¿æ–‡æœ¬
        document_chunks = self.split_text(content)
        app.logger.info(f"å°†å…¬å‘Š '{title}' åˆ†å‰²ä¸º {len(document_chunks)} ä¸ªå—")
        
        all_doc_ids = []
        
        for i, chunk in enumerate(document_chunks):
            # ä¸ºæ¯ä¸ªå—ç”Ÿæˆå”¯ä¸€ID
            doc_id = f"{base_id}_{i}" if len(document_chunks) > 1 else base_id
            
            # å‡†å¤‡æ–‡æ¡£å†…å®¹ (ä¸ºå—æ·»åŠ æ ‡é¢˜ä»¥ä¿æŒä¸Šä¸‹æ–‡)
            document = f"{title}\n{chunk}"
            
            # ä¸ºè¿™ä¸ªå—æ›´æ–°å…ƒæ•°æ®
            chunk_metadata = metadata.copy()
            chunk_metadata["chunk_index"] = i
            chunk_metadata["total_chunks"] = len(document_chunks)
            chunk_metadata["base_id"] = base_id
            
            # è®¡ç®—åµŒå…¥å‘é‡
            embeddings = self.compute_embeddings([document])
            
            # æ·»åŠ åˆ°é›†åˆ
            try:
                self.announcement_collection.add(
                    documents=[document],
                    metadatas=[chunk_metadata],
                    ids=[doc_id],
                    embeddings=embeddings
                )
                all_doc_ids.append(doc_id)
            except Exception as e:
                app.logger.error(f"æ·»åŠ å…¬å‘Šå— {i+1}/{len(document_chunks)} æ—¶å‡ºé”™: {str(e)}")
                # ç»§ç»­æ·»åŠ å…¶ä»–å—
                continue
        
        if all_doc_ids:
            app.logger.info(f"æˆåŠŸæ·»åŠ å…¬å‘Š: {title}ï¼Œåˆ†ä¸º {len(all_doc_ids)}/{len(document_chunks)} ä¸ªå—")
            return base_id
        else:
            raise Exception("æ·»åŠ å…¬å‘Šæ—¶æ‰€æœ‰å—éƒ½å¤±è´¥")
    
    def search_news(self, query: str, n_results: int = 5) -> List[Dict]:
        """
        æœç´¢æ–°é—»
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            n_results: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            List[Dict]: æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            # è®¡ç®—æŸ¥è¯¢çš„åµŒå…¥å‘é‡
            query_embedding = self.compute_embeddings([query])[0]
            
            # ä½¿ç”¨å‘é‡æœç´¢
            results = self.news_collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results * 2,  # è·å–æ›´å¤šç»“æœï¼Œå› ä¸ºå¯èƒ½æœ‰é‡å¤çš„base_id
                include=["metadatas", "documents", "distances"]
            )
            
            # æ ¼å¼åŒ–ç»“æœ
            formatted_results = []
            if results["ids"] and results["ids"][0]:
                # æ”¶é›†ä¸åŒbase_idçš„ç¬¬ä¸€ä¸ªç»“æœ
                seen_base_ids = set()
                
                for i in range(len(results["ids"][0])):
                    base_id = results["metadatas"][0][i].get("base_id", results["ids"][0][i])
                    
                    # å¦‚æœå·²ç»åŒ…å«äº†è¿™ä¸ªbase_idçš„æ–‡æ¡£ï¼Œåˆ™è·³è¿‡
                    if base_id in seen_base_ids:
                        continue
                    
                    seen_base_ids.add(base_id)
                    
                    formatted_results.append({
                        "id": base_id,
                        "title": results["metadatas"][0][i]["title"],
                        "source": results["metadatas"][0][i]["source"],
                        "publish_date": results["metadatas"][0][i]["publish_date"],
                        "content": results["documents"][0][i],
                        "relevance_score": 1 - float(results["distances"][0][i]) if results["distances"] else 0.0
                    })
                    
                    # å¦‚æœå·²ç»æ”¶é›†äº†è¶³å¤Ÿçš„ä¸åŒæ–‡æ¡£ï¼Œå°±åœæ­¢
                    if len(formatted_results) >= n_results:
                        break
            
            return formatted_results
        except Exception as e:
            app.logger.error(f"æœç´¢æ–°é—»æ—¶å‡ºé”™: {str(e)}")
            return []
    
    def search_announcements(self, query: str, n_results: int = 5) -> List[Dict]:
        """
        æœç´¢å…¬å‘Š
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            n_results: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            List[Dict]: æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            # è®¡ç®—æŸ¥è¯¢çš„åµŒå…¥å‘é‡
            query_embedding = self.compute_embeddings([query])[0]
            
            # ä½¿ç”¨å‘é‡æœç´¢
            results = self.announcement_collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results * 2,  # è·å–æ›´å¤šç»“æœï¼Œå› ä¸ºå¯èƒ½æœ‰é‡å¤çš„base_id
                include=["metadatas", "documents", "distances"]
            )
            
            # æ ¼å¼åŒ–ç»“æœ
            formatted_results = []
            if results["ids"] and results["ids"][0]:
                # æ”¶é›†ä¸åŒbase_idçš„ç¬¬ä¸€ä¸ªç»“æœ
                seen_base_ids = set()
                
                for i in range(len(results["ids"][0])):
                    base_id = results["metadatas"][0][i].get("base_id", results["ids"][0][i])
                    
                    # å¦‚æœå·²ç»åŒ…å«äº†è¿™ä¸ªbase_idçš„æ–‡æ¡£ï¼Œåˆ™è·³è¿‡
                    if base_id in seen_base_ids:
                        continue
                    
                    seen_base_ids.add(base_id)
                    
                    formatted_results.append({
                        "id": base_id,
                        "title": results["metadatas"][0][i]["title"],
                        "department": results["metadatas"][0][i]["department"],
                        "publish_date": results["metadatas"][0][i]["publish_date"],
                        "importance": results["metadatas"][0][i]["importance"],
                        "content": results["documents"][0][i],
                        "relevance_score": 1 - float(results["distances"][0][i]) if results["distances"] else 0.0
                    })
                    
                    # å¦‚æœå·²ç»æ”¶é›†äº†è¶³å¤Ÿçš„ä¸åŒæ–‡æ¡£ï¼Œå°±åœæ­¢
                    if len(formatted_results) >= n_results:
                        break
            
            return formatted_results
        except Exception as e:
            app.logger.error(f"æœç´¢å…¬å‘Šæ—¶å‡ºé”™: {str(e)}")
            return []
    
    def search_all(self, query: str, n_results: int = 5) -> Dict[str, List[Dict]]:
        """
        åŒæ—¶æœç´¢æ–°é—»å’Œå…¬å‘Š
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            n_results: æ¯ç§ç±»å‹è¿”å›çš„ç»“æœæ•°é‡
            
        Returns:
            Dict[str, List[Dict]]: æœç´¢ç»“æœå­—å…¸ï¼ŒåŒ…å«æ–°é—»å’Œå…¬å‘Š
        """
        news_results = self.search_news(query, n_results)
        announcement_results = self.search_announcements(query, n_results)
        
        return {
            "news": news_results,
            "announcements": announcement_results
        }
    
    def generate_response(
        self, 
        query: str, 
        context: str, 
        temperature: float = 0.7,
        max_tokens: int = 1000
    ) -> str:
        """
        æ ¹æ®æŸ¥è¯¢å’Œä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context: æ£€ç´¢çš„ä¸Šä¸‹æ–‡å†…å®¹
            temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶å›ç­”çš„åˆ›é€ æ€§ï¼Œå€¼è¶Šé«˜è¶Šåˆ›é€ æ€§
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°ï¼Œé™åˆ¶å›ç­”é•¿åº¦
            
        Returns:
            str: ç”Ÿæˆçš„å›ç­”
        """
        try:
            # è®¾ç½®ç³»ç»Ÿæç¤ºï¼ŒæŒ‡å¯¼å¤§æ¨¡å‹çš„è¡Œä¸º
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªåŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)çš„æ™ºèƒ½åŠ©æ‰‹ã€‚ä½ çš„å›ç­”å°†åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
            å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰è¶³å¤Ÿçš„ä¿¡æ¯ï¼Œè¯·å¦è¯šåœ°è¡¨ç¤ºä½ ä¸çŸ¥é“ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯ã€‚
            å¦‚æœç”¨æˆ·è¯¢é—®çš„å†…å®¹ä¸ä¸Šä¸‹æ–‡æ— å…³ï¼Œè¯·ç¤¼è²Œåœ°å¼•å¯¼ç”¨æˆ·æé—®ä¸æ–°é—»æˆ–å…¬å‘Šç›¸å…³çš„é—®é¢˜ã€‚"""
            
            # è®¾ç½®ç”¨æˆ·æç¤ºï¼ŒåŒ…å«æŸ¥è¯¢å’Œä¸Šä¸‹æ–‡
            user_prompt = f"""ç”¨æˆ·é—®é¢˜: {query}
                        
                ----ä¸Šä¸‹æ–‡ä¿¡æ¯----
                {context}
                ----ä¸Šä¸‹æ–‡ä¿¡æ¯ç»“æŸ----

                åŸºäºä¸Šè¿°ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¯·å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è¶³ä»¥å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºã€‚"""

            # è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹APIç”Ÿæˆå›ç­”
            response = self.llm_client.chat.completions.create(
                model=self.llm_model,  # ä½¿ç”¨é…ç½®çš„æ¨¡å‹ï¼Œå¦‚"qwen-plus"
                messages=[
                    {"role": "system", "content": system_prompt},  # ç³»ç»Ÿè§’è‰²æç¤º
                    {"role": "user", "content": user_prompt}       # ç”¨æˆ·è§’è‰²æç¤º
                ],
                temperature=temperature,  # æ§åˆ¶å¤šæ ·æ€§
                max_tokens=max_tokens     # æ§åˆ¶å›ç­”é•¿åº¦
            )
            
            # æå–å¹¶è¿”å›ç”Ÿæˆçš„å›ç­”å†…å®¹
            return response.choices[0].message.content
        
        except Exception as e:
            # å¼‚å¸¸å¤„ç†ï¼šè®°å½•é”™è¯¯å¹¶è¿”å›é”™è¯¯ä¿¡æ¯
            app.logger.error(f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}")
            return f"ç”Ÿæˆå›ç­”æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
           
    def format_context(self, search_results: Dict[str, List[Dict]]) -> str:
        """
        å°†æœç´¢ç»“æœæ ¼å¼åŒ–ä¸ºä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç”¨äºLLMè¾“å…¥
        
        Args:
            search_results: æœç´¢ç»“æœ
            
        Returns:
            str: æ ¼å¼åŒ–åçš„ä¸Šä¸‹æ–‡
        """
        context = []
        
        # æ·»åŠ æ–°é—»
        if search_results["news"]:
            context.append("## ç›¸å…³æ–°é—»")
            for i, news in enumerate(search_results["news"]):
                context.append(f"{i+1}. æ ‡é¢˜: {news['title']}")
                context.append(f"   æ¥æº: {news['source']} ({news['publish_date']})")
                context.append(f"   å†…å®¹: {news['content']}")
                context.append("")
        
        # æ·»åŠ å…¬å‘Š
        if search_results["announcements"]:
            context.append("## ç›¸å…³å…¬å‘Š")
            for i, announcement in enumerate(search_results["announcements"]):
                importance_marker = "ğŸ”´" if announcement['importance'] == "high" else "ğŸŸ¢"
                context.append(f"{i+1}. {importance_marker} {announcement['title']}")
                context.append(f"   å‘å¸ƒ: {announcement['department']} ({announcement['publish_date']})")
                context.append(f"   å†…å®¹: {announcement['content']}")
                context.append("")
        
        if not context:
            return "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
            
        return "\n".join(context)
            
    def query(
        self, 
        query: str, 
        n_results: int = 3,
        temperature: float = 0.7, 
        max_tokens: int = 1000
    ) -> Dict:
        """
        ç«¯åˆ°ç«¯RAGæŸ¥è¯¢æµç¨‹
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            n_results: æ¯ç±»æ£€ç´¢çš„ç»“æœæ•°é‡
            temperature: LLMæ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            
        Returns:
            Dict: åŒ…å«æ£€ç´¢ç»“æœå’Œç”Ÿæˆçš„å›ç­”
        """
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        try:
            search_results = self.search_all(query, n_results)
            
            # 2. æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
            context = self.format_context(search_results)
            
            # 3. ç”Ÿæˆå›ç­”
            answer = self.generate_response(
                query=query,
                context=context,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            # 4. è¿”å›ç»“æœ
            return {
                "query": query,
                "search_results": search_results,
                "context": context,
                "answer": answer
            }
        except Exception as e:
            app.logger.error(f"æŸ¥è¯¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            # è¿”å›åŸºæœ¬å“åº”
            return {
                "query": query,
                "search_results": {"news": [], "announcements": []},
                "context": "æŸ¥è¯¢å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯",
                "answer": f"å¾ˆæŠ±æ­‰ï¼Œåœ¨å¤„ç†æ‚¨çš„æŸ¥è¯¢æ—¶å‘ç”Ÿäº†é”™è¯¯: {str(e)}ã€‚è¯·ç¨åå†è¯•æˆ–è”ç³»ç®¡ç†å‘˜ã€‚"
            }

   

# åˆå§‹åŒ–RAGç³»ç»Ÿ
# ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
EMBEDDING_MODEL_PATH = os.environ.get('EMBEDDING_MODEL_PATH', '/models/sentence-transformers_text2vec-large-chinese')
LLM_API_KEY = os.environ.get('LLM_API_KEY', 'your_openai_key_here')
LLM_BASE_URL = os.environ.get('LLM_BASE_URL', 'https://dashscope.aliyuncs.com/compatible-mode/v1')
LLM_MODEL = os.environ.get('LLM_MODEL', 'qwen-plus')
CHROMA_HOST = os.environ.get('CHROMA_HOST','192.168.222.128')
CHROMA_PORT = os.environ.get('CHROMA_PORT','8000')
USE_LANGCHAIN = os.environ.get('USE_LANGCHAIN', 'true').lower() == 'true'
CHUNK_SIZE = int(os.environ.get('CHUNK_SIZE', '1000'))
CHUNK_OVERLAP = int(os.environ.get('CHUNK_OVERLAP', '200'))

if CHROMA_PORT and CHROMA_PORT.isdigit():
    CHROMA_PORT = int(CHROMA_PORT)
else:
    CHROMA_PORT = None

# åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–RAGç³»ç»Ÿ
rag_system = None

# æ›¿æ¢ä¹‹å‰çš„ @app.before_first_request
with app.app_context():
    try:
        app.logger.info("åˆå§‹åŒ–RAGç³»ç»Ÿ...")
        rag_system = ChineseRAGSystem(
            embedding_model_path=EMBEDDING_MODEL_PATH,
            llm_api_key=LLM_API_KEY,
            llm_base_url=LLM_BASE_URL,
            llm_model=LLM_MODEL,
            chroma_host=CHROMA_HOST,
            chroma_port=CHROMA_PORT,
            use_langchain=USE_LANGCHAIN,
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP
        )
        app.logger.info("RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        app.logger.error(f"åˆå§‹åŒ–RAGç³»ç»Ÿæ—¶å‡ºé”™: {str(e)}")


# æ³¨å†Œè·¯ç”±

@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return jsonify({
        "status": "healthy", 
        "timestamp": datetime.now().isoformat(),
        "system_info": {
            "use_langchain": USE_LANGCHAIN,
            "chunk_size": CHUNK_SIZE,
            "chunk_overlap": CHUNK_OVERLAP,
            "numpy_version": np.__version__
        }
    })

@app.route('/query', methods=['POST'])
def query_endpoint():
    """æŸ¥è¯¢æ¥å£"""
    if not rag_system:
        return jsonify({"error": "RAGç³»ç»Ÿå°šæœªåˆå§‹åŒ–"}), 500
    
    data = request.json
    if not data or 'query' not in data:
        return jsonify({"error": "è¯·æä¾›æŸ¥è¯¢å†…å®¹"}), 400
    
    query = data['query']
    n_results = data.get('n_results', 3)
    temperature = data.get('temperature', 0.7)
    max_tokens = data.get('max_tokens', 1000)
    
    try:
        result = rag_system.query(
            query=query,
            n_results=n_results,
            temperature=temperature,
            max_tokens=max_tokens
        )
        # ä½¿ç”¨ensure_ascii=Falseç¡®ä¿ä¸­æ–‡å­—ç¬¦ä¸ä¼šè¢«ç¼–ç æˆUnicodeè½¬ä¹‰åºåˆ—
        return app.response_class(
            response=json.dumps(result, ensure_ascii=False),
            status=200,
            mimetype='application/json'
        )
    except Exception as e:
        app.logger.error(f"å¤„ç†æŸ¥è¯¢è¯·æ±‚æ—¶å‡ºé”™: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/add/news', methods=['POST'])
def add_news_endpoint():
    """æ·»åŠ æ–°é—»æ¥å£"""
    if not rag_system:
        return jsonify({"error": "RAGç³»ç»Ÿå°šæœªåˆå§‹åŒ–"}), 500
    
    # æ£€æŸ¥è¡¨å•æˆ–JSONæ•°æ®
    if request.is_json:
        data = request.json
    else:
        data = request.form.to_dict()
    
    # éªŒè¯å¿…è¦å­—æ®µ
    if not data or 'title' not in data:
        return jsonify({"error": "è¯·æä¾›æ ‡é¢˜"}), 400
    
    if 'content' not in data:
        return jsonify({"error": "è¯·æä¾›å†…å®¹"}), 400
    
    original_content = data['content']
    processed_content = original_content
    
    # æ£€æŸ¥contentæ˜¯å¦åŒ…å«HTMLå†…å®¹
    if '<' in original_content and '>' in original_content:
        try:
            # ä½¿ç”¨BeautifulSoupè§£æHTMLå†…å®¹
            soup = BeautifulSoup(original_content, 'html.parser')
            
            # è·å–çº¯æ–‡æœ¬å†…å®¹
            text_content = rag_system.doc_processor.sanitize_html(original_content)
            
            # å¤„ç†åµŒå…¥çš„å›¾ç‰‡
            embedded_contents = []
            for img in soup.find_all('img'):
                src = img.get('src', '')
                if src and (src.startswith('http://') or src.startswith('https://')):
                    try:
                        # ä¸‹è½½å›¾ç‰‡
                        img_response = requests.get(src, timeout=10)
                        if img_response.status_code == 200:
                            # å¤„ç†å›¾ç‰‡ä¸­çš„æ–‡æœ¬
                            img_text, confidence = rag_system.doc_processor.process_image(img_response.content)
                            if img_text and img_text != "No text detected in image":
                                embedded_contents.append(f"ã€å›¾ç‰‡å†…å®¹ã€‘: {img_text}")
                    except Exception as e:
                        app.logger.warning(f"å¤„ç†åµŒå…¥å›¾ç‰‡æ—¶å‡ºé”™: {str(e)}")
            
            # å¤„ç†åµŒå…¥çš„æ–‡æ¡£é“¾æ¥
            for a in soup.find_all('a'):
                href = a.get('href', '')
                if href and (href.endswith('.pdf') or href.endswith('.docx') or href.endswith('.doc')):
                    try:
                        # ä¸‹è½½æ–‡æ¡£
                        doc_response = requests.get(href, timeout=20)
                        if doc_response.status_code == 200:
                            # æ ¹æ®æ–‡ä»¶ç±»å‹å¤„ç†æ–‡æ¡£
                            if href.endswith('.pdf'):
                                doc_text = rag_system.doc_processor.process_pdf(doc_response.content)
                                embedded_contents.append(f"ã€PDFæ–‡æ¡£å†…å®¹ã€‘: {doc_text}")
                            elif href.endswith('.docx') or href.endswith('.doc'):
                                doc_text = rag_system.doc_processor.process_word_document(doc_response.content)
                                embedded_contents.append(f"ã€Wordæ–‡æ¡£å†…å®¹ã€‘: {doc_text}")
                    except Exception as e:
                        app.logger.warning(f"å¤„ç†åµŒå…¥æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")
            
            # ç»„åˆæ‰€æœ‰å†…å®¹
            if embedded_contents:
                extracted_content = "\n\n".join(embedded_contents)
                processed_content = f"{text_content}\n\n{extracted_content}"
            else:
                processed_content = text_content
                
        except Exception as e:
            app.logger.error(f"å¤„ç†HTMLå†…å®¹æ—¶å‡ºé”™: {str(e)}")
            # å¤±è´¥æ—¶è‡³å°‘æ¸…ç†HTMLæ ‡ç­¾
            processed_content = rag_system.doc_processor.sanitize_html(original_content)
    
    try:
        # å‡†å¤‡æ ‡ç­¾
        tags = None
        if 'tags' in data and data['tags']:
            if isinstance(data['tags'], str):
                tags = [tag.strip() for tag in data['tags'].split(',') if tag.strip()]
            elif isinstance(data['tags'], list):
                tags = data['tags']
        
        doc_id = rag_system.add_news(
            title=data['title'],
            content=processed_content,
            source=data.get('source'),
            publish_date=data.get('publish_date'),
            tags=tags
        )
        return jsonify({"success": True, "id": doc_id})
    except Exception as e:
        app.logger.error(f"æ·»åŠ æ–°é—»æ—¶å‡ºé”™: {str(e)}")
        return jsonify({"error": str(e)}), 500
@app.route('/add/announcement', methods=['POST'])
def add_announcement_endpoint():
    """æ·»åŠ å…¬å‘Šæ¥å£"""
    if not rag_system:
        return jsonify({"error": "RAGç³»ç»Ÿå°šæœªåˆå§‹åŒ–"}), 500
    
    # æ£€æŸ¥è¡¨å•æˆ–JSONæ•°æ®
    if request.is_json:
        data = request.json
    else:
        data = request.form.to_dict()
    
    # éªŒè¯å¿…è¦å­—æ®µ
    if not data or 'title' not in data:
        return jsonify({"error": "è¯·æä¾›æ ‡é¢˜"}), 400
    
    if 'content' not in data:
        return jsonify({"error": "è¯·æä¾›å†…å®¹"}), 400
    
    original_content = data['content']
    processed_content = original_content
    
    # æ£€æŸ¥contentæ˜¯å¦åŒ…å«HTMLå†…å®¹
    if '<' in original_content and '>' in original_content:
        try:
            # ä½¿ç”¨BeautifulSoupè§£æHTMLå†…å®¹
            soup = BeautifulSoup(original_content, 'html.parser')
            
            # è·å–çº¯æ–‡æœ¬å†…å®¹
            text_content = rag_system.doc_processor.sanitize_html(original_content)
            
            # å¤„ç†åµŒå…¥çš„å›¾ç‰‡
            embedded_contents = []
            for img in soup.find_all('img'):
                src = img.get('src', '')
                if src and (src.startswith('http://') or src.startswith('https://')):
                    try:
                        # ä¸‹è½½å›¾ç‰‡
                        img_response = requests.get(src, timeout=10)
                        if img_response.status_code == 200:
                            # å¤„ç†å›¾ç‰‡ä¸­çš„æ–‡æœ¬
                            img_text, confidence = rag_system.doc_processor.process_image(img_response.content)
                            if img_text and img_text != "No text detected in image":
                                embedded_contents.append(f"ã€å›¾ç‰‡å†…å®¹ã€‘: {img_text}")
                    except Exception as e:
                        app.logger.warning(f"å¤„ç†åµŒå…¥å›¾ç‰‡æ—¶å‡ºé”™: {str(e)}")
            
            # å¤„ç†åµŒå…¥çš„æ–‡æ¡£é“¾æ¥
            for a in soup.find_all('a'):
                href = a.get('href', '')
                if href and (href.endswith('.pdf') or href.endswith('.docx') or href.endswith('.doc')):
                    try:
                        # ä¸‹è½½æ–‡æ¡£
                        doc_response = requests.get(href, timeout=20)
                        if doc_response.status_code == 200:
                            # æ ¹æ®æ–‡ä»¶ç±»å‹å¤„ç†æ–‡æ¡£
                            if href.endswith('.pdf'):
                                doc_text = rag_system.doc_processor.process_pdf(doc_response.content)
                                embedded_contents.append(f"ã€PDFæ–‡æ¡£å†…å®¹ã€‘: {doc_text}")
                            elif href.endswith('.docx') or href.endswith('.doc'):
                                doc_text = rag_system.doc_processor.process_word_document(doc_response.content)
                                embedded_contents.append(f"ã€Wordæ–‡æ¡£å†…å®¹ã€‘: {doc_text}")
                    except Exception as e:
                        app.logger.warning(f"å¤„ç†åµŒå…¥æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")
            
            # ç»„åˆæ‰€æœ‰å†…å®¹
            if embedded_contents:
                extracted_content = "\n\n".join(embedded_contents)
                processed_content = f"{text_content}\n\n{extracted_content}"
            else:
                processed_content = text_content
                
        except Exception as e:
            app.logger.error(f"å¤„ç†HTMLå†…å®¹æ—¶å‡ºé”™: {str(e)}")
            # å¤±è´¥æ—¶è‡³å°‘æ¸…ç†HTMLæ ‡ç­¾
            processed_content = rag_system.doc_processor.sanitize_html(original_content)
    
    try:
        doc_id = rag_system.add_announcement(
            title=data['title'],
            content=processed_content,
            department=data.get('department'),
            publish_date=data.get('publish_date'),
            importance=data.get('importance', 'normal')
        )
        return jsonify({"success": True, "id": doc_id})
    except Exception as e:
        app.logger.error(f"æ·»åŠ å…¬å‘Šæ—¶å‡ºé”™: {str(e)}")
        return jsonify({"error": str(e)}), 500
@app.route('/import/csv', methods=['POST'])
def import_csv_endpoint():
    """æ‰¹é‡å¯¼å…¥CSVæ¥å£"""
    if not rag_system:
        return jsonify({"error": "RAGç³»ç»Ÿå°šæœªåˆå§‹åŒ–"}), 500
    
    if 'file' not in request.files:
        return jsonify({"error": "è¯·æä¾›CSVæ–‡ä»¶"}), 400
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({"error": "æœªé€‰æ‹©æ–‡ä»¶"}), 400
    
    if not file.filename.endswith('.csv'):
        return jsonify({"error": "è¯·ä¸Šä¼ CSVæ–‡ä»¶"}), 400
    
    doc_type = request.form.get('type')
    if doc_type not in ['news', 'announcement']:
        return jsonify({"error": "æ–‡æ¡£ç±»å‹å¿…é¡»æ˜¯'news'æˆ–'announcement'"}), 400
    
    try:
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        temp_path = f"/tmp/{uuid.uuid4()}.csv"
        file.save(temp_path)
        
        # å¯¼å…¥CSV
        result = rag_system.batch_add_from_csv(temp_path, doc_type)
        
        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        os.remove(temp_path)
        
        return jsonify(result)
    except Exception as e:
        app.logger.error(f"å¯¼å…¥CSVæ—¶å‡ºé”™: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/delete/news/<doc_id>', methods=['DELETE'])
def delete_news_endpoint(doc_id):
    """åˆ é™¤æŒ‡å®šIDçš„æ–°é—»"""
    if not rag_system:
        return jsonify({"error": "RAGç³»ç»Ÿå°šæœªåˆå§‹åŒ–"}), 500
    
    try:
        # æ£€æŸ¥doc_idæ˜¯å¦åŒ…å«å—ç´¢å¼•
        if '_' in doc_id:
            base_id = doc_id.split('_')[0]
            # æŸ¥æ‰¾æ‰€æœ‰ç›¸å…³å—
            query = f"base_id:'{base_id}'"
            results = rag_system.news_collection.get(
                where={"base_id": base_id}
            )
            
            if not results["ids"]:
                return jsonify({"error": f"æ‰¾ä¸åˆ°IDä¸º{base_id}çš„æ–°é—»"}), 404
            
            # åˆ é™¤æ‰€æœ‰ç›¸å…³å—
            rag_system.news_collection.delete(ids=results["ids"])
            app.logger.info(f"æˆåŠŸåˆ é™¤æ–°é—»ID: {base_id} çš„æ‰€æœ‰ {len(results['ids'])} ä¸ªå—")
            
            return jsonify({"success": True, "message": f"æˆåŠŸåˆ é™¤æ–°é—»ID: {base_id} çš„æ‰€æœ‰ {len(results['ids'])} ä¸ªå—"})
        else:
            # å¯¹äºå•å—æ–‡æ¡£æˆ–å°è¯•åˆ é™¤æ‰€æœ‰ç›¸å…³å—
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯çˆ¶ID
            results = rag_system.news_collection.get(
                where={"base_id": doc_id}
            )
            
            if results["ids"]:
                # è¿™æ˜¯ä¸€ä¸ªçˆ¶IDï¼Œåˆ é™¤æ‰€æœ‰ç›¸å…³å—
                rag_system.news_collection.delete(ids=results["ids"])
                app.logger.info(f"æˆåŠŸåˆ é™¤æ–°é—»ID: {doc_id} çš„æ‰€æœ‰ {len(results['ids'])} ä¸ªå—")
                return jsonify({"success": True, "message": f"æˆåŠŸåˆ é™¤æ–°é—»ID: {doc_id} çš„æ‰€æœ‰ {len(results['ids'])} ä¸ªå—"})
            else:
                # å°è¯•ç›´æ¥åˆ é™¤è¯¥ID
                direct_results = rag_system.news_collection.get(ids=[doc_id], include=[])
                if not direct_results["ids"]:
                    return jsonify({"error": f"æ‰¾ä¸åˆ°IDä¸º{doc_id}çš„æ–°é—»"}), 404
                
                # åˆ é™¤æ–‡æ¡£
                rag_system.news_collection.delete(ids=[doc_id])
                app.logger.info(f"æˆåŠŸåˆ é™¤æ–°é—»ID: {doc_id}")
                
                return jsonify({"success": True, "message": f"æˆåŠŸåˆ é™¤æ–°é—»ID: {doc_id}"})
    except Exception as e:
        app.logger.error(f"åˆ é™¤æ–°é—»æ—¶å‡ºé”™: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/delete/announcement/<doc_id>', methods=['DELETE'])
def delete_announcement_endpoint(doc_id):
    """åˆ é™¤æŒ‡å®šIDçš„å…¬å‘Š"""
    if not rag_system:
        return jsonify({"error": "RAGç³»ç»Ÿå°šæœªåˆå§‹åŒ–"}), 500
    
    try:
        # æ£€æŸ¥doc_idæ˜¯å¦åŒ…å«å—ç´¢å¼•
        if '_' in doc_id:
            base_id = doc_id.split('_')[0]
            # æŸ¥æ‰¾æ‰€æœ‰ç›¸å…³å—
            results = rag_system.announcement_collection.get(
                where={"base_id": base_id}
            )
            
            if not results["ids"]:
                return jsonify({"error": f"æ‰¾ä¸åˆ°IDä¸º{base_id}çš„å…¬å‘Š"}), 404
            
            # åˆ é™¤æ‰€æœ‰ç›¸å…³å—
            rag_system.announcement_collection.delete(ids=results["ids"])
            app.logger.info(f"æˆåŠŸåˆ é™¤å…¬å‘ŠID: {base_id} çš„æ‰€æœ‰ {len(results['ids'])} ä¸ªå—")
            
            return jsonify({"success": True, "message": f"æˆåŠŸåˆ é™¤å…¬å‘ŠID: {base_id} çš„æ‰€æœ‰ {len(results['ids'])} ä¸ªå—"})
        else:
            # å¯¹äºå•å—æ–‡æ¡£æˆ–å°è¯•åˆ é™¤æ‰€æœ‰ç›¸å…³å—
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯çˆ¶ID
            results = rag_system.announcement_collection.get(
                where={"base_id": doc_id}
            )
            
            if results["ids"]:
                # è¿™æ˜¯ä¸€ä¸ªçˆ¶IDï¼Œåˆ é™¤æ‰€æœ‰ç›¸å…³å—
                rag_system.announcement_collection.delete(ids=results["ids"])
                app.logger.info(f"æˆåŠŸåˆ é™¤å…¬å‘ŠID: {doc_id} çš„æ‰€æœ‰ {len(results['ids'])} ä¸ªå—")
                return jsonify({"success": True, "message": f"æˆåŠŸåˆ é™¤å…¬å‘ŠID: {doc_id} çš„æ‰€æœ‰ {len(results['ids'])} ä¸ªå—"})
            else:
                # å°è¯•ç›´æ¥åˆ é™¤è¯¥ID
                direct_results = rag_system.announcement_collection.get(ids=[doc_id], include=[])
                if not direct_results["ids"]:
                    return jsonify({"error": f"æ‰¾ä¸åˆ°IDä¸º{doc_id}çš„å…¬å‘Š"}), 404
                
                # åˆ é™¤æ–‡æ¡£
                rag_system.announcement_collection.delete(ids=[doc_id])
                app.logger.info(f"æˆåŠŸåˆ é™¤å…¬å‘ŠID: {doc_id}")
                
                return jsonify({"success": True, "message": f"æˆåŠŸåˆ é™¤å…¬å‘ŠID: {doc_id}"})
    except Exception as e:
        app.logger.error(f"åˆ é™¤å…¬å‘Šæ—¶å‡ºé”™: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/process/file', methods=['POST'])
def process_file_endpoint():
    """å¤„ç†æ–‡ä»¶æå–å†…å®¹æ¥å£"""
    if not rag_system:
        return jsonify({"error": "RAGç³»ç»Ÿå°šæœªåˆå§‹åŒ–"}), 500
    
    if 'file' not in request.files:
        return jsonify({"error": "è¯·æä¾›æ–‡ä»¶"}), 400
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({"error": "æœªé€‰æ‹©æ–‡ä»¶"}), 400
    
    try:
        file_data = file.read()
        file_type = file.content_type or os.path.splitext(file.filename)[1]
        
        extracted_content = rag_system.doc_processor.get_file_content(file_data, file_type)
        
        return jsonify({
            "success": True, 
            "filename": file.filename,
            "file_type": file_type,
            "content": extracted_content,
            "content_length": len(extracted_content)
        })
    except Exception as e:
        app.logger.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/process/html', methods=['POST'])
def process_html_endpoint():
    """å¤„ç†HTMLå†…å®¹æ¥å£"""
    if not rag_system:
        return jsonify({"error": "RAGç³»ç»Ÿå°šæœªåˆå§‹åŒ–"}), 500
    
    data = request.json
    if not data or 'html' not in data:
        return jsonify({"error": "è¯·æä¾›HTMLå†…å®¹"}), 400
    
    try:
        html_content = data['html']
        sanitized_content = rag_system.doc_processor.sanitize_html(html_content)
        
        return jsonify({
            "success": True,
            "content": sanitized_content,
            "content_length": len(sanitized_content)
        })
    except Exception as e:
        app.logger.error(f"å¤„ç†HTMLå†…å®¹æ—¶å‡ºé”™: {str(e)}")
        return jsonify({"error": str(e)}), 500

# å¯åŠ¨å‡½æ•°
if __name__ == "__main__":
    # å¦‚æœå­˜åœ¨PORTç¯å¢ƒå˜é‡ï¼Œä½¿ç”¨å®ƒï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤çš„5000
    port = int(os.environ.get("PORT", 5000))
    # åœ¨å¼€å‘æ¨¡å¼ä¸‹å¯ç”¨è°ƒè¯•ï¼Œç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨ gunicorn
    debug = os.environ.get("FLASK_ENV") == "development"
    
    app.run(host="0.0.0.0", port=port, debug=debug)
        