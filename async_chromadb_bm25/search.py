from flask import Flask, request, jsonify
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from openai import OpenAI
import pandas as pd
import uuid
from typing import List, Dict, Optional, Union, Any, Tuple
import json
from datetime import datetime
import os
import numpy as np
from bs4 import BeautifulSoup
from PIL import Image
from io import BytesIO
import requests
from paddleocr import PaddleOCR
import mammoth
import PyPDF2
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document as LCDocument
import tempfile
import threading
import queue
import time
import uuid
from typing import Dict, Callable, Any, List, Optional
import logging

from flask import Flask
app = Flask(__name__)

class ESServiceClient:
    """
    ESæœåŠ¡å®¢æˆ·ç«¯ï¼Œç”¨äºä¸ESæœåŠ¡APIé€šä¿¡
    """
    
    def __init__(self, base_url: str = "http://localhost:8085"):
        """
        åˆå§‹åŒ–ESæœåŠ¡å®¢æˆ·ç«¯
        
        Args:
            base_url: ESæœåŠ¡çš„åŸºç¡€URL
        """
        self.base_url = base_url.rstrip('/')
        self.is_available = self._check_health()
        self.session = requests.Session()
    
    def _check_health(self) -> bool:
        """
        æ£€æŸ¥ESæœåŠ¡æ˜¯å¦å¯ç”¨
        
        Returns:
            bool: æœåŠ¡æ˜¯å¦å¯ç”¨
        """
        try:
            response = requests.get(f"{self.base_url}/health", timeout=5)
            return response.status_code == 200 and response.json().get("status") == "healthy"
        except Exception as e:
            app.logger.error(f"ESæœåŠ¡å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")
            return False
    
    def index_news(self, document: Dict, async_mode: bool = True) -> bool:
        """
        ç´¢å¼•æ–°é—»åˆ°ES
        
        Args:
            document: æ–°é—»æ–‡æ¡£
            async_mode: æ˜¯å¦å¼‚æ­¥æ‰§è¡Œ
            
        Returns:
            bool: åŒæ­¥æ¨¡å¼ä¸‹æ˜¯å¦æˆåŠŸï¼Œå¼‚æ­¥æ¨¡å¼ä¸‹å§‹ç»ˆè¿”å›True
        """
        if not self.is_available:
            return False
            
        url = f"{self.base_url}/index/news"
        
        if async_mode:
            # åˆ›å»ºçº¿ç¨‹æ¥æ‰§è¡Œè¯·æ±‚
            thread = threading.Thread(
                target=self._make_request,
                args=(url, document)
            )
            thread.daemon = True
            thread.start()
            return True
        else:
            # åŒæ­¥æ‰§è¡Œ
            return self._make_request(url, document)
    
    def index_notice(self, document: Dict, async_mode: bool = True) -> bool:
        """
        ç´¢å¼•å…¬å‘Šåˆ°ES
        
        Args:
            document: å…¬å‘Šæ–‡æ¡£
            async_mode: æ˜¯å¦å¼‚æ­¥æ‰§è¡Œ
            
        Returns:
            bool: åŒæ­¥æ¨¡å¼ä¸‹æ˜¯å¦æˆåŠŸï¼Œå¼‚æ­¥æ¨¡å¼ä¸‹å§‹ç»ˆè¿”å›True
        """
        if not self.is_available:
            return False
            
        url = f"{self.base_url}/index/notice"
        
        if async_mode:
            # åˆ›å»ºçº¿ç¨‹æ¥æ‰§è¡Œè¯·æ±‚
            thread = threading.Thread(
                target=self._make_request,
                args=(url, document)
            )
            thread.daemon = True
            thread.start()
            return True
        else:
            # åŒæ­¥æ‰§è¡Œ
            return self._make_request(url, document)
    
    def search_news(self, query: str, n_results: int = 5) -> List[Dict]:
        """
        ä»ESæœç´¢æ–°é—»
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            n_results: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            List[Dict]: æ–°é—»åˆ—è¡¨
        """
        if not self.is_available:
            return []
            
        url = f"{self.base_url}/search/news"
        data = {"query": query, "n_results": n_results}
        
        try:
            response = self.session.post(url, json=data, timeout=10)
            if response.status_code == 200:
                return response.json().get("results", [])
            else:
                app.logger.error(f"æœç´¢æ–°é—»å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return []
        except Exception as e:
            app.logger.error(f"æœç´¢æ–°é—»æ—¶å‡ºé”™: {str(e)}")
            return []
    
    def search_notice(self, query: str, n_results: int = 5) -> List[Dict]:
        """
        ä»ESæœç´¢å…¬å‘Š
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            n_results: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            List[Dict]: å…¬å‘Šåˆ—è¡¨
        """
        if not self.is_available:
            return []
            
        url = f"{self.base_url}/search/notice"
        data = {"query": query, "n_results": n_results}
        
        try:
            response = self.session.post(url, json=data, timeout=10)
            if response.status_code == 200:
                return response.json().get("results", [])
            else:
                app.logger.error(f"æœç´¢å…¬å‘Šå¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return []
        except Exception as e:
            app.logger.error(f"æœç´¢å…¬å‘Šæ—¶å‡ºé”™: {str(e)}")
            return []
    
    def search_all(self, query: str, n_results: int = 5) -> Dict[str, List[Dict]]:
        """
        ä»ESåŒæ—¶æœç´¢æ–°é—»å’Œå…¬å‘Š
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            n_results: æ¯ç§ç±»å‹è¿”å›çš„ç»“æœæ•°é‡
            
        Returns:
            Dict[str, List[Dict]]: åŒ…å«æ–°é—»å’Œå…¬å‘Šçš„å­—å…¸
        """
        if not self.is_available:
            return {"news": [], "announcements": []}
            
        url = f"{self.base_url}/search/all"
        data = {"query": query, "n_results": n_results}
        
        try:
            response = self.session.post(url, json=data, timeout=10)
            if response.status_code == 200:
                return response.json()
            else:
                app.logger.error(f"æœç´¢å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return {"news": [], "announcements": []}
        except Exception as e:
            app.logger.error(f"æœç´¢æ—¶å‡ºé”™: {str(e)}")
            return {"news": [], "announcements": []}
    
    def delete_news(self, doc_id: str) -> bool:
        """
        åˆ é™¤æ–°é—»
        
        Args:
            doc_id: æ–‡æ¡£ID
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸåˆ é™¤
        """
        if not self.is_available:
            return False
            
        url = f"{self.base_url}/delete/news/{doc_id}"
        
        try:
            response = self.session.delete(url, timeout=10)
            return response.status_code == 200
        except Exception as e:
            app.logger.error(f"åˆ é™¤æ–°é—»æ—¶å‡ºé”™: {str(e)}")
            return False
    
    def delete_notice(self, doc_id: str) -> bool:
        """
        åˆ é™¤å…¬å‘Š
        
        Args:
            doc_id: æ–‡æ¡£ID
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸåˆ é™¤
        """
        if not self.is_available:
            return False
            
        url = f"{self.base_url}/delete/notice/{doc_id}"
        
        try:
            response = self.session.delete(url, timeout=10)
            return response.status_code == 200
        except Exception as e:
            app.logger.error(f"åˆ é™¤å…¬å‘Šæ—¶å‡ºé”™: {str(e)}")
            return False
    
    def _make_request(self, url: str, data: Dict) -> bool:
        """
        æ‰§è¡Œè¯·æ±‚
        
        Args:
            url: è¯·æ±‚URL
            data: è¯·æ±‚æ•°æ®
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            response = self.session.post(url, json=data, timeout=10)
            return response.status_code == 200
        except Exception as e:
            app.logger.error(f"è¯·æ±‚ {url} æ—¶å‡ºé”™: {str(e)}")
            return False

class RankFusion:
    """ç»“æœèåˆç®—æ³•å·¥å…·ç±»ï¼Œç”¨äºèåˆå‘é‡å’ŒBM25æœç´¢ç»“æœ"""
    
    @staticmethod
    def contextual_fusion(query: str, dense_results: dict, lexical_results: dict, k: int = 60) -> dict:
        """
        ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„èåˆç®—æ³•ï¼Œé’ˆå¯¹ä¸åŒç±»å‹çš„æŸ¥è¯¢åŠ¨æ€è°ƒæ•´æƒé‡
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            dense_results: å‘é‡æ£€ç´¢ç»“æœ (æ ¼å¼: {"news": [...], "announcements": [...]})
            lexical_results: æ–‡æœ¬æ£€ç´¢ç»“æœ (æ ¼å¼: {"news": [...], "announcements": [...]})
            k: RRFå¸¸æ•°
            
        Returns:
            èåˆåçš„ç»“æœå­—å…¸ (æ ¼å¼: {"news": [...], "announcements": [...]})
        """
        # æå–æŸ¥è¯¢ç‰¹å¾
        query_terms = set(query.lower().split())
        is_status_query = any(term in query_terms for term in ['çŠ¶æ€', 'å–æ¶ˆ', 'å®Œæˆ', 'æ”¯ä»˜'])
        is_time_query = any(term in query_terms for term in ['æ—¶é—´', 'æ—¥æœŸ', 'å¹´', 'æœˆ', 'æ—¥'])
        is_type_query = any(term in query_terms for term in ['ç±»å‹', 'ç§ç±»', 'åˆ†ç±»'])
        
        # åŠ¨æ€è°ƒæ•´æƒé‡
        if is_status_query or is_type_query:
            # çŠ¶æ€å’Œç±»å‹æŸ¥è¯¢ï¼ŒBM25å¯èƒ½æ›´å‡†ç¡®
            vector_weight = 0.4
            lexical_weight = 0.6
        elif is_time_query:
            # æ—¶é—´æŸ¥è¯¢ï¼Œä¸¤è€…éƒ½é‡è¦
            vector_weight = 0.5
            lexical_weight = 0.5
        else:
            # é»˜è®¤æƒé‡
            vector_weight = 0.7
            lexical_weight = 0.3
        
        # ç”¨äºä¿å­˜èåˆç»“æœ
        fused_results = {"news": [], "announcements": []}
        
        # å¤„ç†æ–°é—»å’Œå…¬å‘Š
        for content_type in ["news", "announcements"]:
            # è·å–å„è‡ªçš„ç»“æœ
            vector_content = dense_results.get(content_type, [])
            lexical_content = lexical_results.get(content_type, [])
            
            # è®¡ç®—èåˆåˆ†æ•°
            scores = {}
            
            # å¤„ç†å‘é‡ç»“æœ
            for rank, item in enumerate(vector_content, start=1):
                item_id = item.get("id", "") 
                if not item_id:
                    continue
                    
                if item_id not in scores:
                    scores[item_id] = {"item": item, "score": 0, "matches": set()}
                
                scores[item_id]["score"] += vector_weight * (1.0 / (k + rank))
                scores[item_id]["matches"].add("vector")
            
            # å¤„ç†BM25ç»“æœ
            for rank, item in enumerate(lexical_content, start=1):
                item_id = item.get("id", "")
                if not item_id:
                    continue
                    
                if item_id not in scores:
                    scores[item_id] = {"item": item, "score": 0, "matches": set()}
                
                scores[item_id]["score"] += lexical_weight * (1.0 / (k + rank))
                scores[item_id]["matches"].add("lexical")
                
                # é¢å¤–çš„ä¸Šä¸‹æ–‡å¥–åŠ±
                content = item.get("content", "")
                title = item.get("title", "")
                
                # æ£€æŸ¥ç»“æœæ˜¯å¦åŒ…å«æŸ¥è¯¢è¯
                term_matches = sum(1 for term in query_terms if term in (content + title).lower())
                term_match_ratio = term_matches / len(query_terms) if query_terms else 0
                
                # è¯åŒ¹é…å¥–åŠ±
                scores[item_id]["score"] *= (1 + 0.2 * term_match_ratio)
            
            # å¤šæ£€ç´¢æºå¥–åŠ±
            for item_id, data in scores.items():
                if len(data["matches"]) > 1:  # åŒæ—¶å‡ºç°åœ¨ä¸¤ç§æ£€ç´¢ä¸­
                    data["score"] *= 1.25
            
            # æŒ‰åˆ†æ•°æ’åº
            sorted_items = sorted(scores.values(), key=lambda x: x["score"], reverse=True)
            fused_results[content_type] = [item_data["item"] for item_data in sorted_items]
        
        return fused_results

class AsyncTaskManager:
    """å¼‚æ­¥ä»»åŠ¡ç®¡ç†å™¨ï¼Œä½¿ç”¨çº¿ç¨‹æ± å¤„ç†è€—æ—¶ä»»åŠ¡"""
    
    def __init__(self, max_workers=5):
        """
        åˆå§‹åŒ–ä»»åŠ¡ç®¡ç†å™¨
        
        Args:
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
        """
        self.max_workers = max_workers
        self.task_queue = queue.Queue()
        self.workers = []
        self.results = {}  # å­˜å‚¨ä»»åŠ¡ç»“æœ
        self.status = {}   # å­˜å‚¨ä»»åŠ¡çŠ¶æ€ (pending, running, completed, failed)
        self.callbacks = {} # ä»»åŠ¡å®Œæˆåçš„å›è°ƒå‡½æ•°
        self.logger = logging.getLogger("AsyncTaskManager")
        self._start_workers()
        
    def _worker_loop(self):
        """å·¥ä½œçº¿ç¨‹å¾ªç¯ï¼Œä¸æ–­ä»é˜Ÿåˆ—è·å–ä»»åŠ¡å¹¶æ‰§è¡Œ"""
        while True:
            try:
                # ä»é˜Ÿåˆ—è·å–ä»»åŠ¡
                task_id, task_func, args, kwargs = self.task_queue.get()
                
                # æ›´æ–°ä»»åŠ¡çŠ¶æ€
                self.status[task_id] = "running"
                self.logger.info(f"å¼€å§‹æ‰§è¡Œä»»åŠ¡ {task_id}")
                
                try:
                    # æ‰§è¡Œä»»åŠ¡
                    result = task_func(*args, **kwargs)
                    # å­˜å‚¨ç»“æœ
                    self.results[task_id] = result
                    self.status[task_id] = "completed"
                    self.logger.info(f"ä»»åŠ¡ {task_id} å®Œæˆ")
                    
                    # æ‰§è¡Œå›è°ƒï¼ˆå¦‚æœæœ‰ï¼‰
                    if task_id in self.callbacks and self.callbacks[task_id]:
                        try:
                            self.callbacks[task_id](result)
                            self.logger.info(f"ä»»åŠ¡ {task_id} å›è°ƒæ‰§è¡ŒæˆåŠŸ")
                        except Exception as e:
                            self.logger.error(f"ä»»åŠ¡ {task_id} å›è°ƒæ‰§è¡Œå¤±è´¥: {str(e)}")
                    
                except Exception as e:
                    # ä»»åŠ¡æ‰§è¡Œå¤±è´¥
                    self.status[task_id] = "failed"
                    self.results[task_id] = str(e)
                    self.logger.error(f"ä»»åŠ¡ {task_id} æ‰§è¡Œå¤±è´¥: {str(e)}")
                
                # æ ‡è®°ä»»åŠ¡å®Œæˆ
                self.task_queue.task_done()
                
            except Exception as e:
                self.logger.error(f"å·¥ä½œçº¿ç¨‹æ‰§è¡Œå‡ºé”™: {str(e)}")
                # çŸ­æš‚ä¼‘æ¯ä»¥é¿å…CPUå ç”¨è¿‡é«˜
                time.sleep(0.1)
    
    def _start_workers(self):
        """å¯åŠ¨å·¥ä½œçº¿ç¨‹"""
        for i in range(self.max_workers):
            thread = threading.Thread(target=self._worker_loop, daemon=True)
            thread.start()
            self.workers.append(thread)
            self.logger.info(f"å¯åŠ¨å·¥ä½œçº¿ç¨‹ {i+1}")
    
    def submit_task(self, task_func: Callable, callback: Optional[Callable]=None, *args, **kwargs) -> str:
        """
        æäº¤ä»»åŠ¡åˆ°é˜Ÿåˆ—
        
        Args:
            task_func: è¦æ‰§è¡Œçš„å‡½æ•°
            callback: ä»»åŠ¡å®Œæˆåçš„å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶ä»»åŠ¡ç»“æœä½œä¸ºå‚æ•°
            *args, **kwargs: ä¼ é€’ç»™ä»»åŠ¡å‡½æ•°çš„å‚æ•°
            
        Returns:
            str: ä»»åŠ¡ID
        """
        task_id = str(uuid.uuid4())
        self.status[task_id] = "pending"
        
        if callback:
            self.callbacks[task_id] = callback
            
        # å°†ä»»åŠ¡æ”¾å…¥é˜Ÿåˆ—
        self.task_queue.put((task_id, task_func, args, kwargs))
        self.logger.info(f"æäº¤ä»»åŠ¡ {task_id} åˆ°é˜Ÿåˆ—")
        
        return task_id
    
    def get_task_status(self, task_id: str) -> Dict:
        """
        è·å–ä»»åŠ¡çŠ¶æ€
        
        Args:
            task_id: ä»»åŠ¡ID
            
        Returns:
            Dict: åŒ…å«ä»»åŠ¡çŠ¶æ€å’Œç»“æœï¼ˆå¦‚æœå·²å®Œæˆï¼‰
        """
        if task_id not in self.status:
            return {"status": "not_found"}
            
        result = {
            "status": self.status[task_id]
        }
        
        # å¦‚æœä»»åŠ¡å·²å®Œæˆæˆ–å¤±è´¥ï¼ŒåŒ…å«ç»“æœ
        if self.status[task_id] in ["completed", "failed"] and task_id in self.results:
            result["result"] = self.results[task_id]
            
        return result
    
    def wait_for_task(self, task_id: str, timeout: Optional[float]=None) -> Any:
        """
        ç­‰å¾…ä»»åŠ¡å®Œæˆå¹¶è¿”å›ç»“æœ
        
        Args:
            task_id: ä»»åŠ¡ID
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            Any: ä»»åŠ¡ç»“æœ
            
        Raises:
            TimeoutError: å¦‚æœç­‰å¾…è¶…æ—¶
            ValueError: å¦‚æœä»»åŠ¡ä¸å­˜åœ¨
            RuntimeError: å¦‚æœä»»åŠ¡æ‰§è¡Œå¤±è´¥
        """
        if task_id not in self.status:
            raise ValueError(f"ä»»åŠ¡ {task_id} ä¸å­˜åœ¨")
            
        start_time = time.time()
        while self.status[task_id] in ["pending", "running"]:
            time.sleep(0.1)
            
            if timeout and (time.time() - start_time) > timeout:
                raise TimeoutError(f"ç­‰å¾…ä»»åŠ¡ {task_id} è¶…æ—¶")
        
        if self.status[task_id] == "failed":
            raise RuntimeError(f"ä»»åŠ¡ {task_id} æ‰§è¡Œå¤±è´¥: {self.results[task_id]}")
            
        return self.results[task_id]
    
    def clean_old_tasks(self, max_age: float=3600):
        """
        æ¸…ç†æ—§ä»»åŠ¡æ•°æ®
        
        Args:
            max_age: æœ€å¤§ä¿ç•™æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤1å°æ—¶
        """
        # å®ç°æ¸…ç†é€»è¾‘...
        pass

class DocumentProcessor:
    """Document processing utilities for different file types"""
    
    def __init__(self):
        """Initialize document processor with OCR model"""
        # Handle numpy compatibility issue
        if not hasattr(np, 'int'):
            np.int = np.int32
        
        # Initialize OCR model
        try:
            self.ocr = PaddleOCR(
                det_model_dir="/root/.paddleocr/whl/det/ch/ch_PP-OCRv3_det_infer",
                rec_model_dir="/root/.paddleocr/whl/rec/ch/ch_PP-OCRv3_rec_infer",
                cls_model_dir="/root/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer",
                use_angle_cls=True,
                lang="ch"
            )
            app.logger.info("OCR model initialized successfully")
        except Exception as e:
            self.ocr = None
            app.logger.error(f"Failed to initialize OCR model: {str(e)}")
    
    def sanitize_html(self, html_content: str) -> str:
        """
        Sanitize HTML content to extract clean text
        
        Args:
            html_content: HTML string to sanitize
            
        Returns:
            str: Clean text extracted from HTML
        """
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Remove script and style elements
            for script in soup(["script", "style"]):
                script.extract()
                
            # Get text
            text = soup.get_text(separator=' ', strip=True)
            
            # Clean up whitespace
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = '\n'.join(chunk for chunk in chunks if chunk)
            
            return text
        except Exception as e:
            app.logger.error(f"HTML sanitization error: {str(e)}")
            # Return the original content if sanitization fails
            return html_content

    def process_image(self, image_data: bytes) -> Tuple[str, float]:
        """
        Extract text from image using OCR
        
        Args:
            image_data: Binary image data
            
        Returns:
            Tuple[str, float]: Extracted text and average confidence score
        """
        if self.ocr is None:
            app.logger.error("OCR model not initialized")
            return "OCR model not available", 0.0
            
        try:
            # Convert bytes to PIL Image
            img = Image.open(BytesIO(image_data))
            
            # Convert to numpy array
            img_np = np.array(img)
            
            # Run OCR
            result = self.ocr.ocr(img_np, cls=True)
            
            # Extract text and calculate average confidence
            text_parts = []
            confidence_sum = 0.0
            count = 0
            
            if result:
                for line in result:
                    if isinstance(line, list) and line and len(line) > 0:
                        if len(line[-1]) >= 2:
                            text = line[-1][0]  # Text content
                            confidence = float(line[-1][1])  # Confidence score
                            
                            if text and confidence > 0.5:  # Only include reasonably confident results
                                text_parts.append(text)
                                confidence_sum += confidence
                                count += 1
            
            # Calculate average confidence if there are valid detections
            avg_confidence = confidence_sum / count if count > 0 else 0.0
            full_text = "\n".join(text_parts)
            
            if not full_text.strip():
                return "No text detected in image", 0.0
                
            return full_text, avg_confidence
        
        except Exception as e:
            app.logger.error(f"Image processing error: {str(e)}")
            return f"Failed to process image: {str(e)}", 0.0

    def process_word_document(self, docx_data: bytes) -> str:
        """
        Extract text from Word document
        
        Args:
            docx_data: Binary Word document data
            
        Returns:
            str: Extracted text
        """
        try:
            result = mammoth.extract_raw_text(BytesIO(docx_data))
            return result.value
        except Exception as e:
            app.logger.error(f"Word document processing error: {str(e)}")
            return f"Failed to process Word document: {str(e)}"

    def process_pdf(self, pdf_data: bytes) -> str:
        """
        Extract text from PDF document
        
        Args:
            pdf_data: Binary PDF data
            
        Returns:
            str: Extracted text
        """
        try:
            with BytesIO(pdf_data) as pdf_file:
                pdf_reader = PyPDF2.PdfReader(pdf_file)
                text = []
                
                for page_num in range(len(pdf_reader.pages)):
                    page = pdf_reader.pages[page_num]
                    text.append(page.extract_text())
                
            return "\n\n".join(text)
        except Exception as e:
            app.logger.error(f"PDF processing error: {str(e)}")
            return f"Failed to process PDF: {str(e)}"

    def get_file_content(self, file_data: bytes, file_type: str) -> str:
        """
        Process file based on its type and return text content
        
        Args:
            file_data: Binary file data
            file_type: MIME type or file extension
            
        Returns:
            str: Extracted text content
        """
        file_type = file_type.lower()
        
        if file_type in ['image/jpeg', 'image/png', 'image/jpg', 'image/gif', 'image/bmp']:
            text, confidence = self.process_image(file_data)
            return text
        elif file_type in ['application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'application/msword', '.docx', '.doc']:
            return self.process_word_document(file_data)
        elif file_type in ['application/pdf', '.pdf']:
            return self.process_pdf(file_data)
        elif file_type in ['text/html', '.html', '.htm']:
            return self.sanitize_html(file_data.decode('utf-8', errors='replace'))
        elif file_type in ['text/plain', '.txt']:
            return file_data.decode('utf-8', errors='replace')
        else:
            return f"Unsupported file type: {file_type}"

class ChineseRAGSystem:
    def __init__(
        self, 
        embedding_model_path: str = "/models/sentence-transformers_text2vec-large-chinese",
        llm_api_key: str = None,
        llm_base_url: str = "https://dashscope.aliyuncs.com/compatible-mode/v1",
        llm_model: str = "qwen-plus",
        chroma_host: str = None, 
        chroma_port: int = None,
        use_langchain: bool = True,
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        use_hybrid_search: bool = False,
        es_service_url: str = "http://localhost:8085"
    ):
        """
        åˆå§‹åŒ–ä¸­æ–‡RAGç³»ç»Ÿ
        
        Args:
            embedding_model_path: åµŒå…¥æ¨¡å‹çš„æœ¬åœ°è·¯å¾„
            llm_api_key: å¤§æ¨¡å‹APIå¯†é’¥
            llm_base_url: å¤§æ¨¡å‹APIåŸºç¡€URL
            llm_model: ä½¿ç”¨çš„å¤§æ¨¡å‹åç§°
            chroma_host: ChromaDBæœåŠ¡å™¨åœ°å€ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æœ¬åœ°å†…å­˜æ¨¡å¼
            chroma_port: ChromaDBæœåŠ¡å™¨ç«¯å£
            use_langchain: æ˜¯å¦ä½¿ç”¨LangChainè¿›è¡Œæ–‡æ¡£åˆ†å—
            chunk_size: æ–‡æ¡£åˆ†å—å¤§å°
            chunk_overlap: æ–‡æ¡£åˆ†å—é‡å éƒ¨åˆ†å¤§å°
            use_hybrid_search: æ˜¯å¦å¼€å¯æ··åˆæ£€ç´¢(chromadb+bm25)
            es_service_url: esæœåŠ¡å™¨
        """
        
        # ESæœåŠ¡é…ç½®
        self.use_hybrid_search = use_hybrid_search
        self.es_service_url = es_service_url
        
        # åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨
        self.doc_processor = DocumentProcessor()
        
        # æ˜¯å¦ä½¿ç”¨LangChain
        self.use_langchain = use_langchain
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # åŠ è½½åµŒå…¥æ¨¡å‹
        app.logger.info(f"æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹: {embedding_model_path}...")
        self.embedding_model = SentenceTransformer(
            model_name_or_path=embedding_model_path,
            local_files_only=True
        )
        self.vector_dim = self.embedding_model.get_sentence_embedding_dimension()
        app.logger.info(f"åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆï¼Œå‘é‡ç»´åº¦ï¼š{self.vector_dim}")
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        self.llm_api_key = llm_api_key or os.environ.get("LLM_API_KEY")
        self.llm_client = OpenAI(
            api_key=self.llm_api_key,
            base_url=llm_base_url
        )
        self.llm_model = llm_model
        app.logger.info(f"LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {llm_model}")
        
        # åˆå§‹åŒ–ESæœåŠ¡å®¢æˆ·ç«¯ï¼ˆå¦‚æœé…ç½®äº†ESæœåŠ¡URLï¼‰
        self.es_client = None
        if self.es_service_url and self.use_hybrid_search:
            self.es_client = ESServiceClient(base_url=self.es_service_url)
            app.logger.info(f"ESæœåŠ¡å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆï¼ŒæœåŠ¡å¯ç”¨: {self.es_client.is_available}")
        else:
            app.logger.info("æœªé…ç½®ESæœåŠ¡URLæˆ–æœªå¯ç”¨æ··åˆæœç´¢ï¼Œä¸åˆå§‹åŒ–ESå®¢æˆ·ç«¯")
        
        # è®¾ç½®ChromaDBå®¢æˆ·ç«¯
        try:
            if chroma_host and chroma_port:
                app.logger.info(f"å°è¯•è¿æ¥åˆ°è¿œç¨‹ChromaDB: {chroma_host}:{chroma_port}")
                self.db_client = chromadb.HttpClient(
                    host=chroma_host,
                    port=chroma_port,
                    settings=Settings(anonymized_telemetry=False)
                )
                # å°è¯•ä¸€ä¸ªç®€å•æ“ä½œæ¥æµ‹è¯•è¿æ¥
                self.db_client.heartbeat()
                app.logger.info("æˆåŠŸè¿æ¥åˆ°ChromaDBæœåŠ¡å™¨")
            else:
                app.logger.info("ä½¿ç”¨å†…å­˜æ¨¡å¼ChromaDB")
                self.db_client = chromadb.EphemeralClient(
                    settings=Settings(anonymized_telemetry=False)
                )
        except Exception as e:
            app.logger.error(f"è¿æ¥ChromaDBæ—¶å‡ºé”™: {str(e)}")
            app.logger.info("å›é€€åˆ°å†…å­˜æ¨¡å¼ChromaDB")
            self.db_client = chromadb.EphemeralClient(
                settings=Settings(anonymized_telemetry=False)
            )
        
        # ç›´æ¥åˆ›å»ºé›†åˆï¼Œä¸è®¾ç½®embedding_functionï¼Œæˆ‘ä»¬å°†æ‰‹åŠ¨è®¡ç®—åµŒå…¥å‘é‡
        try:
            self.news_collection = self.db_client.get_or_create_collection(
                name="cooper_news"
            )
            
            self.announcement_collection = self.db_client.get_or_create_collection(
                name="cooper_notice"
            )
            app.logger.info("æˆåŠŸåˆ›å»º/è·å–é›†åˆ")
        except Exception as e:
            app.logger.error(f"åˆ›å»ºé›†åˆæ—¶å‡ºé”™: {str(e)}")
            raise
            
        # åˆå§‹åŒ–LangChainæ–‡æœ¬åˆ†å‰²å™¨
        if self.use_langchain:
            self.text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap,
                length_function=len,
                separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼Œ", " ", ""]
            )
    
    def compute_embeddings(self, texts):
        """è®¡ç®—æ–‡æœ¬çš„åµŒå…¥å‘é‡"""
        try:
            # ç¡®ä¿è¾“å…¥æ˜¯åˆ—è¡¨
            if not isinstance(texts, list):
                texts = [texts]
                
            # è®¡ç®—åµŒå…¥å‘é‡
            embeddings = self.embedding_model.encode(texts)
            
            # è½¬æ¢ä¸ºåˆ—è¡¨
            if isinstance(embeddings, np.ndarray):
                return embeddings.tolist()
            elif isinstance(embeddings, list):
                # æ£€æŸ¥æ˜¯å¦æ˜¯numpyæ•°ç»„çš„åˆ—è¡¨
                if embeddings and isinstance(embeddings[0], np.ndarray):
                    return [emb.tolist() for emb in embeddings]
                return embeddings
            else:
                app.logger.warning(f"Warning: Unexpected embedding type: {type(embeddings)}")
                return [[0.0] * self.vector_dim]  # è¿”å›é›¶å‘é‡ä½œä¸ºåå¤‡
        except Exception as e:
            app.logger.error(f"è®¡ç®—åµŒå…¥å‘é‡æ—¶å‡ºé”™: {str(e)}")
            return [[0.0] * self.vector_dim]  # è¿”å›é›¶å‘é‡ä½œä¸ºåå¤‡
    
    def split_text(self, text: str) -> List[str]:
        """
        å°†é•¿æ–‡æœ¬åˆ†å‰²æˆè¾ƒå°çš„å—
        
        Args:
            text: è¦åˆ†å‰²çš„æ–‡æœ¬
            
        Returns:
            List[str]: åˆ†å‰²åçš„æ–‡æœ¬å—åˆ—è¡¨
        """
        if self.use_langchain:
            try:
                # ä½¿ç”¨LangChainçš„æ–‡æœ¬åˆ†å‰²å™¨
                chunks = self.text_splitter.split_text(text)
                # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªå—
                if not chunks:
                    chunks = [text]
                return chunks
            except Exception as e:
                app.logger.error(f"ä½¿ç”¨LangChainåˆ†å‰²æ–‡æœ¬æ—¶å‡ºé”™: {str(e)}")
                # å›é€€åˆ°ç®€å•åˆ†å‰²
                return [text]
        else:
            # ç®€å•çš„åŸºäºæ®µè½çš„åˆ†å‰²
            paragraphs = [p.strip() for p in text.split("\n\n") if p.strip()]
            
            # å¦‚æœæ®µè½å¾ˆå°‘ï¼Œç›´æ¥è¿”å›
            if len(paragraphs) <= 1:
                return [text]
                
            # åˆå¹¶çŸ­æ®µè½
            chunks = []
            current_chunk = ""
            
            for para in paragraphs:
                if len(current_chunk) + len(para) <= self.chunk_size:
                    current_chunk += (para + "\n\n")
                else:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = para + "\n\n"
            
            # æ·»åŠ æœ€åä¸€ä¸ªå—
            if current_chunk:
                chunks.append(current_chunk.strip())
            
            # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªå—
            if not chunks:
                chunks = [text]
                
            return chunks

    def search_news(self, query: str, n_results: int = 5) -> List[Dict]:
        """
        æœç´¢æ–°é—»
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            n_results: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            List[Dict]: æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            # è®¡ç®—æŸ¥è¯¢çš„åµŒå…¥å‘é‡
            query_embedding = self.compute_embeddings([query])[0]
            
            # ä½¿ç”¨å‘é‡æœç´¢
            results = self.news_collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results * 2,  # è·å–æ›´å¤šç»“æœï¼Œå› ä¸ºå¯èƒ½æœ‰é‡å¤çš„base_id
                include=["metadatas", "documents", "distances"]
            )
            
            # æ ¼å¼åŒ–ç»“æœ
            formatted_results = []
            if results["ids"] and results["ids"][0]:
                # æ”¶é›†ä¸åŒbase_idçš„ç¬¬ä¸€ä¸ªç»“æœ
                seen_base_ids = set()
                
                for i in range(len(results["ids"][0])):
                    base_id = results["metadatas"][0][i].get("base_id", results["ids"][0][i])
                    
                    # å¦‚æœå·²ç»åŒ…å«äº†è¿™ä¸ªbase_idçš„æ–‡æ¡£ï¼Œåˆ™è·³è¿‡
                    if base_id in seen_base_ids:
                        continue
                    
                    seen_base_ids.add(base_id)
                    
                    formatted_results.append({
                        "id": base_id,
                        "title": results["metadatas"][0][i]["title"],
                        "source": results["metadatas"][0][i]["source"],
                        "publish_date": results["metadatas"][0][i]["publish_date"],
                        "content": results["documents"][0][i],
                        "relevance_score": 1 - float(results["distances"][0][i]) if results["distances"] else 0.0
                    })
                    
                    # å¦‚æœå·²ç»æ”¶é›†äº†è¶³å¤Ÿçš„ä¸åŒæ–‡æ¡£ï¼Œå°±åœæ­¢
                    if len(formatted_results) >= n_results:
                        break
            
            return formatted_results
        except Exception as e:
            app.logger.error(f"æœç´¢æ–°é—»æ—¶å‡ºé”™: {str(e)}")
            return []
    
    def search_announcements(self, query: str, n_results: int = 5) -> List[Dict]:
        """
        æœç´¢å…¬å‘Š
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            n_results: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            List[Dict]: æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            # è®¡ç®—æŸ¥è¯¢çš„åµŒå…¥å‘é‡
            query_embedding = self.compute_embeddings([query])[0]
            
            # ä½¿ç”¨å‘é‡æœç´¢
            results = self.announcement_collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results * 2,  # è·å–æ›´å¤šç»“æœï¼Œå› ä¸ºå¯èƒ½æœ‰é‡å¤çš„base_id
                include=["metadatas", "documents", "distances"]
            )
            
            # æ ¼å¼åŒ–ç»“æœ
            formatted_results = []
            if results["ids"] and results["ids"][0]:
                # æ”¶é›†ä¸åŒbase_idçš„ç¬¬ä¸€ä¸ªç»“æœ
                seen_base_ids = set()
                
                for i in range(len(results["ids"][0])):
                    base_id = results["metadatas"][0][i].get("base_id", results["ids"][0][i])
                    
                    # å¦‚æœå·²ç»åŒ…å«äº†è¿™ä¸ªbase_idçš„æ–‡æ¡£ï¼Œåˆ™è·³è¿‡
                    if base_id in seen_base_ids:
                        continue
                    
                    seen_base_ids.add(base_id)
                    
                    formatted_results.append({
                        "id": base_id,
                        "title": results["metadatas"][0][i]["title"],
                        "department": results["metadatas"][0][i]["department"],
                        "publish_date": results["metadatas"][0][i]["publish_date"],
                        "importance": results["metadatas"][0][i]["importance"],
                        "content": results["documents"][0][i],
                        "relevance_score": 1 - float(results["distances"][0][i]) if results["distances"] else 0.0
                    })
                    
                    # å¦‚æœå·²ç»æ”¶é›†äº†è¶³å¤Ÿçš„ä¸åŒæ–‡æ¡£ï¼Œå°±åœæ­¢
                    if len(formatted_results) >= n_results:
                        break
            
            return formatted_results
        except Exception as e:
            app.logger.error(f"æœç´¢å…¬å‘Šæ—¶å‡ºé”™: {str(e)}")
            return []
    
    def search_all(self, query: str, n_results: int = 5) -> Dict[str, List[Dict]]:
        """
        åŒæ—¶æœç´¢æ–°é—»å’Œå…¬å‘Š
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            n_results: æ¯ç§ç±»å‹è¿”å›çš„ç»“æœæ•°é‡
            
        Returns:
            Dict[str, List[Dict]]: æœç´¢ç»“æœå­—å…¸ï¼ŒåŒ…å«æ–°é—»å’Œå…¬å‘Š
        """
        news_results = self.search_news(query, n_results)
        announcement_results = self.search_announcements(query, n_results)
        
        return {
            "news": news_results,
            "announcements": announcement_results
        }
    
    def generate_response(
        self, 
        query: str, 
        context: str, 
        temperature: float = 0.7,
        max_tokens: int = 1000
    ) -> str:
        """
        æ ¹æ®æŸ¥è¯¢å’Œä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            context: æ£€ç´¢çš„ä¸Šä¸‹æ–‡å†…å®¹
            temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶å›ç­”çš„åˆ›é€ æ€§ï¼Œå€¼è¶Šé«˜è¶Šåˆ›é€ æ€§
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°ï¼Œé™åˆ¶å›ç­”é•¿åº¦
            
        Returns:
            str: ç”Ÿæˆçš„å›ç­”
        """
        try:
            # è®¾ç½®ç³»ç»Ÿæç¤ºï¼ŒæŒ‡å¯¼å¤§æ¨¡å‹çš„è¡Œä¸º
            system_prompt = """
            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸­æ–‡æ–°é—»ä¸å…¬å‘Šæ™ºèƒ½åŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ï¼Œä¸è¦æ·»åŠ ä»»ä½•æœªåœ¨ä¸Šä¸‹æ–‡ä¸­æ˜ç¡®æåˆ°çš„ä¿¡æ¯ã€‚
            å›ç­”è¦æ±‚ï¼š
            1. ç®€æ´æ˜äº†ï¼šä¿æŒå›ç­”ç®€æ´ã€ç»“æ„æ¸…æ™°ï¼Œé‡ç‚¹çªå‡º
            2. ä¿¡æ¯å½’å› ï¼šå¼•ç”¨ä¿¡æ¯æ—¶æŒ‡æ˜æ¥æºï¼ˆä¾‹å¦‚"æ ¹æ®XXæ–°é—»æŠ¥é“/XXå…¬å‘Šé€šçŸ¥..."ï¼‰
            3. å¤„ç†ä¸ç¡®å®šæ€§ï¼šå¦‚æœä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è¶³æˆ–å­˜åœ¨çŸ›ç›¾ï¼Œæ˜ç¡®æŒ‡å‡ºå¹¶è¯´æ˜é™åˆ¶
            4. æ—¶æ•ˆæ€§æ ‡æ³¨ï¼šæåŠæ—¥æœŸå’Œæ—¶é—´ä¿¡æ¯æ—¶ï¼Œæ³¨æ˜ä¿¡æ¯çš„æ—¶é—´èƒŒæ™¯
            5. åŒºåˆ†å¤„ç†ï¼šæ–°é—»å†…å®¹ä»¥å®¢è§‚é™ˆè¿°ä¸ºä¸»ï¼Œå…¬å‘Šå†…å®¹éœ€å¼ºè°ƒå…¶å®˜æ–¹æ€§å’ŒæŒ‡å¯¼æ„ä¹‰

            å½“æ— æ³•ä»ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ—¶ï¼Œè¯·ç›´æ¥å›ç­”ï¼š"æ ¹æ®ç°æœ‰ä¿¡æ¯ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚è¯·é—®æ‚¨æ˜¯å¦æƒ³äº†è§£æˆ‘ä»¬ç³»ç»Ÿä¸­çš„å…¶ä»–æ–°é—»æˆ–å…¬å‘Šï¼Ÿ"

            å¯¹äºå¤æ‚è¯¢é—®ï¼Œå…ˆåˆ†æé—®é¢˜çš„æ ¸å¿ƒéœ€æ±‚ï¼Œå†ä»ä¸Šä¸‹æ–‡æå–ç›¸å…³ä¿¡æ¯ï¼Œç¡®ä¿å›ç­”å…¨é¢ä¸”å‡†ç¡®ã€‚
            """
            
            # è®¾ç½®ç”¨æˆ·æç¤ºï¼ŒåŒ…å«æŸ¥è¯¢å’Œä¸Šä¸‹æ–‡
            user_prompt = f"""ç”¨æˆ·é—®é¢˜: {query}
                        
                ----ä¸Šä¸‹æ–‡ä¿¡æ¯----
                {context}
                ----ä¸Šä¸‹æ–‡ä¿¡æ¯ç»“æŸ----

                åŸºäºä¸Šè¿°ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¯·å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è¶³ä»¥å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºã€‚"""

            # è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹APIç”Ÿæˆå›ç­”
            response = self.llm_client.chat.completions.create(
                model=self.llm_model,  # ä½¿ç”¨é…ç½®çš„æ¨¡å‹ï¼Œå¦‚"qwen-plus"
                messages=[
                    {"role": "system", "content": system_prompt},  # ç³»ç»Ÿè§’è‰²æç¤º
                    {"role": "user", "content": user_prompt}       # ç”¨æˆ·è§’è‰²æç¤º
                ],
                temperature=temperature,  # æ§åˆ¶å¤šæ ·æ€§
                max_tokens=max_tokens     # æ§åˆ¶å›ç­”é•¿åº¦
            )
            
            # æå–å¹¶è¿”å›ç”Ÿæˆçš„å›ç­”å†…å®¹
            return response.choices[0].message.content
        
        except Exception as e:
            # å¼‚å¸¸å¤„ç†ï¼šè®°å½•é”™è¯¯å¹¶è¿”å›é”™è¯¯ä¿¡æ¯
            app.logger.error(f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}")
            return f"ç”Ÿæˆå›ç­”æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
           
    def format_context(self, search_results: Dict[str, List[Dict]]) -> str:
        """
        å°†æœç´¢ç»“æœæ ¼å¼åŒ–ä¸ºä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç”¨äºLLMè¾“å…¥
        
        Args:
            search_results: æœç´¢ç»“æœ
            
        Returns:
            str: æ ¼å¼åŒ–åçš„ä¸Šä¸‹æ–‡
        """
        context = []
        
        # æ·»åŠ æ–°é—»
        if search_results["news"]:
            context.append("## ç›¸å…³æ–°é—»")
            for i, news in enumerate(search_results["news"]):
                context.append(f"{i+1}. æ ‡é¢˜: {news['title']}")
                context.append(f"   æ¥æº: {news['source']} ({news['publish_date']})")
                context.append(f"   å†…å®¹: {news['content']}")
                context.append("")
        
        # æ·»åŠ å…¬å‘Š
        if search_results["announcements"]:
            context.append("## ç›¸å…³å…¬å‘Š")
            for i, announcement in enumerate(search_results["announcements"]):
                importance_marker = "ğŸ”´" if announcement['importance'] == "high" else "ğŸŸ¢"
                context.append(f"{i+1}. {importance_marker} {announcement['title']}")
                context.append(f"   å‘å¸ƒ: {announcement['department']} ({announcement['publish_date']})")
                context.append(f"   å†…å®¹: {announcement['content']}")
                context.append("")
        
        if not context:
            return "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
            
        return "\n".join(context)
            
    def query(
        self, 
        query: str, 
        n_results: int = 3,
        temperature: float = 0.7, 
        max_tokens: int = 1000,
        use_hybrid_search: bool = None  # å¯é€‰å‚æ•°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼
    ) -> Dict:
        """
        ç«¯åˆ°ç«¯RAGæŸ¥è¯¢æµç¨‹
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            n_results: æ¯ç±»æ£€ç´¢çš„ç»“æœæ•°é‡
            temperature: LLMæ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            use_hybrid_search: æ˜¯å¦ä½¿ç”¨æ··åˆæœç´¢ï¼ˆå‘é‡+BM25ï¼‰ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨é…ç½®å€¼
            
        Returns:
            Dict: åŒ…å«æ£€ç´¢ç»“æœå’Œç”Ÿæˆçš„å›ç­”
        """
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        try:
            # ç¡®å®šæ˜¯å¦ä½¿ç”¨æ··åˆæœç´¢
            if use_hybrid_search is None:
                use_hybrid_search = self.use_hybrid_search and self.es_client and self.es_client.is_available
            
            # æ‰§è¡Œæ£€ç´¢
            if use_hybrid_search and self.es_client and self.es_client.is_available:
                search_results = self.hybrid_search_all(query, n_results)
                app.logger.info(f"ä½¿ç”¨æ··åˆæœç´¢ï¼ˆå‘é‡+BM25ï¼‰æ£€ç´¢ç»“æœ")
            else:
                search_results = self.search_all(query, n_results)
                app.logger.info(f"ä½¿ç”¨çº¯å‘é‡æ£€ç´¢ç»“æœ")
            
            # 2. æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
            context = self.format_context(search_results)
            
            # 3. ç”Ÿæˆå›ç­”
            answer = self.generate_response(
                query=query,
                context=context,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            # 4. è¿”å›ç»“æœ
            return {
                "query": query,
                "search_results": search_results,
                "context": context,
                "answer": answer,
                "search_type": "hybrid" if (use_hybrid_search and self.es_client and self.es_client.is_available) else "vector"
            }
        except Exception as e:
            app.logger.error(f"æŸ¥è¯¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            # è¿”å›åŸºæœ¬å“åº”
            return {
                "query": query,
                "search_results": {"news": [], "announcements": []},
                "context": "æŸ¥è¯¢å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯",
                "answer": f"å¾ˆæŠ±æ­‰ï¼Œåœ¨å¤„ç†æ‚¨çš„æŸ¥è¯¢æ—¶å‘ç”Ÿäº†é”™è¯¯: {str(e)}ã€‚è¯·ç¨åå†è¯•æˆ–è”ç³»ç®¡ç†å‘˜ã€‚",
                "search_type": "error"
            }

    def add_news_async(self, 
                    title: str, 
                    content: str, 
                    source: str = None, 
                    publish_date: str = None, 
                    tags: List[str] = None, 
                    id: str = None) -> str:
        """
        å¼‚æ­¥æ·»åŠ æ–°é—»æ–‡ç« ï¼Œå¤„ç†HTMLå†…å®¹
        
        Args:
            title: æ–°é—»æ ‡é¢˜
            content: æ–°é—»æ­£æ–‡ï¼ˆå¯èƒ½åŒ…å«HTMLï¼‰
            source: æ–°é—»æ¥æº
            publish_date: å‘å¸ƒæ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼‰
            tags: æ ‡ç­¾åˆ—è¡¨
            id: å”¯ä¸€IDï¼Œå¦‚æœæœªæä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆ
            
        Returns:
            str: ä»»åŠ¡ID
        """
        # å¦‚æœæ²¡æœ‰æä¾›IDï¼Œç”Ÿæˆä¸€ä¸ª
        base_id = id or str(uuid.uuid4())
        
        # å‡†å¤‡è¦åœ¨çº¿ç¨‹ä¸­æ‰§è¡Œçš„ä»»åŠ¡å‡½æ•°
        def process_and_add_task():
            try:
                original_content = content
                processed_content = original_content
                
                # æ£€æŸ¥contentæ˜¯å¦åŒ…å«HTMLå†…å®¹
                if '<' in original_content and '>' in original_content:
                    try:
                        # ä½¿ç”¨BeautifulSoupè§£æHTMLå†…å®¹
                        soup = BeautifulSoup(original_content, 'html.parser')
                        
                        # è·å–çº¯æ–‡æœ¬å†…å®¹
                        text_content = self.doc_processor.sanitize_html(original_content)
                        
                        # å¤„ç†åµŒå…¥çš„å›¾ç‰‡
                        embedded_contents = []
                        for img in soup.find_all('img'):
                            src = img.get('src', '')
                            if src and (src.startswith('http://') or src.startswith('https://')):
                                try:
                                    # ä¸‹è½½å›¾ç‰‡
                                    img_response = requests.get(src, timeout=10)
                                    if img_response.status_code == 200:
                                        # å¤„ç†å›¾ç‰‡ä¸­çš„æ–‡æœ¬
                                        img_text, confidence = self.doc_processor.process_image(img_response.content)
                                        if img_text and img_text != "No text detected in image":
                                            embedded_contents.append(f"ã€å›¾ç‰‡å†…å®¹ã€‘: {img_text}")
                                except Exception as e:
                                    app.logger.warning(f"å¤„ç†åµŒå…¥å›¾ç‰‡æ—¶å‡ºé”™: {str(e)}")
                        
                        # å¤„ç†åµŒå…¥çš„æ–‡æ¡£é“¾æ¥
                        for a in soup.find_all('a'):
                            href = a.get('href', '')
                            if href and (href.endswith('.pdf') or href.endswith('.docx') or href.endswith('.doc')):
                                try:
                                    # ä¸‹è½½æ–‡æ¡£
                                    doc_response = requests.get(href, timeout=20)
                                    if doc_response.status_code == 200:
                                        # æ ¹æ®æ–‡ä»¶ç±»å‹å¤„ç†æ–‡æ¡£
                                        if href.endswith('.pdf'):
                                            doc_text = self.doc_processor.process_pdf(doc_response.content)
                                            embedded_contents.append(f"ã€PDFæ–‡æ¡£å†…å®¹ã€‘: {doc_text}")
                                        elif href.endswith('.docx') or href.endswith('.doc'):
                                            doc_text = self.doc_processor.process_word_document(doc_response.content)
                                            embedded_contents.append(f"ã€Wordæ–‡æ¡£å†…å®¹ã€‘: {doc_text}")
                                except Exception as e:
                                    app.logger.warning(f"å¤„ç†åµŒå…¥æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")
                        
                        # ç»„åˆæ‰€æœ‰å†…å®¹
                        if embedded_contents:
                            extracted_content = "\n\n".join(embedded_contents)
                            processed_content = f"{text_content}\n\n{extracted_content}"
                        else:
                            processed_content = text_content
                            
                    except Exception as e:
                        app.logger.error(f"å¤„ç†HTMLå†…å®¹æ—¶å‡ºé”™: {str(e)}")
                        # å¤±è´¥æ—¶è‡³å°‘æ¸…ç†HTMLæ ‡ç­¾
                        processed_content = self.doc_processor.sanitize_html(original_content)
                
                # å‡†å¤‡å…ƒæ•°æ®
                metadata = {
                    "title": title,
                    "source": source or "æœªçŸ¥æ¥æº",
                    "publish_date": publish_date or datetime.now().strftime("%Y-%m-%d"),
                    "type": "news",
                }
                
                if tags:
                    if isinstance(tags, str):
                        metadata["tags"] = tags
                    else:
                        metadata["tags"] = ",".join(tags)
                
                # åˆ†å‰²é•¿æ–‡æœ¬
                document_chunks = self.split_text(processed_content)
                app.logger.info(f"å°†æ–°é—»æ–‡ç«  '{title}' åˆ†å‰²ä¸º {len(document_chunks)} ä¸ªå—")
                
                all_doc_ids = []
                
                for i, chunk in enumerate(document_chunks):
                    # ä¸ºæ¯ä¸ªå—ç”Ÿæˆå”¯ä¸€ID
                    doc_id = f"{base_id}_{i}" if len(document_chunks) > 1 else base_id
                    
                    # å‡†å¤‡æ–‡æ¡£å†…å®¹ (ä¸ºå—æ·»åŠ æ ‡é¢˜ä»¥ä¿æŒä¸Šä¸‹æ–‡)
                    document = f"{title}\n{chunk}"
                    
                    # ä¸ºè¿™ä¸ªå—æ›´æ–°å…ƒæ•°æ®
                    chunk_metadata = metadata.copy()
                    chunk_metadata["chunk_index"] = i
                    chunk_metadata["total_chunks"] = len(document_chunks)
                    chunk_metadata["base_id"] = base_id
                    
                    # è®¡ç®—åµŒå…¥å‘é‡
                    embeddings = self.compute_embeddings([document])
                    
                    # æ·»åŠ åˆ°é›†åˆ
                    try:
                        self.news_collection.add(
                            documents=[document],
                            metadatas=[chunk_metadata],
                            ids=[doc_id],
                            embeddings=embeddings
                        )
                        all_doc_ids.append(doc_id)
                    except Exception as e:
                        app.logger.error(f"æ·»åŠ æ–°é—»å— {i+1}/{len(document_chunks)} æ—¶å‡ºé”™: {str(e)}")
                        # ç»§ç»­æ·»åŠ å…¶ä»–å—
                        continue
                
                if all_doc_ids:
                    app.logger.info(f"æˆåŠŸæ·»åŠ æ–°é—»: {title}ï¼Œåˆ†ä¸º {len(all_doc_ids)}/{len(document_chunks)} ä¸ªå—")
                    if self.es_client and self.es_client.is_available:
                        for i, chunk in enumerate(document_chunks):
                            doc_id = f"{base_id}_{i}" if len(document_chunks) > 1 else base_id
                            
                            # å‡†å¤‡ESæ–‡æ¡£
                            es_document = {
                                "id": doc_id,
                                "title": title,
                                "content": chunk,
                                "source": source or "æœªçŸ¥æ¥æº",
                                "publish_date": publish_date or datetime.now().strftime("%Y-%m-%d"),
                                "base_id": base_id,
                                "chunk_index": i,
                                "total_chunks": len(document_chunks)
                            }
                            
                            if tags:
                                if isinstance(tags, list):
                                    es_document["tags"] = ",".join(tags)
                                else:
                                    es_document["tags"] = tags
                            
                            # å¼‚æ­¥ç´¢å¼•åˆ°ES (ä¸è¦åœ¨çº¿ç¨‹ä¸­åˆ›å»ºæ–°çº¿ç¨‹ï¼Œç›´æ¥è°ƒç”¨)
                            self.es_client.index_news(es_document, async_mode=True)
                    

                    app.logger.info(f"å¼‚æ­¥æˆåŠŸæ·»åŠ æ–°é—»: {title}ï¼Œå·²åŒæ—¶ç´¢å¼•åˆ°ESæœåŠ¡")
                    
                    return base_id
                else:
                    raise Exception("æ·»åŠ æ–°é—»æ—¶æ‰€æœ‰å—éƒ½å¤±è´¥")
            
            except Exception as e:
                app.logger.error(f"å¼‚æ­¥æ·»åŠ æ–°é—»æ—¶å‡ºé”™: {str(e)}")
                raise
        
        # æäº¤åˆ°ä»»åŠ¡ç®¡ç†å™¨
        task_id = task_manager.submit_task(process_and_add_task)
        app.logger.info(f"æäº¤æ·»åŠ æ–°é—»ä»»åŠ¡: {task_id}, æ–‡æ¡£åŸºç¡€ID: {base_id}")
        
        return task_id

    def add_announcement_async(self, 
                            title: str, 
                            content: str, 
                            department: str = None, 
                            publish_date: str = None,
                            importance: str = "normal",
                            id: str = None) -> str:
        """
        å¼‚æ­¥æ·»åŠ å…¬å‘Šï¼Œå¤„ç†HTMLå†…å®¹
        
        Args:
            title: å…¬å‘Šæ ‡é¢˜
            content: å…¬å‘Šå†…å®¹ï¼ˆå¯èƒ½åŒ…å«HTMLï¼‰
            department: å‘å¸ƒéƒ¨é—¨
            publish_date: å‘å¸ƒæ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼‰
            importance: é‡è¦æ€§ï¼ˆhigh, normal, lowï¼‰
            id: å”¯ä¸€IDï¼Œå¦‚æœæœªæä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆ
            
        Returns:
            str: ä»»åŠ¡ID
        """
        # å¦‚æœæ²¡æœ‰æä¾›IDï¼Œç”Ÿæˆä¸€ä¸ª
        base_id = id or str(uuid.uuid4())
        
        # å‡†å¤‡è¦åœ¨çº¿ç¨‹ä¸­æ‰§è¡Œçš„ä»»åŠ¡å‡½æ•°
        def process_and_add_task():
            try:
                original_content = content
                processed_content = original_content
                
                # æ£€æŸ¥contentæ˜¯å¦åŒ…å«HTMLå†…å®¹
                if '<' in original_content and '>' in original_content:
                    try:
                        # ä½¿ç”¨BeautifulSoupè§£æHTMLå†…å®¹
                        soup = BeautifulSoup(original_content, 'html.parser')
                        
                        # è·å–çº¯æ–‡æœ¬å†…å®¹
                        text_content = self.doc_processor.sanitize_html(original_content)
                        
                        # å¤„ç†åµŒå…¥çš„å›¾ç‰‡
                        embedded_contents = []
                        for img in soup.find_all('img'):
                            src = img.get('src', '')
                            if src and (src.startswith('http://') or src.startswith('https://')):
                                try:
                                    # ä¸‹è½½å›¾ç‰‡
                                    img_response = requests.get(src, timeout=10)
                                    if img_response.status_code == 200:
                                        # å¤„ç†å›¾ç‰‡ä¸­çš„æ–‡æœ¬
                                        img_text, confidence = self.doc_processor.process_image(img_response.content)
                                        if img_text and img_text != "No text detected in image":
                                            embedded_contents.append(f"ã€å›¾ç‰‡å†…å®¹ã€‘: {img_text}")
                                except Exception as e:
                                    app.logger.warning(f"å¤„ç†åµŒå…¥å›¾ç‰‡æ—¶å‡ºé”™: {str(e)}")
                        
                        # å¤„ç†åµŒå…¥çš„æ–‡æ¡£é“¾æ¥
                        for a in soup.find_all('a'):
                            href = a.get('href', '')
                            if href and (href.endswith('.pdf') or href.endswith('.docx') or href.endswith('.doc')):
                                try:
                                    # ä¸‹è½½æ–‡æ¡£
                                    doc_response = requests.get(href, timeout=20)
                                    if doc_response.status_code == 200:
                                        # æ ¹æ®æ–‡ä»¶ç±»å‹å¤„ç†æ–‡æ¡£
                                        if href.endswith('.pdf'):
                                            doc_text = self.doc_processor.process_pdf(doc_response.content)
                                            embedded_contents.append(f"ã€PDFæ–‡æ¡£å†…å®¹ã€‘: {doc_text}")
                                        elif href.endswith('.docx') or href.endswith('.doc'):
                                            doc_text = self.doc_processor.process_word_document(doc_response.content)
                                            embedded_contents.append(f"ã€Wordæ–‡æ¡£å†…å®¹ã€‘: {doc_text}")
                                except Exception as e:
                                    app.logger.warning(f"å¤„ç†åµŒå…¥æ–‡æ¡£æ—¶å‡ºé”™: {str(e)}")
                        
                        # ç»„åˆæ‰€æœ‰å†…å®¹
                        if embedded_contents:
                            extracted_content = "\n\n".join(embedded_contents)
                            processed_content = f"{text_content}\n\n{extracted_content}"
                        else:
                            processed_content = text_content
                            
                    except Exception as e:
                        app.logger.error(f"å¤„ç†HTMLå†…å®¹æ—¶å‡ºé”™: {str(e)}")
                        # å¤±è´¥æ—¶è‡³å°‘æ¸…ç†HTMLæ ‡ç­¾
                        processed_content = self.doc_processor.sanitize_html(original_content)
                
                # å‡†å¤‡å…ƒæ•°æ®
                metadata = {
                    "title": title,
                    "department": department or "æœªçŸ¥éƒ¨é—¨",
                    "publish_date": publish_date or datetime.now().strftime("%Y-%m-%d"),
                    "importance": importance,
                    "type": "announcement",
                }
                
                # åˆ†å‰²é•¿æ–‡æœ¬
                document_chunks = self.split_text(processed_content)
                app.logger.info(f"å°†å…¬å‘Š '{title}' åˆ†å‰²ä¸º {len(document_chunks)} ä¸ªå—")
                
                all_doc_ids = []
                
                for i, chunk in enumerate(document_chunks):
                    # ä¸ºæ¯ä¸ªå—ç”Ÿæˆå”¯ä¸€ID
                    doc_id = f"{base_id}_{i}" if len(document_chunks) > 1 else base_id
                    
                    # å‡†å¤‡æ–‡æ¡£å†…å®¹ (ä¸ºå—æ·»åŠ æ ‡é¢˜ä»¥ä¿æŒä¸Šä¸‹æ–‡)
                    document = f"{title}\n{chunk}"
                    
                    # ä¸ºè¿™ä¸ªå—æ›´æ–°å…ƒæ•°æ®
                    chunk_metadata = metadata.copy()
                    chunk_metadata["chunk_index"] = i
                    chunk_metadata["total_chunks"] = len(document_chunks)
                    chunk_metadata["base_id"] = base_id
                    
                    # è®¡ç®—åµŒå…¥å‘é‡
                    embeddings = self.compute_embeddings([document])
                    
                    # æ·»åŠ åˆ°é›†åˆ
                    try:
                        self.announcement_collection.add(
                            documents=[document],
                            metadatas=[chunk_metadata],
                            ids=[doc_id],
                            embeddings=embeddings
                        )
                        all_doc_ids.append(doc_id)
                    except Exception as e:
                        app.logger.error(f"æ·»åŠ å…¬å‘Šå— {i+1}/{len(document_chunks)} æ—¶å‡ºé”™: {str(e)}")
                        # ç»§ç»­æ·»åŠ å…¶ä»–å—
                        continue
                
                if all_doc_ids:
                    app.logger.info(f"æˆåŠŸæ·»åŠ å…¬å‘Š: {title}ï¼Œåˆ†ä¸º {len(all_doc_ids)}/{len(document_chunks)} ä¸ªå—")
                    if self.es_client and self.es_client.is_available:
                        for i, chunk in enumerate(document_chunks):
                            doc_id = f"{base_id}_{i}" if len(document_chunks) > 1 else base_id
                            
                            # å‡†å¤‡ESæ–‡æ¡£
                            es_document = {
                                "id": doc_id,
                                "title": title,
                                "content": chunk,
                                "department": department or "æœªçŸ¥éƒ¨é—¨",
                                "publish_date": publish_date or datetime.now().strftime("%Y-%m-%d"),
                                "importance": importance,
                                "base_id": base_id,
                                "chunk_index": i,
                                "total_chunks": len(document_chunks)
                            }
                            
                            # å¼‚æ­¥ç´¢å¼•åˆ°ES (ä¸è¦åœ¨çº¿ç¨‹ä¸­åˆ›å»ºæ–°çº¿ç¨‹ï¼Œç›´æ¥è°ƒç”¨)
                            self.es_client.index_notice(es_document, async_mode=True)

                    app.logger.info(f"å¼‚æ­¥æˆåŠŸæ·»åŠ å…¬å‘Š: {title}ï¼Œå·²åŒæ—¶ç´¢å¼•åˆ°ESæœåŠ¡")
                    return base_id
                else:
                    raise Exception("æ·»åŠ å…¬å‘Šæ—¶æ‰€æœ‰å—éƒ½å¤±è´¥")
            
            except Exception as e:
                app.logger.error(f"å¼‚æ­¥æ·»åŠ å…¬å‘Šæ—¶å‡ºé”™: {str(e)}")
                raise
        
        # æäº¤åˆ°ä»»åŠ¡ç®¡ç†å™¨
        task_id = task_manager.submit_task(process_and_add_task)
        app.logger.info(f"æäº¤æ·»åŠ å…¬å‘Šä»»åŠ¡: {task_id}, æ–‡æ¡£åŸºç¡€ID: {base_id}")
        
        return task_id

    def hybrid_search_all(self, query: str, n_results: int = 5) -> Dict[str, List[Dict]]:
        """
        æ··åˆæœç´¢ï¼šä½¿ç”¨å‘é‡æ£€ç´¢å’ŒBM25èåˆæœç´¢ç»“æœ
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            n_results: æ¯ç§ç±»å‹è¿”å›çš„ç»“æœæ•°é‡
            
        Returns:
            Dict[str, List[Dict]]: èåˆåçš„æœç´¢ç»“æœ
        """
        # 1. å‘é‡æ£€ç´¢
        vector_results = self.search_all(query, n_results)
        
        # 2. å¦‚æœESæœåŠ¡å¯ç”¨ï¼Œæ‰§è¡ŒBM25æ£€ç´¢
        if self.es_client and self.es_client.is_available:
            bm25_results = self.es_client.search_all(query, n_results)
            
            # 3. èåˆç»“æœ
            try:
                fused_results = RankFusion.contextual_fusion(
                    query=query,
                    dense_results=vector_results, 
                    lexical_results=bm25_results
                )
                return fused_results
            except Exception as e:
                app.logger.error(f"èåˆæœç´¢ç»“æœæ—¶å‡ºé”™: {str(e)}")
                # å‡ºé”™æ—¶å›é€€åˆ°å‘é‡æ£€ç´¢ç»“æœ
                return vector_results
        
        # å¦‚æœESæœåŠ¡ä¸å¯ç”¨ï¼Œç›´æ¥è¿”å›å‘é‡æ£€ç´¢ç»“æœ
        return vector_results

# åˆå§‹åŒ–RAGç³»ç»Ÿ
# ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
EMBEDDING_MODEL_PATH = os.environ.get('EMBEDDING_MODEL_PATH', '/models/sentence-transformers_text2vec-large-chinese')
LLM_API_KEY = os.environ.get('LLM_API_KEY', 'your_openai_key_here')
LLM_BASE_URL = os.environ.get('LLM_BASE_URL', 'https://dashscope.aliyuncs.com/compatible-mode/v1')
LLM_MODEL = os.environ.get('LLM_MODEL', 'qwen-plus')
CHROMA_HOST = os.environ.get('CHROMA_HOST','192.168.222.128')
CHROMA_PORT = os.environ.get('CHROMA_PORT','8000')
USE_LANGCHAIN = os.environ.get('USE_LANGCHAIN', 'true').lower() == 'true'
CHUNK_SIZE = int(os.environ.get('CHUNK_SIZE', '1000'))
CHUNK_OVERLAP = int(os.environ.get('CHUNK_OVERLAP', '200'))
USE_HYBRID_SEARCH = os.environ.get('use_hybrid_search', 'true').lower() == 'true'
ES_SERVICE_URL = os.environ.get('es_service_url', 'http://192.168.222.128:8085')

if CHROMA_PORT and CHROMA_PORT.isdigit():
    CHROMA_PORT = int(CHROMA_PORT)
else:
    CHROMA_PORT = None

# åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–RAGç³»ç»Ÿ
rag_system = None

# æ›¿æ¢ä¹‹å‰çš„ @app.before_first_request
with app.app_context():
    try:
        app.logger.info("åˆå§‹åŒ–RAGç³»ç»Ÿ...")
        rag_system = ChineseRAGSystem(
            embedding_model_path=EMBEDDING_MODEL_PATH,
            llm_api_key=LLM_API_KEY,
            llm_base_url=LLM_BASE_URL,
            llm_model=LLM_MODEL,
            chroma_host=CHROMA_HOST,
            chroma_port=CHROMA_PORT,
            use_langchain=USE_LANGCHAIN,
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            use_hybrid_search = USE_HYBRID_SEARCH,
            es_service_url = ES_SERVICE_URL if USE_HYBRID_SEARCH else None
        )

        # æ£€æŸ¥ESæœåŠ¡çŠ¶æ€
        if USE_HYBRID_SEARCH:
            if rag_system.es_client and rag_system.es_client.is_available:
                app.logger.info(f"ESæœåŠ¡å¯ç”¨ï¼Œæ··åˆæœç´¢å·²å¯ç”¨")
            else:
                app.logger.warning(f"ESæœåŠ¡ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨çº¯å‘é‡æœç´¢")
        
        app.logger.info("RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        app.logger.error(f"åˆå§‹åŒ–RAGç³»ç»Ÿæ—¶å‡ºé”™: {str(e)}")


# åˆ›å»ºä»»åŠ¡ç®¡ç†å™¨
task_manager = AsyncTaskManager(max_workers=3)  # è®¾ç½®3ä¸ªå·¥ä½œçº¿ç¨‹

# æ³¨å†Œè·¯ç”±
@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return jsonify({
        "status": "healthy", 
        "timestamp": datetime.now().isoformat(),
        "system_info": {
            "use_langchain": USE_LANGCHAIN,
            "chunk_size": CHUNK_SIZE,
            "chunk_overlap": CHUNK_OVERLAP,
            "numpy_version": np.__version__
        }
    })

@app.route('/query', methods=['POST'])
def query_endpoint():
    """æŸ¥è¯¢æ¥å£"""
    if not rag_system:
        return jsonify({"error": "RAGç³»ç»Ÿå°šæœªåˆå§‹åŒ–"}), 500
    
    data = request.json
    if not data or 'query' not in data:
        return jsonify({"error": "è¯·æä¾›æŸ¥è¯¢å†…å®¹"}), 400
    
    query = data['query']
    n_results = data.get('n_results', 3)
    temperature = data.get('temperature', 0.7)
    max_tokens = data.get('max_tokens', 1000)
    
    try:
        result = rag_system.query(
            query=query,
            n_results=n_results,
            temperature=temperature,
            max_tokens=max_tokens
        )
        # ä½¿ç”¨ensure_ascii=Falseç¡®ä¿ä¸­æ–‡å­—ç¬¦ä¸ä¼šè¢«ç¼–ç æˆUnicodeè½¬ä¹‰åºåˆ—
        return app.response_class(
            response=json.dumps(result, ensure_ascii=False),
            status=200,
            mimetype='application/json'
        )
    except Exception as e:
        app.logger.error(f"å¤„ç†æŸ¥è¯¢è¯·æ±‚æ—¶å‡ºé”™: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/add/news', methods=['POST'])
def add_news_endpoint():
    """æ·»åŠ æ–°é—»æ¥å£"""
    if not rag_system:
        return jsonify({"error": "RAGç³»ç»Ÿå°šæœªåˆå§‹åŒ–"}), 500
    
    # æ£€æŸ¥è¡¨å•æˆ–JSONæ•°æ®
    if request.is_json:
        data = request.json
    else:
        data = request.form.to_dict()
    
    # éªŒè¯å¿…è¦å­—æ®µ
    if not data or 'title' not in data:
        return jsonify({"error": "è¯·æä¾›æ ‡é¢˜"}), 400
    
    if 'content' not in data:
        return jsonify({"error": "è¯·æä¾›å†…å®¹"}), 400
    
    try:
        # å‡†å¤‡æ ‡ç­¾
        tags = None
        if 'tags' in data and data['tags']:
            if isinstance(data['tags'], str):
                tags = [tag.strip() for tag in data['tags'].split(',') if tag.strip()]
            elif isinstance(data['tags'], list):
                tags = data['tags']
        
        # å¼‚æ­¥æ·»åŠ æ–°é—»
        task_id = rag_system.add_news_async(
            title=data['title'],
            content=data['content'],
            source=data.get('source'),
            publish_date=data.get('publish_date'),
            tags=tags
        )
        
        # è¿”å›ä»»åŠ¡IDå’ŒæˆåŠŸæ¶ˆæ¯
        return jsonify({
            "success": True, 
            "message": "æ–°é—»æ­£åœ¨å¼‚æ­¥å¤„ç†ä¸­ï¼ŒåŒ…æ‹¬HTMLå†…å®¹æå–å’ŒåµŒå…¥å›¾ç‰‡/æ–‡æ¡£çš„å¤„ç†",
            "task_id": task_id
        })
    except Exception as e:
        app.logger.error(f"æ·»åŠ æ–°é—»æ—¶å‡ºé”™: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/add/announcement', methods=['POST'])
def add_announcement_endpoint():
    """æ·»åŠ å…¬å‘Šæ¥å£"""
    if not rag_system:
        return jsonify({"error": "RAGç³»ç»Ÿå°šæœªåˆå§‹åŒ–"}), 500
    
    # æ£€æŸ¥è¡¨å•æˆ–JSONæ•°æ®
    if request.is_json:
        data = request.json
    else:
        data = request.form.to_dict()
    
    # éªŒè¯å¿…è¦å­—æ®µ
    if not data or 'title' not in data:
        return jsonify({"error": "è¯·æä¾›æ ‡é¢˜"}), 400
    
    if 'content' not in data:
        return jsonify({"error": "è¯·æä¾›å†…å®¹"}), 400
    
    try:
        # å¼‚æ­¥æ·»åŠ å…¬å‘Š
        task_id = rag_system.add_announcement_async(
            title=data['title'],
            content=data['content'],
            department=data.get('department'),
            publish_date=data.get('publish_date'),
            importance=data.get('importance', 'normal')
        )
        
        # è¿”å›ä»»åŠ¡IDå’ŒæˆåŠŸæ¶ˆæ¯
        return jsonify({
            "success": True, 
            "message": "å…¬å‘Šæ­£åœ¨å¼‚æ­¥å¤„ç†ä¸­ï¼ŒåŒ…æ‹¬HTMLå†…å®¹æå–å’ŒåµŒå…¥å›¾ç‰‡/æ–‡æ¡£çš„å¤„ç†",
            "task_id": task_id
        })
    except Exception as e:
        app.logger.error(f"æ·»åŠ å…¬å‘Šæ—¶å‡ºé”™: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/task/status/<task_id>', methods=['GET'])
def task_status_endpoint(task_id):
    """æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€æ¥å£"""
    status = task_manager.get_task_status(task_id)
    return jsonify(status)

@app.route('/delete/news/<doc_id>', methods=['DELETE'])
def delete_news_endpoint(doc_id):
    """åˆ é™¤æŒ‡å®šIDçš„æ–°é—»"""
    if not rag_system:
        return jsonify({"error": "RAGç³»ç»Ÿå°šæœªåˆå§‹åŒ–"}), 500
    
    try:
        # æ£€æŸ¥doc_idæ˜¯å¦åŒ…å«å—ç´¢å¼•
        if '_' in doc_id:
            base_id = doc_id.split('_')[0]
            # æŸ¥æ‰¾æ‰€æœ‰ç›¸å…³å—
            query = f"base_id:'{base_id}'"
            results = rag_system.news_collection.get(
                where={"base_id": base_id}
            )
            
            if not results["ids"]:
                return jsonify({"error": f"æ‰¾ä¸åˆ°IDä¸º{base_id}çš„æ–°é—»"}), 404
            
            # åˆ é™¤æ‰€æœ‰ç›¸å…³å—
            rag_system.news_collection.delete(ids=results["ids"])
            app.logger.info(f"æˆåŠŸåˆ é™¤æ–°é—»ID: {base_id} çš„æ‰€æœ‰ {len(results['ids'])} ä¸ªå—")
            
            return jsonify({"success": True, "message": f"æˆåŠŸåˆ é™¤æ–°é—»ID: {base_id} çš„æ‰€æœ‰ {len(results['ids'])} ä¸ªå—"})
        else:
            # å¯¹äºå•å—æ–‡æ¡£æˆ–å°è¯•åˆ é™¤æ‰€æœ‰ç›¸å…³å—
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯çˆ¶ID
            results = rag_system.news_collection.get(
                where={"base_id": doc_id}
            )
            
            if results["ids"]:
                # è¿™æ˜¯ä¸€ä¸ªçˆ¶IDï¼Œåˆ é™¤æ‰€æœ‰ç›¸å…³å—
                rag_system.news_collection.delete(ids=results["ids"])
                app.logger.info(f"æˆåŠŸåˆ é™¤æ–°é—»ID: {doc_id} çš„æ‰€æœ‰ {len(results['ids'])} ä¸ªå—")
                return jsonify({"success": True, "message": f"æˆåŠŸåˆ é™¤æ–°é—»ID: {doc_id} çš„æ‰€æœ‰ {len(results['ids'])} ä¸ªå—"})
            else:
                # å°è¯•ç›´æ¥åˆ é™¤è¯¥ID
                direct_results = rag_system.news_collection.get(ids=[doc_id], include=[])
                if not direct_results["ids"]:
                    return jsonify({"error": f"æ‰¾ä¸åˆ°IDä¸º{doc_id}çš„æ–°é—»"}), 404
                
                # åˆ é™¤æ–‡æ¡£
                rag_system.news_collection.delete(ids=[doc_id])
                app.logger.info(f"æˆåŠŸåˆ é™¤æ–°é—»ID: {doc_id}")
                
                return jsonify({"success": True, "message": f"æˆåŠŸåˆ é™¤æ–°é—»ID: {doc_id}"})
    except Exception as e:
        app.logger.error(f"åˆ é™¤æ–°é—»æ—¶å‡ºé”™: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/delete/announcement/<doc_id>', methods=['DELETE'])
def delete_announcement_endpoint(doc_id):
    """åˆ é™¤æŒ‡å®šIDçš„å…¬å‘Š"""
    if not rag_system:
        return jsonify({"error": "RAGç³»ç»Ÿå°šæœªåˆå§‹åŒ–"}), 500
    
    try:
        # æ£€æŸ¥doc_idæ˜¯å¦åŒ…å«å—ç´¢å¼•
        if '_' in doc_id:
            base_id = doc_id.split('_')[0]
            # æŸ¥æ‰¾æ‰€æœ‰ç›¸å…³å—
            results = rag_system.announcement_collection.get(
                where={"base_id": base_id}
            )
            
            if not results["ids"]:
                return jsonify({"error": f"æ‰¾ä¸åˆ°IDä¸º{base_id}çš„å…¬å‘Š"}), 404
            
            # åˆ é™¤æ‰€æœ‰ç›¸å…³å—
            rag_system.announcement_collection.delete(ids=results["ids"])
            app.logger.info(f"æˆåŠŸåˆ é™¤å…¬å‘ŠID: {base_id} çš„æ‰€æœ‰ {len(results['ids'])} ä¸ªå—")
            
            return jsonify({"success": True, "message": f"æˆåŠŸåˆ é™¤å…¬å‘ŠID: {base_id} çš„æ‰€æœ‰ {len(results['ids'])} ä¸ªå—"})
        else:
            # å¯¹äºå•å—æ–‡æ¡£æˆ–å°è¯•åˆ é™¤æ‰€æœ‰ç›¸å…³å—
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯çˆ¶ID
            results = rag_system.announcement_collection.get(
                where={"base_id": doc_id}
            )
            
            if results["ids"]:
                # è¿™æ˜¯ä¸€ä¸ªçˆ¶IDï¼Œåˆ é™¤æ‰€æœ‰ç›¸å…³å—
                rag_system.announcement_collection.delete(ids=results["ids"])
                app.logger.info(f"æˆåŠŸåˆ é™¤å…¬å‘ŠID: {doc_id} çš„æ‰€æœ‰ {len(results['ids'])} ä¸ªå—")
                return jsonify({"success": True, "message": f"æˆåŠŸåˆ é™¤å…¬å‘ŠID: {doc_id} çš„æ‰€æœ‰ {len(results['ids'])} ä¸ªå—"})
            else:
                # å°è¯•ç›´æ¥åˆ é™¤è¯¥ID
                direct_results = rag_system.announcement_collection.get(ids=[doc_id], include=[])
                if not direct_results["ids"]:
                    return jsonify({"error": f"æ‰¾ä¸åˆ°IDä¸º{doc_id}çš„å…¬å‘Š"}), 404
                
                # åˆ é™¤æ–‡æ¡£
                rag_system.announcement_collection.delete(ids=[doc_id])
                app.logger.info(f"æˆåŠŸåˆ é™¤å…¬å‘ŠID: {doc_id}")
                
                return jsonify({"success": True, "message": f"æˆåŠŸåˆ é™¤å…¬å‘ŠID: {doc_id}"})
    except Exception as e:
        app.logger.error(f"åˆ é™¤å…¬å‘Šæ—¶å‡ºé”™: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/process/file', methods=['POST'])
def process_file_endpoint():
    """å¤„ç†æ–‡ä»¶æå–å†…å®¹æ¥å£"""
    if not rag_system:
        return jsonify({"error": "RAGç³»ç»Ÿå°šæœªåˆå§‹åŒ–"}), 500
    
    if 'file' not in request.files:
        return jsonify({"error": "è¯·æä¾›æ–‡ä»¶"}), 400
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({"error": "æœªé€‰æ‹©æ–‡ä»¶"}), 400
    
    try:
        file_data = file.read()
        file_type = file.content_type or os.path.splitext(file.filename)[1]
        
        extracted_content = rag_system.doc_processor.get_file_content(file_data, file_type)
        
        return jsonify({
            "success": True, 
            "filename": file.filename,
            "file_type": file_type,
            "content": extracted_content,
            "content_length": len(extracted_content)
        })
    except Exception as e:
        app.logger.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/process/html', methods=['POST'])
def process_html_endpoint():
    """å¤„ç†HTMLå†…å®¹æ¥å£"""
    if not rag_system:
        return jsonify({"error": "RAGç³»ç»Ÿå°šæœªåˆå§‹åŒ–"}), 500
    
    data = request.json
    if not data or 'html' not in data:
        return jsonify({"error": "è¯·æä¾›HTMLå†…å®¹"}), 400
    
    try:
        html_content = data['html']
        sanitized_content = rag_system.doc_processor.sanitize_html(html_content)
        
        return jsonify({
            "success": True,
            "content": sanitized_content,
            "content_length": len(sanitized_content)
        })
    except Exception as e:
        app.logger.error(f"å¤„ç†HTMLå†…å®¹æ—¶å‡ºé”™: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/hybrid_query', methods=['POST'])
def hybrid_query_endpoint():
    """æ··åˆæŸ¥è¯¢æ¥å£ï¼ˆå‘é‡+BM25ï¼‰"""
    if not rag_system:
        return jsonify({"error": "RAGç³»ç»Ÿå°šæœªåˆå§‹åŒ–"}), 500
    
    data = request.json
    if not data or 'query' not in data:
        return jsonify({"error": "è¯·æä¾›æŸ¥è¯¢å†…å®¹"}), 400
    
    query = data['query']
    n_results = data.get('n_results', 3)
    temperature = data.get('temperature', 0.7)
    max_tokens = data.get('max_tokens', 1000)
    
    try:
        result = rag_system.query(
            query=query,
            n_results=n_results,
            temperature=temperature,
            max_tokens=max_tokens,
            use_hybrid_search=True  # å¼ºåˆ¶ä½¿ç”¨æ··åˆæœç´¢
        )
        # ä½¿ç”¨ensure_ascii=Falseç¡®ä¿ä¸­æ–‡å­—ç¬¦ä¸ä¼šè¢«ç¼–ç æˆUnicodeè½¬ä¹‰åºåˆ—
        return app.response_class(
            response=json.dumps(result, ensure_ascii=False),
            status=200,
            mimetype='application/json'
        )
    except Exception as e:
        app.logger.error(f"å¤„ç†æ··åˆæŸ¥è¯¢è¯·æ±‚æ—¶å‡ºé”™: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/es_status', methods=['GET'])
def es_status_endpoint():
    """ESæœåŠ¡çŠ¶æ€æ¥å£"""
    if not rag_system:
        return jsonify({"error": "RAGç³»ç»Ÿå°šæœªåˆå§‹åŒ–"}), 500
    
    if rag_system.es_client:
        is_available = rag_system.es_client.is_available
        return jsonify({
            "es_service_enabled": True,
            "es_service_available": is_available,
            "es_service_url": rag_system.es_service_url,
            "hybrid_search_enabled": rag_system.use_hybrid_search
        })
    else:
        return jsonify({
            "es_service_enabled": False,
            "es_service_available": False,
            "hybrid_search_enabled": rag_system.use_hybrid_search
        })
        
# å¯åŠ¨å‡½æ•°
if __name__ == "__main__":
    # å¦‚æœå­˜åœ¨PORTç¯å¢ƒå˜é‡ï¼Œä½¿ç”¨å®ƒï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤çš„5000
    port = int(os.environ.get("PORT", 5000))
    # åœ¨å¼€å‘æ¨¡å¼ä¸‹å¯ç”¨è°ƒè¯•ï¼Œç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨ gunicorn
    debug = os.environ.get("FLASK_ENV") == "development"
    
    app.run(host="0.0.0.0", port=port, debug=debug)
        